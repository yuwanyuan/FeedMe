{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "Google Research 2025: Bolder breakthroughs, bigger impact",
      "link": "https://research.google/blog/google-research-2025-bolder-breakthroughs-bigger-impact/",
      "pubDate": "Wed, 17 Dec 2025 16:00:00 GMT",
      "isoDate": "2025-12-17T16:00:00.000Z",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">Google Research teams have invested over the years in advancing research and technology in a diverse range of strategic areas. We are working across time horizons, from bold moonshots and curiosity-driven transformative research where we explore the art of the possible, to innovation and applied research with accelerated impact. The <a href=\"https://research.google/blog/accelerating-the-magic-cycle-of-research-breakthroughs-and-real-world-applications/\">Magic Cycle of research</a> is accelerating — we’re driving research breakthroughs and translating them into real-world solutions, with impact on products, science and society, in close collaboration with many teams across Google and global partners.</p><p data-block-key=\"fc6rj\">This was quite a year! Our foundational AI breakthroughs helped make generative models more efficient, factual, multilingual, and multi-cultural, and we introduced generative UI. We advanced new architectures and algorithmic research and pioneered AI tools and agentic models that help accelerate scientific discovery. We achieved quantum breakthroughs that bring us closer to real-world applications of quantum computing; advanced research on Earth sciences to enable a level of planetary understanding never before possible; drove forward scientific domains including genomics, biology and neuroscience; and made headway on societal priorities like climate resilience, health and education.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-0d-Hero.width-1250.png\" alt=\"A timeline of Google Research’s 2025 moments\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-0d-Hero.width-1250.png\" alt=\"A timeline of Google Research’s 2025 moments\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"un5jf\"><i>A look back at some of Google Research's 2025 moments realized in collaboration with many teams across Google. This image was created with Nano Banana.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Factual, multilingual, multi-cultural GenAI\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Advancing generative models to be more efficient, factual, multilingual and multi-cultural</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">To help fuel this era of rapid innovation, we’re investing in efficiency, making Google products more cost and energy efficient, and setting the bar for the industry. We continue to develop new approaches based on <a href=\"https://research.google/blog/looking-back-at-speculative-decoding/\">speculative decoding</a>, such as <a href=\"https://arxiv.org/abs/2403.10444\" target=\"_blank\" rel=\"noopener noreferrer\">block verification</a>, to further accelerate efficiency gains. At the other end of the infrastructure stack, <a href=\"https://research.google/blog/solving-virtual-machine-puzzles-how-ai-is-optimizing-cloud-computing/\">LAVA</a> is a new scheduling algorithm that continuously re-predicts the lifespans of tasks on virtual machines. It is designed to optimize resource efficiency in large cloud data centers, without sacrificing reliability.</p><p data-block-key=\"birtg\">Equally critical, our pioneering research on LLM factuality, dating back to 2021, helps make <a href=\"https://blog.google/products/gemini/gemini-3/#gemini-3\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 3</a> our most capable and factual LLM yet. It achieves state-of-the-art performance on public factuality benchmarks like <a href=\"https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified\" target=\"_blank\" rel=\"noopener noreferrer\">SimpleQA Verified</a> and the new <a href=\"https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/\" target=\"_blank\" rel=\"noopener noreferrer\">FACTS benchmark suite</a> that we released with Google DeepMind and Kaggle. Users can be confident that products such as the Gemini app, <a href=\"https://blog.google/products/search/generative-ai-google-search-may-2024/\" target=\"_blank\" rel=\"noopener noreferrer\">AI Overviews</a> and <a href=\"https://blog.google/products/search/google-search-ai-mode-update/#ai-mode-search\" target=\"_blank\" rel=\"noopener noreferrer\">AI Mode</a> in Search, and Vertex AI all provide outputs grounded in world knowledge. This year we studied how LLMs <a href=\"https://arxiv.org/abs/2505.24858\" target=\"_blank\" rel=\"noopener noreferrer\">convey uncertainty</a>; presented a framework for assessing whether LLMs <a href=\"https://arxiv.org/abs/2503.15299\" target=\"_blank\" rel=\"noopener noreferrer\">encode more factual knowledge</a> in their parameters than they express in their outputs; presented a multilingual dataset that evaluates cross-lingual knowledge, called <a href=\"https://research.google/blog/eclektic-a-novel-benchmark-for-evaluating-cross-lingual-knowledge-transfer-in-llms/\">ECLeKTic</a>; and more.</p><p data-block-key=\"tc29\">We also explored the role of <a href=\"https://research.google/blog/deeper-insights-into-retrieval-augmented-generation-the-role-of-sufficient-context/\">sufficient context</a> in <a href=\"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\" target=\"_blank\" rel=\"noopener noreferrer\">retrieval augmented generation</a> systems, which enhance LLMs by providing them with relevant external context. We <a href=\"https://arxiv.org/abs/2411.06037\" target=\"_blank\" rel=\"noopener noreferrer\">demonstrated</a> that it is possible to know when an LLM has enough information to provide a correct answer to a question. This work supported the launch of the <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/retrieval-and-ranking#llm_reranker\" target=\"_blank\" rel=\"noopener noreferrer\">LLM Re-Ranker</a> in the <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/rag-overview\" target=\"_blank\" rel=\"noopener noreferrer\">Vertex AI RAG Engine</a>, leading to better retrieval metrics and system accuracy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"FACTS\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-3-FACTS.width-1250.png\" alt=\"A data table ranks 15 AI models by their &quot;FACTS Score,&quot; showing Gemini 3 Pro in first place with a score of 68.8.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-3-FACTS.width-1250.png\" alt=\"A data table ranks 15 AI models by their &quot;FACTS Score,&quot; showing Gemini 3 Pro in first place with a score of 68.8.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>We evaluated leading LLMs on the</i> <a href=\"https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>FACTS Benchmark Suite</i></a><i>, which includes four different factuality benchmarks. The table above lists 15 leading models and their overall FACTS score. Gemini 3 Pro leads in overall performance.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">With the rise of multimodal content, we’ve expanded our work on factuality to images, audio, video, 3D environments and LLM-generated applications. This work helps to improve the quality of Google’s video and image model families, including <a href=\"https://deepmind.google/models/veo/\" target=\"_blank\" rel=\"noopener noreferrer\">Veo</a>, <a href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#imagen-4-veo-3\" target=\"_blank\" rel=\"noopener noreferrer\">Imagen</a> and <a href=\"https://blog.google/products/gemini/updated-image-editing-model/\" target=\"_blank\" rel=\"noopener noreferrer\">Nano Banana</a>. It is a great example of the cycle of research and how we’re continuously adapting to real user needs. Our latest research includes making <a href=\"https://arxiv.org/abs/2504.17502\" target=\"_blank\" rel=\"noopener noreferrer\">text-to-image generation</a> and <a href=\"https://arxiv.org/pdf/2506.07631\" target=\"_blank\" rel=\"noopener noreferrer\">image captions</a> more accurate, and creating <a href=\"https://arxiv.org/abs/2505.22657\" target=\"_blank\" rel=\"noopener noreferrer\">3DMem-Bench</a> for evaluating an agent’s ability to reason over long-term memory in 3D.</p><p data-block-key=\"f4fhj\">Our long-running multilinguality research helped <a href=\"https://deepmind.google/models/gemma/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma</a> expand to over 140 languages, making it today’s best multilingual open model. We’re also augmenting our models with socio-cultural intelligence, attuning them to diverse user needs and global contexts. We introduced <a href=\"https://arxiv.org/abs/2510.06124\" target=\"_blank\" rel=\"noopener noreferrer\">TUNA</a>, a comprehensive taxonomy of user needs and actions, launched a community-based data <a href=\"https://research.google/blog/amplify-initiative-localized-data-for-globalized-ai/\">collection platform</a> to target under-represented languages and geographies, and developed new methods to ground models in <a href=\"https://arxiv.org/pdf/2502.13497\" target=\"_blank\" rel=\"noopener noreferrer\">diverse cultural knowledge</a> and datasets. This research helps to ensure that Google models can connect with users globally in responsible and culturally-aware ways.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Interactive interfaces with generative UI\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Introducing interactive interfaces with generative UI</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">In a world where users expect more engaging and visual experiences, we introduced a novel implementation of <a href=\"https://research.google/blog/generative-ui-a-rich-custom-visual-interactive-user-experience-for-any-prompt/\">generative UI</a> in Gemini 3. This powerful capability enables AI models to dynamically create immersive visual experiences and interactive interfaces, such as web pages, games, tools and apps, in response to a prompt. Our research comes to life in AI Mode on <a href=\"http://blog.google/products/search/gemini-3-search-ai-mode\" target=\"_blank\" rel=\"noopener noreferrer\">Google Search</a>, and in experiments such as dynamic view, in the <a href=\"https://blog.google/products/gemini/gemini-3-gemini-app\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini app</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"GenUI Van Gogh demo\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"JALZmOVlR7s\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=JALZmOVlR7s\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>Example of generative UI capabilities in dynamic view in the Gemini app. This is based on the prompt, “</i>Create a Van Gogh gallery with life context for each piece.<i>” More examples can be found</i> <a href=\"https://generativeui.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>here</i></a><i>.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"Generative UI RNA questions in Search\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EOY-5-GenUI.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>Example of generative UI capabilities in AI Mode in Google Search. This is based on the prompt, “</i>Show me how rna polymerase works. what are the stages of transcription and how is it different in prokaryotic and eukaryotic cells.<i>”</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Quantum computing\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Quantum computing: The next frontier</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">Our strategic investment in quantum computing is poised to accelerate the next frontier of computing and scientific discovery. In the 1980s, Clarke, Devoret, and Martinis laid the foundations for superconducting qubits, which led to their recognition as <a href=\"https://blog.google/inside-google/company-announcements/googler-michel-devoret-awarded-the-nobel-prize-in-physics/\" target=\"_blank\" rel=\"noopener noreferrer\">2025 Physics Nobel Laureates</a>. The 40-year <a href=\"https://quantumai.google/roadmap\" target=\"_blank\" rel=\"noopener noreferrer\">journey</a> since has yielded the nascent quantum computing industry and led to breakthroughs like our recently announced <a href=\"https://research.google/blog/a-verifiable-quantum-advantage/\">verifiable quantum advantage</a>, published on the cover of <a href=\"https://www.nature.com/articles/s41586-025-09526-6\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature</i></a>. This work describes our “<a href=\"https://blog.google/technology/research/quantum-echoes-willow-verifiable-quantum-advantage/\" target=\"_blank\" rel=\"noopener noreferrer\">Quantum Echoes</a>” algorithm, which runs on our <a href=\"https://research.google/blog/making-quantum-error-correction-work/\">Willow chip</a> 13,000 times faster than the best classical algorithm on one of the world’s fastest supercomputers. It offers a new way to explain interactions between atoms in a molecule observed <a href=\"https://arxiv.org/abs/2510.19550\" target=\"_blank\" rel=\"noopener noreferrer\">using nuclear magnetic resonance spectroscopy</a>. It brings us closer to <a href=\"https://blog.google/technology/research/useful-quantum-computing-applications/\" target=\"_blank\" rel=\"noopener noreferrer\">real-world applications</a> of quantum computing, such as advancing drug design and helping to make fusion energy a reality.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"Circuits to Chandeliers: A Quantum History\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"lpzR_rPbrac\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=lpzR_rPbrac\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Accelerating scientific discovery\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Accelerating scientific discovery</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">AI-powered models and platforms are fundamentally changing <i>how</i> science is conducted. We released <a href=\"https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/\">AI co-scientist</a>, a collaboration across Google Research, Cloud AI and Google DeepMind. This multi-agent <a href=\"https://arxiv.org/abs/2502.18864\" target=\"_blank\" rel=\"noopener noreferrer\">AI system</a> helps scientists generate novel hypotheses. We also shared our <a href=\"https://research.google/blog/accelerating-scientific-discovery-with-ai-powered-empirical-software/\">AI-powered empirical software</a> system, a Gemini-backed coding agent to help scientists write expert-level empirical software to evaluate and iterate on hypotheses. These tools accelerate the very process of making scientific discoveries. They open the door to a future where every scientist in a lab has a team of AI assistants simultaneously investigating thousands of potential solutions to the scientific challenges that motivate their research. Already at Stanford, our AI co-scientist has helped <a href=\"https://advanced.onlinelibrary.wiley.com/doi/pdf/10.1002/advs.202508751\" target=\"_blank\" rel=\"noopener noreferrer\">identify drugs</a> that could be repurposed to treat liver fibrosis. At Imperial College London, researchers working on antimicrobial resistance <a href=\"https://www.imperial.ac.uk/news/261293/googles-ai-co-scientist-could-enhance-research/\" target=\"_blank\" rel=\"noopener noreferrer\">found</a> that it produced the same hypothesis in days that their team took years to develop.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"An overview of the AI co-scientist\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EOY-7-AICoScientist.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>An overview of the AI co-scientist. It uses a coalition of specialized agents who iteratively generate, evaluate, and refine hypotheses.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Biology, genomics, and neuroscience\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Advancing science — from biology to genomics to neuroscience</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"xa7ab\">We continue to advance core scientific research. <a href=\"https://research.google/blog/using-ai-to-identify-genetic-variants-in-tumors-with-deepsomatic/\">DeepSomatic</a> and <a href=\"https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/\" target=\"_blank\" rel=\"noopener noreferrer\">C2S-Scale</a> join the AI-powered fight against cancer and are paving the way for brand-new therapies. Published in <a href=\"https://www.nature.com/articles/s41587-025-02839-x\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature Biotechnology</i></a><i>,</i> DeepSomatic is an <a href=\"https://github.com/google/deepsomatic\" target=\"_blank\" rel=\"noopener noreferrer\">open-source</a> tool that builds on <a href=\"https://blog.google/technology/research/ten-years-google-genomics/\" target=\"_blank\" rel=\"noopener noreferrer\">10 years of genomics</a> research at Google and helps scientists and doctors identify genetic variants in cancer cells. Our partners at <a href=\"https://www.childrensmercy.org/childrens-mercy-research-institute/about/\" target=\"_blank\" rel=\"noopener noreferrer\">Children’s Mercy</a> are using it to understand <a href=\"https://www.medrxiv.org/content/10.1101/2024.11.05.24316078v1\" target=\"_blank\" rel=\"noopener noreferrer\">how and why a particular form of cancer</a> affects a patient in order to develop personalized cures. C2S-Scale, which we released in collaboration with Google DeepMind and Yale, is a 27 billion parameter foundation model for single-cell analysis that made headlines for generating a <a href=\"https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/\" target=\"_blank\" rel=\"noopener noreferrer\">novel hypothesis</a> about cancer cellular behavior.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"NotebookLM summaries of our genomics research\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-8-NLMGenomics.width-1250.png\" alt=\"Two Google Research banners featuring microscopic cells and a DNA strand ask questions about linking genetics to health and genome sequencing.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-8-NLMGenomics.width-1250.png\" alt=\"Two Google Research banners featuring microscopic cells and a DNA strand ask questions about linking genetics to health and genome sequencing.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>Our public NotebookLMs make our decade of genomics research more accessible and allow people to dive deeper into topics that interest them. Explore \"</i><a href=\"https://notebooklm.google.com/notebook/31c20c44-8c94-4f81-a2b8-a020a761d122\" target=\"_blank\" rel=\"noopener noreferrer\"><i>How do scientists link genetics to health?</i></a><i>\" and “</i><a href=\"https://notebooklm.google.com/notebook/a09e40ad-d41f-43af-a3ca-5fc82bd459e5\" target=\"_blank\" rel=\"noopener noreferrer\"><i>How can scientists know what's in your genome?</i></a><i>”</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">Turning to neuroscience, we <a href=\"https://www.nature.com/articles/s41586-025-08985-1\" target=\"_blank\" rel=\"noopener noreferrer\">published in <i>Nature</i></a> the first-ever method for using commonly available light microscopes to comprehensively map all the neurons and their connections in a block of brain tissue. Working with the <a href=\"https://ista.ac.at/en/home/\" target=\"_blank\" rel=\"noopener noreferrer\">Institute of Science and Technology Austria</a>, we applied our suite of image analysis and ML tools for <a href=\"https://sites.research.google/neural-mapping/\">connectomics</a>, leveraging over a decade of contributions we’ve made to this scientific field to understand the workings of the brain. We hope the method, called <a href=\"https://research.google/blog/a-new-light-on-neural-connections/\">LICONN</a>, will enable more labs around the world to pursue connectomics studies.</p><p data-block-key=\"bp5mb\">We also <a href=\"https://zapbench-release.storage.googleapis.com/landing.html\" target=\"_blank\" rel=\"noopener noreferrer\">open-sourced</a> the <a href=\"https://research.google/blog/improving-brain-models-with-zapbench/\">Zebrafish Activity Prediction Benchmark</a> (ZAPBench) in collaboration with <a href=\"https://www.janelia.org/\" target=\"_blank\" rel=\"noopener noreferrer\">HHMI Janelia</a> and <a href=\"https://lichtmanlab.fas.harvard.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">Harvard</a>. With recordings of more than 70,000 neurons from the larval zebrafish brain, it will enable scientists to <a href=\"https://arxiv.org/abs/2503.02618\" target=\"_blank\" rel=\"noopener noreferrer\">investigate the relationship</a> between the structural wiring and dynamic neural activity across an entire vertebrate brain for the first time.</p><p data-block-key=\"8r98j\">Plus, we demonstrated how LLMs can help us understand the human brain. In a <a href=\"https://www.nature.com/articles/s41593-022-01026-4\" target=\"_blank\" rel=\"noopener noreferrer\">series</a> of <a href=\"https://www.nature.com/articles/s41467-024-46631-y\" target=\"_blank\" rel=\"noopener noreferrer\">studies</a> conducted over five years with <a href=\"https://hassonlab.princeton.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">Princeton University</a>, <a href=\"https://nyulangone.org/locations/comprehensive-epilepsy-center\" target=\"_blank\" rel=\"noopener noreferrer\">NYU</a>, and <a href=\"https://www.deepcognitionlab.com/\" target=\"_blank\" rel=\"noopener noreferrer\">HUJI</a>, we explored connections in the ways the human brain and deep language models <a href=\"https://research.google/blog/deciphering-language-processing-in-the-human-brain-through-llm-representations/\">process natural language</a>. We discovered <a href=\"https://www.nature.com/articles/s41562-025-02105-9\" target=\"_blank\" rel=\"noopener noreferrer\">remarkable alignment</a> between the neural activity in the speech and language areas of the human brain and the speech and language embeddings of a Transformer-based speech-to-text model, and showed how the <a href=\"https://www.nature.com/articles/s41467-025-65518-0.epdf?sharing_token=XqLw7uzRcb-8uuak6fH8BdRgN0jAjWel9jnR3ZoTv0PSBTWDWtB7_YH7achJWXIc1XVbsxtqM_vvMoZPLZuQIoT45xjjIkxPMOnKorePnnoS9QL1SRZ58ZI65TPosI2w5PF8h5ZazOVlXhxgrnGSxw1GguylV5BBtKxLUyCAfLM%3D\" target=\"_blank\" rel=\"noopener noreferrer\">temporal structure</a> of language processing in the brain corresponds to the layered hierarchy of deep language models. Our research indicates that language representation in deep learning models could offer a novel framework for understanding the brain’s neural code; it also paves the way for innovative approaches to creating artificial neural networks with better information processing capabilities.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Planetary intelligence and crisis resilience\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Enabling planetary intelligence and crisis resilience</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\"><a href=\"https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/\">Earth AI</a> is Google’s family of geospatial AI models and reasoning agents that provides users with actionable insights, grounded in real-world understanding. Developed in collaboration with teams across Google, it builds on our years of modeling the world, paired with Gemini’s advanced reasoning, to offer an unprecedented level of understanding about our planet. It <a href=\"https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/\">brings together</a> many of Google’s geospatial models and technologies such as <a href=\"https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/\">remote sensing imagery</a>, <a href=\"https://research.google/blog/fast-accurate-climate-modeling-with-neuralgcm/\">weather</a>, <a href=\"https://blog.google/products/maps/google-maps-apis-environment-sustainability/\" target=\"_blank\" rel=\"noopener noreferrer\">air quality</a>, <a href=\"https://sites.research.google/gr/floodforecasting/\">floods</a>, <a href=\"https://research.google/blog/insights-into-population-dynamics-a-foundation-model-for-geospatial-inference/\">population dynamics</a>, <a href=\"https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/\" target=\"_blank\" rel=\"noopener noreferrer\">AlphaEarth Foundations</a>, <a href=\"https://research.google/blog/introducing-mobility-ai-advancing-urban-transportation/\">mobility</a>, <a href=\"https://blog.google/technology/research/open-buildings-ai-powered-maps-for-a-changing-world/\" target=\"_blank\" rel=\"noopener noreferrer\">maps</a> and more. Thanks to Gemini’s reasoning power, Earth AI can synthesize vast datasets about the planet to generate insights in minutes that would previously take years of research. Earth AI offerings are available in <a href=\"https://mapsplatform.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Maps Platform</a>, <a href=\"https://blog.google/technology/research/new-updates-and-more-access-to-google-earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth and to Trusted Testers via Google Cloud</a>, and are already being used by partners, helping cities, enterprises and nonprofits with critical tasks from <a href=\"https://www.youtube.com/watch?v=ZxmB8Z5i1Ls&amp;list=PL95lT3XlM14ROFtYnlBDYZbKipH3JLaC7&amp;index=3\" target=\"_blank\" rel=\"noopener noreferrer\">urban planning</a> to <a href=\"https://www.youtube.com/watch?v=8-macH8ozr4&amp;list=PL95lT3XlM14ROFtYnlBDYZbKipH3JLaC7&amp;index=5\" target=\"_blank\" rel=\"noopener noreferrer\">disaster response</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"Google Earth AI\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"UZ4RaLGDXI4\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=UZ4RaLGDXI4\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">We’ve also made significant strides with the climate models that feed our AI capabilities for understanding the Earth, helping communities to prepare for and respond to severe weather and natural disasters. This year, in collaboration with the <a href=\"https://www.earthfirealliance.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth Fire Alliance</a>, the <a href=\"https://www.moore.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Moore Foundation</a> and <a href=\"https://www.muonspace.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Muon Space</a>, we launched the first satellite in the <a href=\"https://sites.research.google/gr/wildfires/firesat/\">FireSat</a> constellation. Named as one of <a href=\"https://time.com/collections/best-inventions-2025/7318230/firesat/\" target=\"_blank\" rel=\"noopener noreferrer\">TIME magazine’s best inventions</a> of 2025, FireSat uses AI to provide critical near–real-time insights for first responders. It has already <a href=\"https://blog.google/technology/research/first-firesat-images/\" target=\"_blank\" rel=\"noopener noreferrer\">detected</a> small wildfires not caught by other space-based systems, and when fully operational with over 50 satellites, it will be able to detect a classroom-sized wildfire anywhere on Earth.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"FireSat\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-10-FireSat.width-1250.png\" alt=\"A satellite map of the Oregon-California border featuring a thermal infrared inset pinpointing a small fire near Medford.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-10-FireSat.width-1250.png\" alt=\"A satellite map of the Oregon-California border featuring a thermal infrared inset pinpointing a small fire near Medford.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>FireSat is equipped with a custom mid-wave infrared (MWIR) sensor that detected a small, relatively cool roadside fire near Medford, Oregon that was not detected by other space-based systems. It is seen here overlaid on a Google Earth basemap. Credit: Muon Space and Earth Fire Alliance.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">We also <a href=\"https://blog.google/technology/research/new-updates-and-more-access-to-google-earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">expanded</a> our flood forecasting models to cover over 2 billion people in 150 countries for the most significant riverine flood events, helping communities stay safe and informed. We partnered with our colleagues at Google DeepMind to debut an experimental model for <a href=\"https://deepmind.google/blog/how-were-supporting-better-tropical-cyclone-prediction-with-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">cyclone predictions</a> using stochastic neural networks that's helping weather agencies predict a cyclone’s path up to 15 days in advance. Moreover, we collaborated with Google DeepMind to launch <a href=\"https://deepmind.google/science/weathernext/\" target=\"_blank\" rel=\"noopener noreferrer\">WeatherNext 2</a>, which delivers our most accurate, mid-range AI weather forecasts to date. It’s now available to users of Search, Gemini and Pixel Weather as well as to developers on Google Maps and Google Cloud.</p><p data-block-key=\"d5150\">At the start of the year, we expanded <a href=\"https://blog.google/intl/en-africa/products/explore-get-answers/nowcasting-on-search-is-bringing-ai-powered-weather-forecasts-to-users-across-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">Nowcasting on Search to Africa</a>, bringing highly precise, short-term precipitation forecasts to users across the continent for the first time. We have since made this available for users worldwide. Powered by our <a href=\"https://arxiv.org/abs/2510.13050\" target=\"_blank\" rel=\"noopener noreferrer\">MetNet</a> model, it represents the first AI weather model on Search to operate at a global scale. In India, the University of Chicago and the Indian Ministry of Agriculture and Farmers’ Welfare used Google’s <a href=\"https://blog.google/technology/research/indian-farmers-monsoon-prediction/\" target=\"_blank\" rel=\"noopener noreferrer\">NeuralGCM model</a> to send longer-range monsoon forecasts to 38 million farmers, helping them make critical decisions about what to plant and when.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Health AI\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Advancing Health AI</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">As we make scientific breakthroughs with the potential to significantly reform healthcare, we’re working with partners and healthcare professionals to bring new capabilities responsibly to people around the world. <a href=\"https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/\">AMIE</a> is our conversational medical agent developed together with Google DeepMind and published in <a href=\"https://www.nature.com/articles/s41586-025-08866-7\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature</i></a>. It can now reason through <a href=\"https://research.google/blog/amie-gains-vision-a-research-ai-agent-for-multi-modal-diagnostic-dialogue/\">multimodal</a> evidence and support longitudinal <a href=\"https://research.google/blog/from-diagnosis-to-treatment-advancing-amie-for-longitudinal-disease-management/\">disease management</a> as well as or better than primary care physicians under simulated settings with professional patient actors. We’re exploring how this research could enable a physician-centered model with <a href=\"https://research.google/blog/enabling-physician-centered-oversight-for-amie/\">asynchronous oversight</a> of AMIE. We also launched <a href=\"https://community.fitbit.com/t5/The-Pulse-Fitbit-Community-Blog/Introducing-the-new-Plan-for-Care-Fitbit-Lab/ba-p/5796289\" target=\"_blank\" rel=\"noopener noreferrer\">Plan for Care Lab</a>, Fitbit’s latest experimental capability, to a select number of opt-in users. It’s designed to help users access personalized support when assessing symptoms at home and preparing for an upcoming doctor’s visit. In addition, <a href=\"https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/\">MedGemma</a>, Google's most capable open model for multimodal medical comprehension, is available as part of our <a href=\"https://developers.google.com/health-ai-developer-foundations\" target=\"_blank\" rel=\"noopener noreferrer\">Health AI Developer Foundations</a> (HAI-DEF). It can support tasks such as classification, report generation, or interpreting complex electronic health records, making it useful for medical research and product development. Since launch, MedGemma and HAI-DEF have &gt;2M downloads. Plus, our <a href=\"https://developers.google.com/open-health-stack\" target=\"_blank\" rel=\"noopener noreferrer\">Open Health Stack</a> was recognized at the <a href=\"https://www.weforum.org/stories/2025/01/social-innovation-has-moved-from-the-margins-to-the-mainstream/\" target=\"_blank\" rel=\"noopener noreferrer\">World Economic Forum</a> for helping to address inequities in health access. It provides the building blocks for developers to create next-gen, data-driven healthcare apps for use in low-resource settings.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Learning and education\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Advancing learning and education</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">Gemini is now infused with <a href=\"https://blog.google/outreach-initiatives/education/google-learnlm-gemini-generative-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM</a>, Google’s family of models fine-tuned for learning, <a href=\"https://storage.googleapis.com/deepmind-media/LearnLM/LearnLM_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">announced</a> last year. We launched <a href=\"https://research.google/blog/learn-your-way-reimagining-textbooks-with-generative-ai/\">Learn Your Way</a> on <a href=\"https://learnyourway.withgoogle.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Labs</a>, powered by LearnLM’s foundational capabilities. It explores the future of textbooks by generating multiple engaging representations of the source material. It transforms static textbooks into active learning experiences that are tailored for every student, with interactive quizzes that enable real-time assessment, feedback, and content personalization. In our<a href=\"https://arxiv.org/abs/2509.13348\" target=\"_blank\" rel=\"noopener noreferrer\"> efficacy study</a>, students using it scored 11 percentage points higher on retention tests. We also piloted our LearnLM model for <a href=\"https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/5-ways-were-bringing-ai-innovations-to-people-across-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">answer assessment</a> with thousands of high school students in Ghana. Plus, we explored the intersection of education and health through a learner-centric approach quantifying the benefits of LearnLM in <a href=\"https://research.google/blog/how-googles-ai-can-help-transform-health-professions-education/\">medical education</a> settings.</p><p data-block-key=\"bo45o\">This research brings us closer to realizing a future where AI makes learning more effective for everyone. In collaboration with teams across Google, we published “<a href=\"https://services.google.com/fh/files/misc/future_of_learning.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">AI and the Future of Learning</a>”, sharing our approach, grounded in learning science, to responsibly enable AI for learning. We’re creating personalized teaching experiences, empowering educators, and working to address challenges such as critical thinking and equal access.</p><p data-block-key=\"chemf\">In parallel, our AI Literacy efforts aim to inspire the next generation of innovators. <a href=\"https://research.google/ai-quests/intl/en_gb\">AI Quests</a>, <a href=\"https://blog.google/outreach-initiatives/education/ai-quests/\" target=\"_blank\" rel=\"noopener noreferrer\">launched</a> with the <a href=\"https://acceleratelearning.stanford.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">Stanford Accelerator for Learning</a>, allows students to step into the shoes of Google researchers and use AI to solve challenges like flood forecasting and detecting eye disease. During <a href=\"https://blog.google/outreach-initiatives/education/next-gen-computer-science-innovators/\" target=\"_blank\" rel=\"noopener noreferrer\">Computer Science Education Week</a>, hundreds of Googler volunteers brought these quests to classrooms around the world.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EOY-11-LearnYourWay.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>Learn Your Way explores how GenAI can transform educational materials into more effective, engaging, learner-driven experiences. It generates multiple representations of the source material, tailored for each student.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"ML foundations and algorithmic research\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Advancing ML foundations and algorithmic research</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">Our broad foundational ML and algorithmic research is the bedrock for groundbreaking advances across domains. This work provides the essential frameworks that power products and services, and underpins the development of next-generation models and intelligent systems. We improved voice search, for example, with our new <a href=\"https://research.google/blog/speech-to-retrieval-s2r-a-new-approach-to-voice-search/\">Speech-to-Retrieval</a> engine, which directly interprets and retrieves information from a spoken query without having to convert it first to text. And our state-of-the-art <a href=\"https://research.google/blog/rich-human-feedback-for-text-to-image-generation/\">predictive modeling</a> of rich human feedback improved text-to-image generation quality in products, including <a href=\"https://developers.googleblog.com/en/imagen-3-arrives-in-the-gemini-api/\" target=\"_blank\" rel=\"noopener noreferrer\">Imagen3</a>, <a href=\"https://blog.google/products/ads-commerce/new-creative-updates-advertisers-generate-lifestyle/\" target=\"_blank\" rel=\"noopener noreferrer\">creative generation</a> and <a href=\"https://blog.google/products/ads-commerce/google-ai-ads-creative/\" target=\"_blank\" rel=\"noopener noreferrer\">editing</a> in Google Ads, and <a href=\"https://blog.google/products/shopping/google-shopping-ai-mode-virtual-try-on-update/\" target=\"_blank\" rel=\"noopener noreferrer\">virtual try on</a> for shopping. We also extended this research to improve video generation quality in the <a href=\"https://blog.google/products/google-cloud/sphere-wizard-of-oz/\" target=\"_blank\" rel=\"noopener noreferrer\">Wizard of Oz film launch</a> at <a href=\"https://www.thesphere.com/shows/wizard-of-oz-experience\" target=\"_blank\" rel=\"noopener noreferrer\">Sphere</a> in Las Vegas.</p><p data-block-key=\"f7the\">The impact of our algorithmic research extends well beyond Google products. Our <a href=\"https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/\">TimesFM</a> model, which <a href=\"https://cloud.google.com/blog/products/data-analytics/timesfm-models-in-bigquery-and-alloydb\" target=\"_blank\" rel=\"noopener noreferrer\">helps businesses</a> with time-series forecasting, now has hundreds of millions of queries per month in <a href=\"https://cloud.google.com/bigquery\" target=\"_blank\" rel=\"noopener noreferrer\">BigQuery</a> and <a href=\"https://cloud.google.com/products/alloydb\" target=\"_blank\" rel=\"noopener noreferrer\">AlloyDB</a>. We introduced a novel approach using <a href=\"https://research.google/blog/time-series-foundation-models-can-be-few-shot-learners/\">in-context fine-tuning</a>, which teaches the model how to learn from multiple examples at inference time to further enhance its performance. Our <a href=\"https://research.google/blog/introducing-mobility-ai-advancing-urban-transportation/\">Mobility AI</a> model leverages our two decades of innovation in maps and transportation to provide transportation agencies with powerful tools for data-driven policymaking and traffic management. It can understand traffic and parking patterns, simulate systems to allow engineers to test different scenarios, and identify effective solutions for transportation networks. This complements our consumer-facing breakthroughs in Google Maps and Search, such as specialized models for <a href=\"https://research.google/blog/how-we-created-hov-specific-etas-in-google-maps/\">calculating ETAs</a> and <a href=\"https://research.google/blog/optimizing-llm-based-trip-planning/\">optimizing trip planning</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-12-MobilityAI.width-1250.png\" alt=\"A &quot;Traffic Simulation API&quot; dashboard displays a map of Seattle with purple-shaded road segments analyzing the impact of lane closures.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-12-MobilityAI.width-1250.png\" alt=\"A &quot;Traffic Simulation API&quot; dashboard displays a map of Seattle with purple-shaded road segments analyzing the impact of lane closures.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>Our Mobility AI Traffic Simulation API is built for modeling complex, city-scale traffic scenarios. The tool provides high-fidelity simulations of road closures, helping to de-risk major infrastructure investments and validate emergency response plans. It was launched in Seattle, Denver, Boston, Philadelphia and Orlando. The above image shows an example simulation in Seattle. Watch the full demo</i> <a href=\"https://www.youtube.com/watch?v=Pv91I43VpOQ\" target=\"_blank\" rel=\"noopener noreferrer\"><i>here</i></a><i>.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">Additionally, we’ve explored a range of topics in economics and computation from pricing dynamics in <a href=\"https://arxiv.org/abs/2502.20346\" target=\"_blank\" rel=\"noopener noreferrer\">modular marketplaces</a> and in <a href=\"https://arxiv.org/abs/2503.10910\" target=\"_blank\" rel=\"noopener noreferrer\">procurement auctions</a>, to <a href=\"https://dl.acm.org/doi/10.1145/3736252.3742578\" target=\"_blank\" rel=\"noopener noreferrer\">data-driven mechanism design</a> and <a href=\"https://dl.acm.org/doi/10.1145/3696410.3714881\" target=\"_blank\" rel=\"noopener noreferrer\">various</a> <a href=\"https://dl.acm.org/doi/10.1145/3736252.3742545\" target=\"_blank\" rel=\"noopener noreferrer\">approaches</a> to optimize ad auctions. We also studied swap regret and <a href=\"https://arxiv.org/abs/2502.20229\" target=\"_blank\" rel=\"noopener noreferrer\">correlated equilibria in games</a>.</p><p data-block-key=\"6ninf\">As AI becomes increasingly integrated into our daily lives, building it with privacy at its core is critical for users and industries. To this end, we’ve developed and published <a href=\"https://research.google/blog/fine-tuning-llms-with-user-level-differential-privacy/\">novel</a> <a href=\"https://research.google/blog/beyond-billion-parameter-burdens-unlocking-data-synthesis-with-a-conditional-generator/\">algorithms</a> for <a href=\"https://research.google/blog/synthetic-and-federated-privacy-preserving-domain-adaptation-with-llms-for-mobile-applications/\">private</a> learning and <a href=\"https://research.google/blog/securing-private-data-at-scale-with-differentially-private-partition-selection/\">private</a> <a href=\"https://research.google/blog/toward-provably-private-insights-into-ai-use/\">analytics</a>, and open sourced robust software tools to enable <a href=\"https://research.google/blog/discovering-new-words-with-confidential-federated-analytics/\">external verifiability</a>. For example, we introduced <a href=\"https://research.google/blog/parfait-enabling-private-ai-with-research-tools/\">Parfait</a>, a new GitHub organization for businesses and open-source projects. It has supported Google deployments of federated learning and analytics from <a href=\"https://support.google.com/gboard/answer/12373137?hl=en#zippy=%2Cfederated-learning\" target=\"_blank\" rel=\"noopener noreferrer\">Gboard</a> to <a href=\"https://arxiv.org/abs/2412.07962\" target=\"_blank\" rel=\"noopener noreferrer\">Google Maps</a>. We also announced <a href=\"https://research.google/blog/differentially-private-machine-learning-at-scale-with-jax-privacy/\">Jax Privacy 1.0</a>, a library for ML with differential privacy, which we used to train <a href=\"https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/\">VaultGemma</a>, the largest and most capable open model trained from scratch with differential privacy, with weights available on <a href=\"https://huggingface.co/google/vaultgemma-1b\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face</a> and <a href=\"https://www.kaggle.com/models/google/vaultgemma\" target=\"_blank\" rel=\"noopener noreferrer\">Kaggle</a>. By leveling up our privacy capabilities, we offer much stronger protections to businesses and users</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Novel architectures\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Introducing novel architectures</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">Our foundational ML research introduces advanced approaches to enable new opportunities. <a href=\"https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/\">Nested Learning</a> is a new ML paradigm that represents a leap forward in our understanding of deep learning. It treats model architecture and optimization as a single system that contains several, smaller, nested optimization problems. By unifying these elements, it solves the problem of catastrophic forgetting, when LLMs become forgetful and less capable at old tasks after learning new tasks. This research could help us build the next generation of more capable, self-improving AI. Meanwhile, our <a href=\"https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/\">Titans architecture and the MIRAS framework</a> mark a significant advancement in sequence modelling. They allow AI models to work much faster and handle massive contexts by employing deep neural networks that learn to memorize as data comes in, improving AI’s long-term memory.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"MIRAS Framework\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-13-MIRAS.width-800.gif\" alt=\"A diagram titled &quot;MIRAS Framework&quot; illustrates how input tokens are processed through associative memory, featuring a sliding focus on individual data blocks.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-13-MIRAS.width-800.gif\" alt=\"A diagram titled &quot;MIRAS Framework&quot; illustrates how input tokens are processed through associative memory, featuring a sliding focus on individual data blocks.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>In the MIRAS framework, we aim to learn an associative memory, mapping between keys and values. For each token, the memory module internally optimizes its inner attentional bias while using its retention gate to make sure that it does not deviate from its past state.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">We also introduced <a href=\"https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/\">MUVERA</a>, a novel retrieval algorithm that reduces complex multi-vector retrieval back to single-vector maximum inner product search, achieving state-of-the-art performance with significantly improved efficiency. It creates new possibilities for information retrieval for use in applications such as recommendation systems and natural language processing. And our progress on <a href=\"https://research.google/blog/graph-foundation-models-for-relational-data/\">graph foundational models</a> pushes the frontiers of graph learning. While most graph neural networks are fixed to a specific graph on which the model has been trained, we developed graph foundational models capable of generalizing to arbitrary tables, features and tasks. This opens up new avenues for model reuse.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Collaboration and open research\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Collaborating with the research ecosystem</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">We partner with the academic community, industry leaders, governments and scientific institutes around the world. We also continue to engage the ecosystem through our Research@ events from <a href=\"https://research.google/blog/accelerating-the-magic-cycle-of-research-breakthroughs-and-real-world-applications/\">Mountain View</a> to <a href=\"https://www.youtube.com/watch?v=sikTOH-0J_c\" target=\"_blank\" rel=\"noopener noreferrer\">Tokyo</a>, <a href=\"https://blog.google/intl/en-au/company-news/technology/research-sydney-charting-new-ai-frontiers-alongside-the-research-ecosystem-in-australia/\" target=\"_blank\" rel=\"noopener noreferrer\">Sydney</a> and <a href=\"https://blog.google/technology/research/ai-collaboration-poland-2025/\" target=\"_blank\" rel=\"noopener noreferrer\">Poland</a>, and we support hundreds of PhD students in Google’s <a href=\"https://blog.google/outreach-initiatives/google-org/phd-fellowship-program-2025/\" target=\"_blank\" rel=\"noopener noreferrer\">Fellowship Program</a>.</p><p data-block-key=\"abe8b\">As a global team, we continue to expand our footprint beyond our major hubs. Having solidified our research <a href=\"https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/supporting-the-future-of-ai-research-in-africa-and-globally/\" target=\"_blank\" rel=\"noopener noreferrer\">investment</a> and <a href=\"https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/5-ways-were-bringing-ai-innovations-to-people-across-africa/#:~:text=Jul%2024%2C%202025,Mail\" target=\"_blank\" rel=\"noopener noreferrer\">innovation</a> in Africa (Accra and Nairobi) and our presence in Australia, we are now preparing to inaugurate a new Google Research hub in Singapore in 2026.</p><p data-block-key=\"j9dl\">We share our work through publications, conferences, academic talks, benchmarks, datasets and open-source releases. We’ve <a href=\"https://research.google/conferences-and-events/?&amp;year=2025\">sponsored and hosted workshops at conferences</a>, most recently at <a href=\"https://research.google/conferences-and-events/google-at-neurips-2025/\">NeurIPS</a>. We recently introduced an <a href=\"https://research.google/blog/gemini-provides-automated-feedback-for-theoretical-computer-scientists-at-stoc-2026/\">experimental program</a> that provided automated feedback to scientists before they submit their conference papers for peer review, helping them to rigorously verify their work and accelerate research workflows. Plus, we launched <a href=\"https://notebooklm.google.com/notebook/24d50377-8c14-4851-bcc2-b2d67b039041\" target=\"_blank\" rel=\"noopener noreferrer\">Google Research Featured Notebooks</a> in collaboration with NotebookLM, to make research more accessible to a broader community.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"AI as an amplifier of human ingenuity\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">AI as an amplifier of human ingenuity</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">This is a golden age for research. Never before have technical breakthroughs and scientific progress so quickly materialized into impactful, real-world solutions, which, in turn, bring to the fore new data and questions that inspire new avenues of foundational research. This <a href=\"https://research.google/blog/accelerating-the-magic-cycle-of-research-breakthroughs-and-real-world-applications/\">magic cycle</a> is accelerating significantly, propelled by more powerful models, new agentic tools that support scientific discovery, and open platforms and tools.</p><p data-block-key=\"77o7f\">Together with our Google colleagues and partners, we’re advancing research and technologies that aim to be helpful in diverse areas. Our research, grounded in a rigorous dedication to safety and trust, serves to unlock human potential — whether that’s to help a scientist accelerate their research, or a student learn more effectively and master new concepts, or to empower a doctor, developer or teacher.</p><p data-block-key=\"atf0n\">It is truly an exciting time to be in research. We’re able to leverage the full stack of Google AI infrastructure, models, platforms, and world-class talent, and contribute to products used by billions. We will keep building on our legacy, asking the biggest questions of today, and aiming to enable the solutions of tomorrow. We’ll keep advancing AI in a bold and responsible way, for the benefit of society, to help enhance human capacity and make AI an amplifier of human ingenuity.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"Accelerating the Speed and Scope of Discovery\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"VqSrYdDM0Zw\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=VqSrYdDM0Zw\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Acknowledgements\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n            <div class=\" component-intro__description --has-heading\">\n                <p data-block-key=\"2nd4e\"><i>With thanks to everyone in Google Research, and many collaborators, who have contributed to this blog and the work represented here.</i></p>\n            </div>\n        \n    </div>\n\n\n\n    \n</div>\n\n    </div>\n</section>\n\n                    \n                ",
      "contentSnippet": "Google Research teams have invested over the years in advancing research and technology in a diverse range of strategic areas. We are working across time horizons, from bold moonshots and curiosity-driven transformative research where we explore the art of the possible, to innovation and applied research with accelerated impact. The Magic Cycle of research is accelerating — we’re driving research breakthroughs and translating them into real-world solutions, with impact on products, science and society, in close collaboration with many teams across Google and global partners.\nThis was quite a year! Our foundational AI breakthroughs helped make generative models more efficient, factual, multilingual, and multi-cultural, and we introduced generative UI. We advanced new architectures and algorithmic research and pioneered AI tools and agentic models that help accelerate scientific discovery. We achieved quantum breakthroughs that bring us closer to real-world applications of quantum computing; advanced research on Earth sciences to enable a level of planetary understanding never before possible; drove forward scientific domains including genomics, biology and neuroscience; and made headway on societal priorities like climate resilience, health and education.\nA look back at some of Google Research's 2025 moments realized in collaboration with many teams across Google. This image was created with Nano Banana.\nAdvancing generative models to be more efficient, factual, multilingual and multi-cultural\nTo help fuel this era of rapid innovation, we’re investing in efficiency, making Google products more cost and energy efficient, and setting the bar for the industry. We continue to develop new approaches based on speculative decoding, such as block verification, to further accelerate efficiency gains. At the other end of the infrastructure stack, LAVA is a new scheduling algorithm that continuously re-predicts the lifespans of tasks on virtual machines. It is designed to optimize resource efficiency in large cloud data centers, without sacrificing reliability.\nEqually critical, our pioneering research on LLM factuality, dating back to 2021, helps make Gemini 3 our most capable and factual LLM yet. It achieves state-of-the-art performance on public factuality benchmarks like SimpleQA Verified and the new FACTS benchmark suite that we released with Google DeepMind and Kaggle. Users can be confident that products such as the Gemini app, AI Overviews and AI Mode in Search, and Vertex AI all provide outputs grounded in world knowledge. This year we studied how LLMs convey uncertainty; presented a framework for assessing whether LLMs encode more factual knowledge in their parameters than they express in their outputs; presented a multilingual dataset that evaluates cross-lingual knowledge, called ECLeKTic; and more.\nWe also explored the role of sufficient context in retrieval augmented generation systems, which enhance LLMs by providing them with relevant external context. We demonstrated that it is possible to know when an LLM has enough information to provide a correct answer to a question. This work supported the launch of the LLM Re-Ranker in the Vertex AI RAG Engine, leading to better retrieval metrics and system accuracy.\nWe evaluated leading LLMs on the FACTS Benchmark Suite, which includes four different factuality benchmarks. The table above lists 15 leading models and their overall FACTS score. Gemini 3 Pro leads in overall performance.\nWith the rise of multimodal content, we’ve expanded our work on factuality to images, audio, video, 3D environments and LLM-generated applications. This work helps to improve the quality of Google’s video and image model families, including Veo, Imagen and Nano Banana. It is a great example of the cycle of research and how we’re continuously adapting to real user needs. Our latest research includes making text-to-image generation and image captions more accurate, and creating 3DMem-Bench for evaluating an agent’s ability to reason over long-term memory in 3D.\nOur long-running multilinguality research helped Gemma expand to over 140 languages, making it today’s best multilingual open model. We’re also augmenting our models with socio-cultural intelligence, attuning them to diverse user needs and global contexts. We introduced TUNA, a comprehensive taxonomy of user needs and actions, launched a community-based data collection platform to target under-represented languages and geographies, and developed new methods to ground models in diverse cultural knowledge and datasets. This research helps to ensure that Google models can connect with users globally in responsible and culturally-aware ways.\nIntroducing interactive interfaces with generative UI\nIn a world where users expect more engaging and visual experiences, we introduced a novel implementation of generative UI in Gemini 3. This powerful capability enables AI models to dynamically create immersive visual experiences and interactive interfaces, such as web pages, games, tools and apps, in response to a prompt. Our research comes to life in AI Mode on Google Search, and in experiments such as dynamic view, in the Gemini app.\n\n      \n      \nWatch the film\nLink to Youtube Video\nExample of generative UI capabilities in dynamic view in the Gemini app. This is based on the prompt, “Create a Van Gogh gallery with life context for each piece.” More examples can be found here.\nplay silent looping video\n            pause silent looping video\n          \n\n\n\n            \n\n\n\n  \n\n\n  \n\n\n          \nunmute video\n        mute video\n    \n\n            \n\n        \n        \n            \n\n            \n\n        \n    \nExample of generative UI capabilities in AI Mode in Google Search. This is based on the prompt, “Show me how rna polymerase works. what are the stages of transcription and how is it different in prokaryotic and eukaryotic cells.”\nQuantum computing: The next frontier\nOur strategic investment in quantum computing is poised to accelerate the next frontier of computing and scientific discovery. In the 1980s, Clarke, Devoret, and Martinis laid the foundations for superconducting qubits, which led to their recognition as 2025 Physics Nobel Laureates. The 40-year journey since has yielded the nascent quantum computing industry and led to breakthroughs like our recently announced verifiable quantum advantage, published on the cover of Nature. This work describes our “Quantum Echoes” algorithm, which runs on our Willow chip 13,000 times faster than the best classical algorithm on one of the world’s fastest supercomputers. It offers a new way to explain interactions between atoms in a molecule observed using nuclear magnetic resonance spectroscopy. It brings us closer to real-world applications of quantum computing, such as advancing drug design and helping to make fusion energy a reality.\n\n      \n      \nWatch the film\nLink to Youtube Video\nAccelerating scientific discovery\nAI-powered models and platforms are fundamentally changing how science is conducted. We released AI co-scientist, a collaboration across Google Research, Cloud AI and Google DeepMind. This multi-agent AI system helps scientists generate novel hypotheses. We also shared our AI-powered empirical software system, a Gemini-backed coding agent to help scientists write expert-level empirical software to evaluate and iterate on hypotheses. These tools accelerate the very process of making scientific discoveries. They open the door to a future where every scientist in a lab has a team of AI assistants simultaneously investigating thousands of potential solutions to the scientific challenges that motivate their research. Already at Stanford, our AI co-scientist has helped identify drugs that could be repurposed to treat liver fibrosis. At Imperial College London, researchers working on antimicrobial resistance found that it produced the same hypothesis in days that their team took years to develop.\nplay silent looping video\n            pause silent looping video\n          \n\n\n\n            \n\n\n\n  \n\n\n  \n\n\n          \nunmute video\n        mute video\n    \n\n            \n\n        \n        \n            \n\n            \n\n        \n    \nAn overview of the AI co-scientist. It uses a coalition of specialized agents who iteratively generate, evaluate, and refine hypotheses.\nAdvancing science — from biology to genomics to neuroscience\nWe continue to advance core scientific research. DeepSomatic and C2S-Scale join the AI-powered fight against cancer and are paving the way for brand-new therapies. Published in Nature Biotechnology, DeepSomatic is an open-source tool that builds on 10 years of genomics research at Google and helps scientists and doctors identify genetic variants in cancer cells. Our partners at Children’s Mercy are using it to understand how and why a particular form of cancer affects a patient in order to develop personalized cures. C2S-Scale, which we released in collaboration with Google DeepMind and Yale, is a 27 billion parameter foundation model for single-cell analysis that made headlines for generating a novel hypothesis about cancer cellular behavior.\nOur public NotebookLMs make our decade of genomics research more accessible and allow people to dive deeper into topics that interest them. Explore \"How do scientists link genetics to health?\" and “How can scientists know what's in your genome?”\nTurning to neuroscience, we published in Nature the first-ever method for using commonly available light microscopes to comprehensively map all the neurons and their connections in a block of brain tissue. Working with the Institute of Science and Technology Austria, we applied our suite of image analysis and ML tools for connectomics, leveraging over a decade of contributions we’ve made to this scientific field to understand the workings of the brain. We hope the method, called LICONN, will enable more labs around the world to pursue connectomics studies.\nWe also open-sourced the Zebrafish Activity Prediction Benchmark (ZAPBench) in collaboration with HHMI Janelia and Harvard. With recordings of more than 70,000 neurons from the larval zebrafish brain, it will enable scientists to investigate the relationship between the structural wiring and dynamic neural activity across an entire vertebrate brain for the first time.\nPlus, we demonstrated how LLMs can help us understand the human brain. In a series of studies conducted over five years with Princeton University, NYU, and HUJI, we explored connections in the ways the human brain and deep language models process natural language. We discovered remarkable alignment between the neural activity in the speech and language areas of the human brain and the speech and language embeddings of a Transformer-based speech-to-text model, and showed how the temporal structure of language processing in the brain corresponds to the layered hierarchy of deep language models. Our research indicates that language representation in deep learning models could offer a novel framework for understanding the brain’s neural code; it also paves the way for innovative approaches to creating artificial neural networks with better information processing capabilities.\nEnabling planetary intelligence and crisis resilience\nEarth AI is Google’s family of geospatial AI models and reasoning agents that provides users with actionable insights, grounded in real-world understanding. Developed in collaboration with teams across Google, it builds on our years of modeling the world, paired with Gemini’s advanced reasoning, to offer an unprecedented level of understanding about our planet. It brings together many of Google’s geospatial models and technologies such as remote sensing imagery, weather, air quality, floods, population dynamics, AlphaEarth Foundations, mobility, maps and more. Thanks to Gemini’s reasoning power, Earth AI can synthesize vast datasets about the planet to generate insights in minutes that would previously take years of research. Earth AI offerings are available in Google Maps Platform, Google Earth and to Trusted Testers via Google Cloud, and are already being used by partners, helping cities, enterprises and nonprofits with critical tasks from urban planning to disaster response.\n\n      \n      \nWatch the film\nLink to Youtube Video\nWe’ve also made significant strides with the climate models that feed our AI capabilities for understanding the Earth, helping communities to prepare for and respond to severe weather and natural disasters. This year, in collaboration with the Earth Fire Alliance, the Moore Foundation and Muon Space, we launched the first satellite in the FireSat constellation. Named as one of TIME magazine’s best inventions of 2025, FireSat uses AI to provide critical near–real-time insights for first responders. It has already detected small wildfires not caught by other space-based systems, and when fully operational with over 50 satellites, it will be able to detect a classroom-sized wildfire anywhere on Earth.\nFireSat is equipped with a custom mid-wave infrared (MWIR) sensor that detected a small, relatively cool roadside fire near Medford, Oregon that was not detected by other space-based systems. It is seen here overlaid on a Google Earth basemap. Credit: Muon Space and Earth Fire Alliance.\nWe also expanded our flood forecasting models to cover over 2 billion people in 150 countries for the most significant riverine flood events, helping communities stay safe and informed. We partnered with our colleagues at Google DeepMind to debut an experimental model for cyclone predictions using stochastic neural networks that's helping weather agencies predict a cyclone’s path up to 15 days in advance. Moreover, we collaborated with Google DeepMind to launch WeatherNext 2, which delivers our most accurate, mid-range AI weather forecasts to date. It’s now available to users of Search, Gemini and Pixel Weather as well as to developers on Google Maps and Google Cloud.\nAt the start of the year, we expanded Nowcasting on Search to Africa, bringing highly precise, short-term precipitation forecasts to users across the continent for the first time. We have since made this available for users worldwide. Powered by our MetNet model, it represents the first AI weather model on Search to operate at a global scale. In India, the University of Chicago and the Indian Ministry of Agriculture and Farmers’ Welfare used Google’s NeuralGCM model to send longer-range monsoon forecasts to 38 million farmers, helping them make critical decisions about what to plant and when.\nAdvancing Health AI\nAs we make scientific breakthroughs with the potential to significantly reform healthcare, we’re working with partners and healthcare professionals to bring new capabilities responsibly to people around the world. AMIE is our conversational medical agent developed together with Google DeepMind and published in Nature. It can now reason through multimodal evidence and support longitudinal disease management as well as or better than primary care physicians under simulated settings with professional patient actors. We’re exploring how this research could enable a physician-centered model with asynchronous oversight of AMIE. We also launched Plan for Care Lab, Fitbit’s latest experimental capability, to a select number of opt-in users. It’s designed to help users access personalized support when assessing symptoms at home and preparing for an upcoming doctor’s visit. In addition, MedGemma, Google's most capable open model for multimodal medical comprehension, is available as part of our Health AI Developer Foundations (HAI-DEF). It can support tasks such as classification, report generation, or interpreting complex electronic health records, making it useful for medical research and product development. Since launch, MedGemma and HAI-DEF have >2M downloads. Plus, our Open Health Stack was recognized at the World Economic Forum for helping to address inequities in health access. It provides the building blocks for developers to create next-gen, data-driven healthcare apps for use in low-resource settings.\nAdvancing learning and education\nGemini is now infused with LearnLM, Google’s family of models fine-tuned for learning, announced last year. We launched Learn Your Way on Google Labs, powered by LearnLM’s foundational capabilities. It explores the future of textbooks by generating multiple engaging representations of the source material. It transforms static textbooks into active learning experiences that are tailored for every student, with interactive quizzes that enable real-time assessment, feedback, and content personalization. In our efficacy study, students using it scored 11 percentage points higher on retention tests. We also piloted our LearnLM model for answer assessment with thousands of high school students in Ghana. Plus, we explored the intersection of education and health through a learner-centric approach quantifying the benefits of LearnLM in medical education settings.\nThis research brings us closer to realizing a future where AI makes learning more effective for everyone. In collaboration with teams across Google, we published “AI and the Future of Learning”, sharing our approach, grounded in learning science, to responsibly enable AI for learning. We’re creating personalized teaching experiences, empowering educators, and working to address challenges such as critical thinking and equal access.\nIn parallel, our AI Literacy efforts aim to inspire the next generation of innovators. AI Quests, launched with the Stanford Accelerator for Learning, allows students to step into the shoes of Google researchers and use AI to solve challenges like flood forecasting and detecting eye disease. During Computer Science Education Week, hundreds of Googler volunteers brought these quests to classrooms around the world.\nplay silent looping video\n            pause silent looping video\n          \n\n\n\n            \n\n\n\n  \n\n\n  \n\n\n          \nunmute video\n        mute video\n    \n\n            \n\n        \n        \n            \n\n            \n\n        \n    \nLearn Your Way explores how GenAI can transform educational materials into more effective, engaging, learner-driven experiences. It generates multiple representations of the source material, tailored for each student.\nAdvancing ML foundations and algorithmic research\nOur broad foundational ML and algorithmic research is the bedrock for groundbreaking advances across domains. This work provides the essential frameworks that power products and services, and underpins the development of next-generation models and intelligent systems. We improved voice search, for example, with our new Speech-to-Retrieval engine, which directly interprets and retrieves information from a spoken query without having to convert it first to text. And our state-of-the-art predictive modeling of rich human feedback improved text-to-image generation quality in products, including Imagen3, creative generation and editing in Google Ads, and virtual try on for shopping. We also extended this research to improve video generation quality in the Wizard of Oz film launch at Sphere in Las Vegas.\nThe impact of our algorithmic research extends well beyond Google products. Our TimesFM model, which helps businesses with time-series forecasting, now has hundreds of millions of queries per month in BigQuery and AlloyDB. We introduced a novel approach using in-context fine-tuning, which teaches the model how to learn from multiple examples at inference time to further enhance its performance. Our Mobility AI model leverages our two decades of innovation in maps and transportation to provide transportation agencies with powerful tools for data-driven policymaking and traffic management. It can understand traffic and parking patterns, simulate systems to allow engineers to test different scenarios, and identify effective solutions for transportation networks. This complements our consumer-facing breakthroughs in Google Maps and Search, such as specialized models for calculating ETAs and optimizing trip planning.\nOur Mobility AI Traffic Simulation API is built for modeling complex, city-scale traffic scenarios. The tool provides high-fidelity simulations of road closures, helping to de-risk major infrastructure investments and validate emergency response plans. It was launched in Seattle, Denver, Boston, Philadelphia and Orlando. The above image shows an example simulation in Seattle. Watch the full demo here.\nAdditionally, we’ve explored a range of topics in economics and computation from pricing dynamics in modular marketplaces and in procurement auctions, to data-driven mechanism design and various approaches to optimize ad auctions. We also studied swap regret and correlated equilibria in games.\nAs AI becomes increasingly integrated into our daily lives, building it with privacy at its core is critical for users and industries. To this end, we’ve developed and published novel algorithms for private learning and private analytics, and open sourced robust software tools to enable external verifiability. For example, we introduced Parfait, a new GitHub organization for businesses and open-source projects. It has supported Google deployments of federated learning and analytics from Gboard to Google Maps. We also announced Jax Privacy 1.0, a library for ML with differential privacy, which we used to train VaultGemma, the largest and most capable open model trained from scratch with differential privacy, with weights available on Hugging Face and Kaggle. By leveling up our privacy capabilities, we offer much stronger protections to businesses and users\nIntroducing novel architectures\nOur foundational ML research introduces advanced approaches to enable new opportunities. Nested Learning is a new ML paradigm that represents a leap forward in our understanding of deep learning. It treats model architecture and optimization as a single system that contains several, smaller, nested optimization problems. By unifying these elements, it solves the problem of catastrophic forgetting, when LLMs become forgetful and less capable at old tasks after learning new tasks. This research could help us build the next generation of more capable, self-improving AI. Meanwhile, our Titans architecture and the MIRAS framework mark a significant advancement in sequence modelling. They allow AI models to work much faster and handle massive contexts by employing deep neural networks that learn to memorize as data comes in, improving AI’s long-term memory.\nIn the MIRAS framework, we aim to learn an associative memory, mapping between keys and values. For each token, the memory module internally optimizes its inner attentional bias while using its retention gate to make sure that it does not deviate from its past state.\nWe also introduced MUVERA, a novel retrieval algorithm that reduces complex multi-vector retrieval back to single-vector maximum inner product search, achieving state-of-the-art performance with significantly improved efficiency. It creates new possibilities for information retrieval for use in applications such as recommendation systems and natural language processing. And our progress on graph foundational models pushes the frontiers of graph learning. While most graph neural networks are fixed to a specific graph on which the model has been trained, we developed graph foundational models capable of generalizing to arbitrary tables, features and tasks. This opens up new avenues for model reuse.\nCollaborating with the research ecosystem\nWe partner with the academic community, industry leaders, governments and scientific institutes around the world. We also continue to engage the ecosystem through our Research@ events from Mountain View to Tokyo, Sydney and Poland, and we support hundreds of PhD students in Google’s Fellowship Program.\nAs a global team, we continue to expand our footprint beyond our major hubs. Having solidified our research investment and innovation in Africa (Accra and Nairobi) and our presence in Australia, we are now preparing to inaugurate a new Google Research hub in Singapore in 2026.\nWe share our work through publications, conferences, academic talks, benchmarks, datasets and open-source releases. We’ve sponsored and hosted workshops at conferences, most recently at NeurIPS. We recently introduced an experimental program that provided automated feedback to scientists before they submit their conference papers for peer review, helping them to rigorously verify their work and accelerate research workflows. Plus, we launched Google Research Featured Notebooks in collaboration with NotebookLM, to make research more accessible to a broader community.\nAI as an amplifier of human ingenuity\nThis is a golden age for research. Never before have technical breakthroughs and scientific progress so quickly materialized into impactful, real-world solutions, which, in turn, bring to the fore new data and questions that inspire new avenues of foundational research. This magic cycle is accelerating significantly, propelled by more powerful models, new agentic tools that support scientific discovery, and open platforms and tools.\nTogether with our Google colleagues and partners, we’re advancing research and technologies that aim to be helpful in diverse areas. Our research, grounded in a rigorous dedication to safety and trust, serves to unlock human potential — whether that’s to help a scientist accelerate their research, or a student learn more effectively and master new concepts, or to empower a doctor, developer or teacher.\nIt is truly an exciting time to be in research. We’re able to leverage the full stack of Google AI infrastructure, models, platforms, and world-class talent, and contribute to products used by billions. We will keep building on our legacy, asking the biggest questions of today, and aiming to enable the solutions of tomorrow. We’ll keep advancing AI in a bold and responsible way, for the benefit of society, to help enhance human capacity and make AI an amplifier of human ingenuity.\n\n      \n      \nWatch the film\nLink to Youtube Video\nAcknowledgements\nWith thanks to everyone in Google Research, and many collaborators, who have contributed to this blog and the work represented here.",
      "creator": "Google",
      "summary": "谷歌研究2025年取得重大突破，推动生成式模型更高效、准确、多语言、跨文化，并引入生成式UI。在AI、量子计算、地球科学、基因组学等领域取得进展，助力科学发现和社会发展。研究团队加速创新周期，将突破转化为实际应用，与全球伙伴紧密合作。"
    },
    {
      "title": "Gemini provides automated feedback for theoretical computer scientists at STOC 2026",
      "link": "https://research.google/blog/gemini-provides-automated-feedback-for-theoretical-computer-scientists-at-stoc-2026/",
      "pubDate": "Sun, 14 Dec 2025 16:00:00 GMT",
      "isoDate": "2025-12-14T16:00:00.000Z",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"o2vbn\">The pursuit of truth in theoretical computer science and mathematics relies on the highest standards of proof, rigor, and clarity. While peer review is the crucial final check, the process of drafting and refining complex theoretical work often takes months, with simple errors, inconsistent variables, or subtle logical gaps frequently slowing down the entire research pipeline. But could a highly specialized AI tool act as a fast, rigorous collaborator, helping authors pre-vet their work before it ever reaches human reviewers?</p><p data-block-key=\"bjaqs\">To test this potential, we created an <a href=\"https://acm-stoc.org/stoc2026/stoc2026-LLM_feedback.html\" target=\"_blank\" rel=\"noopener noreferrer\">experimental program</a> for the <a href=\"https://acm-stoc.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Annual ACM Symposium on Theory of Computing</a> (STOC 2026) — one of the most prestigious venues in theoretical computer science. This program offered authors automated, pre-submission feedback generated by a specialized Gemini AI tool. Our objective was to provide constructive suggestions and identify potential technical issues within 24 hours of submission, helping authors polish their final drafts before the submission deadline.</p><p data-block-key=\"3naog\">The responses were very positive: the tool successfully identified a variety of issues, including calculation and logic errors. Here we report how we developed the tool and the results of its use.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Optimized for mathematical rigor\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Optimized for mathematical rigor</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o2vbn\">The feedback tool leveraged inference scaling methods in an advanced version of <a href=\"https://blog.google/products/gemini/gemini-2-5-deep-think/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 2.5 Deep Think</a>. This setup enables the method to simultaneously explore and combine multiple possible solutions before giving a final answer, rather than pursuing a single, linear chain of thought. By combining different reasoning and evaluation traces, the method reduces inherent hallucinations and focuses on the most salient issues.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Feedback format\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Feedback format</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o2vbn\">Authors received structured feedback divided into key sections: a summary of the paper's contributions, a list of potential mistakes and improvements (often analyzing specific lemmas or theorems), and a list of minor corrections and typos. See some <a href=\"https://www.cs.cmu.edu/~dwoodruf/stoc/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">feedback examples</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Impact and technical depth\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Impact and technical depth</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o2vbn\">The tool successfully identified a wide range of issues, from inconsistent variable names to complex problems like calculation errors, incorrect application of inequalities, and logical gaps in proofs. As one author noted, the tool found \"a critical bug... that made our proof entirely incorrect,\" further adding that it was an \"embarrassingly simple bug that evaded us for months.\"</p><p data-block-key=\"eo65d\">Over 120 participants responded to our post-experiment survey and gave us consent, and the responses were very positive, with individuals citing the model’s success at finding critical errors and its ability to return insightful commentary. In summary:</p><ul><li data-block-key=\"qpe5\">&gt;80% of submitted papers at the time our experiment ended had opted-in for our AI review</li><li data-block-key=\"ev3be\">97% found the feedback helpful</li><li data-block-key=\"dkstm\">97% would use this tool again for future submissions</li><li data-block-key=\"a51g7\">81% found the model improved clarity or readability of the paper</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/STOC-2-Quotes2.width-1250.png\" alt=\"Three testimonials from university professors praising the tool for identifying significant errors and bugs in their research papers.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/STOC-2-Quotes2.width-1250.png\" alt=\"Three testimonials from university professors praising the tool for identifying significant errors and bugs in their research papers.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"The user experience\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The user experience</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o2vbn\">Beyond technical accuracy, authors valued the speed and neutrality of the AI review. Participants noted receiving feedback in just two days. Others praised the \"neutral tone and rigor\" of the output, finding it a useful complement to human readers.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/STOC-1-Quotes2.width-1250.png\" alt=\"A testimonial graphic from Professor Shuchi Chawla praising the tool for providing valuable feedback that exceeded expectations.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/STOC-1-Quotes2.width-1250.png\" alt=\"A testimonial graphic from Professor Shuchi Chawla praising the tool for providing valuable feedback that exceeded expectations.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Interpreting the output\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Interpreting the output</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o2vbn\">Because participants are experts in their respective fields, they were able to readily distinguish helpful insights from occasional \"hallucinations\". While the model sometimes struggled — particularly with parsing complex notation or interpreting figures — authors weren't dismissive of the LLM's output. Rather, they carefully filtered out the noise and extracted the important and correct parts of the output, and then used the feedback as a starting point for verification. This outcome clearly demonstrates the potential for AI to serve as a collaborative partner, augmenting the research workflow by helping human experts to make informed decisions based on the model's rigorous outputs.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Educational impact and future outlook\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Educational impact and future outlook</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o2vbn\">The research community surveyed in this experiment saw significant potential for this tool in training the next generation. 75% of surveyed authors believed the tool has educational value for students by offering immediate feedback on mathematical rigor and presentation clarity.</p><p data-block-key=\"629om\">This pilot demonstrated the potential for specialized AI tools to serve as collaborative partners in fundamental areas, establishing a target for potential future research initiatives. Our overall goal is not to replace the critical peer review process, but rather to augment and enhance it. Reflecting this, 88% of participants expressed strong interest in having continuous access to such a tool throughout their entire research process.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Acknowledgements\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o2vbn\"><i>Vincent Cohen-Addad, Rajesh Jayaram, Jon Schneider, and David Woodruff co-led this project</i><footnote id=\"8746db74-de5b-4496-adf4-df5aa8c2d3ab\">[8746db]</footnote><i>, with key contributions by Lalit Jain, Jieming Mao, and Vahab Mirrokni. We also thank the STOC 2026 PC chair Artur Czumaj and the many other authors who participated in this experiment and provided their valuable feedback, helpful suggestions, and discussions, including Mohammad Taghi Hajiaghayi, Ravi Kumar, Yossi Matias, and Sergei Vassilvitskii. Finally, this work builds on the efforts of the Deep Think team: Garrett Bingham, Irene Cai, Heng-Tze Cheng, Yong Cheng, Kristen Chiafullo, Vincent Cohen-Addad, Paul Covington, Golnaz Ghiasi, Chenjie Gu, Huan Gui, Ana Hosseini, Dawsen Hwang, Lalit Jain, Vihan Jain, Ragha Kotikalapudi, Chenkai Kuang, Chenkai Kuang, Maciej Kula, Nate Kushman, Jane Labanowski, Quoc Le, Jonathan Lee, Zhaoqi Leng, Steve Li, YaGuang Li, Hanzhao (Maggie) Lin, Evan Liu, Yuan Liu, Thang Luong, Jieming Mao, Vahab Mirrokni, Pol Moreno, Nigamaa Nayakanti, Aroonalok Pyne, Shubha Raghvendra, Sashank Reddi, Nikunj Saunshi, Siamak Shakeri, Archit Sharma, Xinying Song, Qijun Tan, Yi Tay, Trieu Trinh, Theophane Weber, Winnie Xu, Zicheng Xu, Shunyu Yao, Lijun Yu, Hao Zhou, Honglei Zhuang, and Song Zuo.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                ",
      "contentSnippet": "The pursuit of truth in theoretical computer science and mathematics relies on the highest standards of proof, rigor, and clarity. While peer review is the crucial final check, the process of drafting and refining complex theoretical work often takes months, with simple errors, inconsistent variables, or subtle logical gaps frequently slowing down the entire research pipeline. But could a highly specialized AI tool act as a fast, rigorous collaborator, helping authors pre-vet their work before it ever reaches human reviewers?\nTo test this potential, we created an experimental program for the Annual ACM Symposium on Theory of Computing (STOC 2026) — one of the most prestigious venues in theoretical computer science. This program offered authors automated, pre-submission feedback generated by a specialized Gemini AI tool. Our objective was to provide constructive suggestions and identify potential technical issues within 24 hours of submission, helping authors polish their final drafts before the submission deadline.\nThe responses were very positive: the tool successfully identified a variety of issues, including calculation and logic errors. Here we report how we developed the tool and the results of its use.\nOptimized for mathematical rigor\nThe feedback tool leveraged inference scaling methods in an advanced version of Gemini 2.5 Deep Think. This setup enables the method to simultaneously explore and combine multiple possible solutions before giving a final answer, rather than pursuing a single, linear chain of thought. By combining different reasoning and evaluation traces, the method reduces inherent hallucinations and focuses on the most salient issues.\nFeedback format\nAuthors received structured feedback divided into key sections: a summary of the paper's contributions, a list of potential mistakes and improvements (often analyzing specific lemmas or theorems), and a list of minor corrections and typos. See some feedback examples.\nImpact and technical depth\nThe tool successfully identified a wide range of issues, from inconsistent variable names to complex problems like calculation errors, incorrect application of inequalities, and logical gaps in proofs. As one author noted, the tool found \"a critical bug... that made our proof entirely incorrect,\" further adding that it was an \"embarrassingly simple bug that evaded us for months.\"\nOver 120 participants responded to our post-experiment survey and gave us consent, and the responses were very positive, with individuals citing the model’s success at finding critical errors and its ability to return insightful commentary. In summary:\n\n>80% of submitted papers at the time our experiment ended had opted-in for our AI review\n97% found the feedback helpful\n97% would use this tool again for future submissions\n81% found the model improved clarity or readability of the paper\n\n\n\n    \nThe user experience\nBeyond technical accuracy, authors valued the speed and neutrality of the AI review. Participants noted receiving feedback in just two days. Others praised the \"neutral tone and rigor\" of the output, finding it a useful complement to human readers.\nInterpreting the output\nBecause participants are experts in their respective fields, they were able to readily distinguish helpful insights from occasional \"hallucinations\". While the model sometimes struggled — particularly with parsing complex notation or interpreting figures — authors weren't dismissive of the LLM's output. Rather, they carefully filtered out the noise and extracted the important and correct parts of the output, and then used the feedback as a starting point for verification. This outcome clearly demonstrates the potential for AI to serve as a collaborative partner, augmenting the research workflow by helping human experts to make informed decisions based on the model's rigorous outputs.\nEducational impact and future outlook\nThe research community surveyed in this experiment saw significant potential for this tool in training the next generation. 75% of surveyed authors believed the tool has educational value for students by offering immediate feedback on mathematical rigor and presentation clarity.\nThis pilot demonstrated the potential for specialized AI tools to serve as collaborative partners in fundamental areas, establishing a target for potential future research initiatives. Our overall goal is not to replace the critical peer review process, but rather to augment and enhance it. Reflecting this, 88% of participants expressed strong interest in having continuous access to such a tool throughout their entire research process.\nAcknowledgements\nVincent Cohen-Addad, Rajesh Jayaram, Jon Schneider, and David Woodruff co-led this project[8746db], with key contributions by Lalit Jain, Jieming Mao, and Vahab Mirrokni. We also thank the STOC 2026 PC chair Artur Czumaj and the many other authors who participated in this experiment and provided their valuable feedback, helpful suggestions, and discussions, including Mohammad Taghi Hajiaghayi, Ravi Kumar, Yossi Matias, and Sergei Vassilvitskii. Finally, this work builds on the efforts of the Deep Think team: Garrett Bingham, Irene Cai, Heng-Tze Cheng, Yong Cheng, Kristen Chiafullo, Vincent Cohen-Addad, Paul Covington, Golnaz Ghiasi, Chenjie Gu, Huan Gui, Ana Hosseini, Dawsen Hwang, Lalit Jain, Vihan Jain, Ragha Kotikalapudi, Chenkai Kuang, Chenkai Kuang, Maciej Kula, Nate Kushman, Jane Labanowski, Quoc Le, Jonathan Lee, Zhaoqi Leng, Steve Li, YaGuang Li, Hanzhao (Maggie) Lin, Evan Liu, Yuan Liu, Thang Luong, Jieming Mao, Vahab Mirrokni, Pol Moreno, Nigamaa Nayakanti, Aroonalok Pyne, Shubha Raghvendra, Sashank Reddi, Nikunj Saunshi, Siamak Shakeri, Archit Sharma, Xinying Song, Qijun Tan, Yi Tay, Trieu Trinh, Theophane Weber, Winnie Xu, Zicheng Xu, Shunyu Yao, Lijun Yu, Hao Zhou, Honglei Zhuang, and Song Zuo.",
      "creator": "Google",
      "summary": "Gemini为理论计算机科学家在STOC 2026上提供自动化反馈。该工具利用高级版Gemini 2.5 Deep Think的推理扩展方法，在提交前24小时内为作者提供结构化反馈，涵盖论文贡献总结、潜在错误和改进建议等。实验显示，该工具能有效识别计算、逻辑错误等问题，97%的参与者认为反馈有帮助，81%认为其提升了论文清晰度。作者普遍赞赏其速度和中立性，并能区分有用建议与“幻觉”输出。"
    },
    {
      "title": "Spotlight on innovation: Google-sponsored Data Science for Health Ideathon across Africa",
      "link": "https://research.google/blog/spotlight-on-innovation-google-sponsored-data-science-for-health-ideathon-across-africa/",
      "pubDate": "Thu, 11 Dec 2025 16:00:00 GMT",
      "isoDate": "2025-12-11T16:00:00.000Z",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"uoa5v\">Generative AI is rapidly changing healthcare, opening up opportunities to better address real-world health challenges. Across the African continent, there is broad interest in tackling such challenges, ranging from cervical cancer screening to maternal health support.</p><p data-block-key=\"93mda\">With the above in mind, in collaboration with three pan-African data science and machine learning communities — <a href=\"https://www.linkedin.com/company/sisonkebiotik/posts/?feedView=all\" target=\"_blank\" rel=\"noopener noreferrer\">SisonkeBiotik</a>, <a href=\"https://ro-ya-cv4africa.github.io/homepage/\" target=\"_blank\" rel=\"noopener noreferrer\">Ro’ya</a>, and <a href=\"https://dsi-africa.org/#carouselWithCaptions\" target=\"_blank\" rel=\"noopener noreferrer\">DS-I Africa</a> — we hosted an Africa-wide Data Science for Health Ideathon, focused on leveraging <a href=\"https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/\">Google’s open Health AI models</a> to tackle real-world health challenges.</p><p data-block-key=\"ai2pi\">From over 30 ambitious and technically rich submissions, six finalist teams were selected for their bold ideas, strong foundations, and potential for meaningful impact across African health systems. These teams received mentorship from global experts and technical resources provided by Google Research and Google DeepMind.</p><p data-block-key=\"8oqa5\">This reflects broad interest across the African continent on understanding how AI can be used to create local solutions to local priorities around health, agriculture and climate. This is part of Google’s broader <a href=\"https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/supporting-the-future-of-ai-research-in-africa-and-globally/\" target=\"_blank\" rel=\"noopener noreferrer\">initiatives</a> on AI for Africa including in areas of <a href=\"https://research.google/blog/afrimed-qa-benchmarking-large-language-models-for-global-health/\">health</a>, <a href=\"https://blog.google/intl/en-africa/products/explore-get-answers/bringing-googles-best-ai-tools-to-university-students-across-africa-at-no-cost/\" target=\"_blank\" rel=\"noopener noreferrer\">education</a>, <a href=\"https://blog.google/outreach-initiatives/google-org/ai-collaboratives-wildfires-food-security/\" target=\"_blank\" rel=\"noopener noreferrer\">food security</a>, <a href=\"https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/google-helps-africa-build-an-ai-data-future-with-225m-in-support/\" target=\"_blank\" rel=\"noopener noreferrer\">infrastructure</a> and <a href=\"https://www.youtube.com/watch?v=JzXhGUhL99c&amp;t=2s\" target=\"_blank\" rel=\"noopener noreferrer\">languages</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uoa5v\">A new model for collaborative health innovation</h2><p data-block-key=\"4shq1\">An <i>ideathon</i>, much like a hackathon, is a platform for interdisciplinary teams to design solutions to crucial challenges, valuing the idea creativity as much as its execution.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Ideathon1_Group.width-1250.png\" alt=\"Ideathon1_Group\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Ideathon1_Group.width-1250.png\" alt=\"Ideathon1_Group\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"j7h2b\"><i>The launch of the Ideathon took place at the 2025 Deep Learning Indaba conference in Kigali, Rwanda.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"uoa5v\">The Ideathon was launched during the <a href=\"https://ds4healthafrica.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">2025 Data Science for Health in Africa Workshop</a> at the <a href=\"https://deeplearningindaba.com/2025/workshops/\" target=\"_blank\" rel=\"noopener noreferrer\">Deep Learning Indaba</a>, the gathering of Africa’s machine learning (ML) and AI community, dedicated to strengthening and celebrating African AI, which took place in Kigali, Rwanda. The Ideathon built on the shared vision of building capacity across African AI and health communities. Participants also had the opportunity at the workshop to attend a hands-on <a href=\"http://goo.gle/medgemma\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma</a> tutorial led by Dr. Sekou Remy and Dr. Mercy Asiedu, research scientists at Google Research.</p><p data-block-key=\"focpb\">Teams were encouraged to explore Google’s open health AI models, namely: <a href=\"https://deepmind.google/models/gemma/medgemma/\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma</a>, <a href=\"https://deepmind.google/models/gemma/txgemma/\" target=\"_blank\" rel=\"noopener noreferrer\">TxGemma</a>, and <a href=\"https://developers.google.com/health-ai-developer-foundations/medsiglip\" target=\"_blank\" rel=\"noopener noreferrer\">MedSigLIP</a>, and apply these models to healthcare challenges ranging from diagnostics to policy frameworks. Submissions were judged on innovation, feasibility, contextual relevance, and the creative use of Google’s AI tools in African settings.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uoa5v\">From idea to impact: A two-phase journey</h2><p data-block-key=\"b5jnq\">The Ideathon unfolded in two phases:</p><ol><li data-block-key=\"44q2r\"><i>Idea development</i>: Teams introduced their members, defined healthcare problems, and outlined AI-driven approaches using Google’s models. Selected teams received mentorship, Google Cloud Vertex AI compute credits, and technical documentation to refine their ideas.</li><li data-block-key=\"17qch\"><i>Prototype &amp; pitch</i>: Teams submitted demo videos showcasing their solutions, which were then reviewed by a panel of expert judges who chose six finalists for the final, live pitch.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uoa5v\">The final pitch: Creativity meets purpose</h2><p data-block-key=\"c1icv\">Six finalist teams presented their solutions during the <a href=\"https://cassyni.com/events/Dun95er1wKLsGCL3jB9Zxh\" target=\"_blank\" rel=\"noopener noreferrer\">DS4H Ideathon Live Pitch</a>, broadcast globally, followed by a team Q&amp;A session with the judges. The event emphasized not only technical excellence but also vision, collaboration, and contextual innovation in African healthcare.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uoa5v\">Winning teams and awards</h2><p data-block-key=\"5jose\">Here are the winning projects that showcase how Google's open models can serve as a foundation on which developers may build and help solve pressing healthcare challenges in Africa:</p><h3 data-block-key=\"gr1h\">First place and Audience Choice Award: Dawa Health – <i>AI-powered multilingual cervical-cancer education &amp; screening tool</i></h3><p data-block-key=\"257vk\">Built on <a href=\"https://developers.google.com/health-ai-developer-foundations/medsiglip\" target=\"_blank\" rel=\"noopener noreferrer\">MedSigLIP</a> and enhanced with <a href=\"https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/rag-overview\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini RAG</a>. Midwives first upload colposcopy images via WhatsApp. The MedSigLIP-based classifier then identifies abnormalities indicating precancer/cancer in real time, while Gemini provides contextual clinical guidance using <a href=\"https://www.who.int/\" target=\"_blank\" rel=\"noopener noreferrer\">WHO</a> and Zambian protocols [<a href=\"https://drive.google.com/file/d/1hSPelTCuLHUIBgwak0ywFeFGAj0XKeDv/view\" target=\"_blank\" rel=\"noopener noreferrer\">1</a>, <a href=\"https://dspace.unza.zm/server/api/core/bitstreams/8a2bfed0-ff6d-46b6-af75-8f09af0e7f7d/content\" target=\"_blank\" rel=\"noopener noreferrer\">2</a>].</p><p data-block-key=\"cnqpr\"><i>Team: Tafadzwa Munzwa, Khanyisile Magagula, Kudzal Mwedzi, Tariro Munzwa.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Ideathon2_DawaHealth.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"j7h2b\"><i>The winning prototype: Dawa Health’s Innovative solution for cervical cancer screening building on MedSigLIP and Gemini RAG.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"uoa5v\">Second place: Solver – <i>CerviScreen AI</i></h3><p data-block-key=\"ct0gj\">A <a href=\"https://fastapi.tiangolo.com/\" target=\"_blank\" rel=\"noopener noreferrer\">FastAPI</a> web app for automated cervical-cytology screening based on MedGemma-27B-IT, fine-tuned with <a href=\"https://arxiv.org/abs/2106.09685\" target=\"_blank\" rel=\"noopener noreferrer\">LoRA</a> on the <a href=\"https://bids.berkeley.edu/cric-cervix-collection\" target=\"_blank\" rel=\"noopener noreferrer\">CRIC dataset</a>. Outputs annotated images and concise clinical recommendations to assist cytopathologists.</p><p data-block-key=\"73lgh\"><i>Team: Bonaventure F.P. Dossou, Ariane Houetohossou, Aurel Tchokponhoue, Ghilith Gbaguidi.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Ideathon3_Screening.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"j7h2b\"><i>Solver Team building on MedGemma-27B-IT for Cervical-Cytology Screening.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"uoa5v\">Third place: Mkunga – <i>Maternal AI call center</i></h3><p data-block-key=\"11b9u\">Adapting MedGemma and Gemini to provide maternal health advice in Swahili using <a href=\"https://docs.cloud.google.com/text-to-speech/docs/gemini-tts\" target=\"_blank\" rel=\"noopener noreferrer\">TTS/STT</a>, deployed on <a href=\"https://cloud.google.com/vertex-ai?gclsrc=aw.ds&amp;gad_source=1&amp;gad_campaignid=23058943583&amp;gclid=Cj0KCQiAi9rJBhCYARIsALyPDtvxBdsKqiFBW0_MWAc326EpZyKVMgk5GAQZ5pG2adiVGHJyDhqvrfYaArJ1EALw_wcB\" target=\"_blank\" rel=\"noopener noreferrer\">Vertex AI</a>. Aims to scale as a low-cost telehealth assistant.</p><p data-block-key=\"cd7nu\"><i>Team: Malik Lanlokun, Beryl Apondi, Janet Njiru, Joseph Wacira.</i></p><h3 data-block-key=\"ablpm\">Best proof-of-concept (offline / low-connectivity): HexAI – <i>DermaDetect</i></h3><p data-block-key=\"b2mj3\">Offline-first mobile app that enables community health workers to triage skin conditions using adapted versions of on-device MedSigLIP and cloud-based MedGemma for advanced analysis, creating a “data flywheel” for continuous improvement.</p><p data-block-key=\"14eku\"><i>Team: Ebrima S Jallow, Omar Keita.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Ideathon4_HexAI.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"j7h2b\"><i>HexAI building on MedSigLIP and MedGemma for dermatological condition triage.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"uoa5v\">Most fun solution: MamaLens Lab</h3><p data-block-key=\"877en\">Multilingual offline Android assistant for community health workers, adapting MedGemma, MedSigLIP, and <a href=\"https://developers.googleblog.com/en/introducing-txgemma-open-models-improving-therapeutics-development/\" target=\"_blank\" rel=\"noopener noreferrer\">TxGemma</a> to assess pregnancy risk in English and Yoruba.</p><p data-block-key=\"7mhpo\"><i>Team: Ilerioluwakiiye Abolade, Faith Nchifor, Toyibat Adele, Simeon Krah.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uoa5v\">What comes next</h2><p data-block-key=\"f3k3k\">The end of the Ideathon marks the beginning of sustained development and scaling. Several finalist teams are exploring ways to scale and deploy their solutions, supported by networks built during the program. The winning team, Dawa Health, is currently piloting an early access program for cervical cancer screening using a MedSigLIP-based classifier with plans to scale to 50,000 patients next year, effectively demonstrating scalable, real world impact.</p><p data-block-key=\"9rise\">To learn more about Google's open models and start exploring them for your own use cases, visit our <a href=\"http://goo.gle/hai-def\" target=\"_blank\" rel=\"noopener noreferrer\">HAI-DEF site</a>. We welcome your <a href=\"https://services.google.com/fb/forms/hai-def-feedback\" target=\"_blank\" rel=\"noopener noreferrer\">feedback</a> on the models and your use cases.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uoa5v\">Acknowledgements</h2><p data-block-key=\"7bked\"><i>This initiative was made possible through the vision, collaboration, and funding support of the organizing team and partners: Comfort Adesina (Ideathon Chair, SisonkeBiotik), Abraham Owodunni (Technical Coordinator, SisonkeBiotik), Taliya Weinstein (Ideathon Host and Advisor, SisonkeBiotik), Ashery Mbilinyi (Ideathon Judge), Lukman Ismaila (Ideathon Judge), and Google Research and DeepMind: Mercy Asiedu, Sekou L. Remy, Sunny Jansen, Fereshteh Mahvar, Tiffany Chen,</i> <i>Yun Liu, Katherine Heller, Joelle Barrel, Richa Tiwari, Sunny Virmani.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                ",
      "contentSnippet": "Generative AI is rapidly changing healthcare, opening up opportunities to better address real-world health challenges. Across the African continent, there is broad interest in tackling such challenges, ranging from cervical cancer screening to maternal health support.\nWith the above in mind, in collaboration with three pan-African data science and machine learning communities — SisonkeBiotik, Ro’ya, and DS-I Africa — we hosted an Africa-wide Data Science for Health Ideathon, focused on leveraging Google’s open Health AI models to tackle real-world health challenges.\nFrom over 30 ambitious and technically rich submissions, six finalist teams were selected for their bold ideas, strong foundations, and potential for meaningful impact across African health systems. These teams received mentorship from global experts and technical resources provided by Google Research and Google DeepMind.\nThis reflects broad interest across the African continent on understanding how AI can be used to create local solutions to local priorities around health, agriculture and climate. This is part of Google’s broader initiatives on AI for Africa including in areas of health, education, food security, infrastructure and languages.\nA new model for collaborative health innovation\nAn ideathon, much like a hackathon, is a platform for interdisciplinary teams to design solutions to crucial challenges, valuing the idea creativity as much as its execution.\nThe launch of the Ideathon took place at the 2025 Deep Learning Indaba conference in Kigali, Rwanda.\nThe Ideathon was launched during the 2025 Data Science for Health in Africa Workshop at the Deep Learning Indaba, the gathering of Africa’s machine learning (ML) and AI community, dedicated to strengthening and celebrating African AI, which took place in Kigali, Rwanda. The Ideathon built on the shared vision of building capacity across African AI and health communities. Participants also had the opportunity at the workshop to attend a hands-on MedGemma tutorial led by Dr. Sekou Remy and Dr. Mercy Asiedu, research scientists at Google Research.\nTeams were encouraged to explore Google’s open health AI models, namely: MedGemma, TxGemma, and MedSigLIP, and apply these models to healthcare challenges ranging from diagnostics to policy frameworks. Submissions were judged on innovation, feasibility, contextual relevance, and the creative use of Google’s AI tools in African settings.\nFrom idea to impact: A two-phase journey\nThe Ideathon unfolded in two phases:\n\nIdea development: Teams introduced their members, defined healthcare problems, and outlined AI-driven approaches using Google’s models. Selected teams received mentorship, Google Cloud Vertex AI compute credits, and technical documentation to refine their ideas.\nPrototype & pitch: Teams submitted demo videos showcasing their solutions, which were then reviewed by a panel of expert judges who chose six finalists for the final, live pitch.\n\n\n\n    \nThe final pitch: Creativity meets purpose\nSix finalist teams presented their solutions during the DS4H Ideathon Live Pitch, broadcast globally, followed by a team Q&A session with the judges. The event emphasized not only technical excellence but also vision, collaboration, and contextual innovation in African healthcare.\nWinning teams and awards\nHere are the winning projects that showcase how Google's open models can serve as a foundation on which developers may build and help solve pressing healthcare challenges in Africa:\nFirst place and Audience Choice Award: Dawa Health – AI-powered multilingual cervical-cancer education & screening tool\nBuilt on MedSigLIP and enhanced with Gemini RAG. Midwives first upload colposcopy images via WhatsApp. The MedSigLIP-based classifier then identifies abnormalities indicating precancer/cancer in real time, while Gemini provides contextual clinical guidance using WHO and Zambian protocols [1, 2].\nTeam: Tafadzwa Munzwa, Khanyisile Magagula, Kudzal Mwedzi, Tariro Munzwa.\nplay silent looping video\n            pause silent looping video\n          \n\n\n\n            \n\n\n\n  \n\n\n  \n\n\n          \nunmute video\n        mute video\n    \n\n            \n\n        \n        \n            \n\n            \n\n        \n    \nThe winning prototype: Dawa Health’s Innovative solution for cervical cancer screening building on MedSigLIP and Gemini RAG.\nSecond place: Solver – CerviScreen AI\nA FastAPI web app for automated cervical-cytology screening based on MedGemma-27B-IT, fine-tuned with LoRA on the CRIC dataset. Outputs annotated images and concise clinical recommendations to assist cytopathologists.\nTeam: Bonaventure F.P. Dossou, Ariane Houetohossou, Aurel Tchokponhoue, Ghilith Gbaguidi.\nplay silent looping video\n            pause silent looping video\n          \n\n\n\n            \n\n\n\n  \n\n\n  \n\n\n          \nunmute video\n        mute video\n    \n\n            \n\n        \n        \n            \n\n            \n\n        \n    \nSolver Team building on MedGemma-27B-IT for Cervical-Cytology Screening.\nThird place: Mkunga – Maternal AI call center\nAdapting MedGemma and Gemini to provide maternal health advice in Swahili using TTS/STT, deployed on Vertex AI. Aims to scale as a low-cost telehealth assistant.\nTeam: Malik Lanlokun, Beryl Apondi, Janet Njiru, Joseph Wacira.\nBest proof-of-concept (offline / low-connectivity): HexAI – DermaDetect\nOffline-first mobile app that enables community health workers to triage skin conditions using adapted versions of on-device MedSigLIP and cloud-based MedGemma for advanced analysis, creating a “data flywheel” for continuous improvement.\nTeam: Ebrima S Jallow, Omar Keita.\nplay silent looping video\n            pause silent looping video\n          \n\n\n\n            \n\n\n\n  \n\n\n  \n\n\n          \nunmute video\n        mute video\n    \n\n            \n\n        \n        \n            \n\n            \n\n        \n    \nHexAI building on MedSigLIP and MedGemma for dermatological condition triage.\nMost fun solution: MamaLens Lab\nMultilingual offline Android assistant for community health workers, adapting MedGemma, MedSigLIP, and TxGemma to assess pregnancy risk in English and Yoruba.\nTeam: Ilerioluwakiiye Abolade, Faith Nchifor, Toyibat Adele, Simeon Krah.\nWhat comes next\nThe end of the Ideathon marks the beginning of sustained development and scaling. Several finalist teams are exploring ways to scale and deploy their solutions, supported by networks built during the program. The winning team, Dawa Health, is currently piloting an early access program for cervical cancer screening using a MedSigLIP-based classifier with plans to scale to 50,000 patients next year, effectively demonstrating scalable, real world impact.\nTo learn more about Google's open models and start exploring them for your own use cases, visit our HAI-DEF site. We welcome your feedback on the models and your use cases.\nAcknowledgements\nThis initiative was made possible through the vision, collaboration, and funding support of the organizing team and partners: Comfort Adesina (Ideathon Chair, SisonkeBiotik), Abraham Owodunni (Technical Coordinator, SisonkeBiotik), Taliya Weinstein (Ideathon Host and Advisor, SisonkeBiotik), Ashery Mbilinyi (Ideathon Judge), Lukman Ismaila (Ideathon Judge), and Google Research and DeepMind: Mercy Asiedu, Sekou L. Remy, Sunny Jansen, Fereshteh Mahvar, Tiffany Chen, Yun Liu, Katherine Heller, Joelle Barrel, Richa Tiwari, Sunny Virmani.",
      "creator": "Google",
      "summary": "谷歌赞助的非洲健康数据科学创想营聚焦于利用其开放医疗AI模型解决非洲现实健康挑战。活动由谷歌与三个非洲数据科学和机器学习社群合作举办，吸引了30多个团队提交方案，最终选出六支优胜团队，获得专家指导和谷歌技术支持。创想营旨在推动AI在非洲医疗、农业和气候领域的本地化创新，是谷歌AI为非洲发展计划的一部分。活动于2025年卢旺达基加利深度学习印达巴会议期间启动，参赛团队探索了谷歌的MedGemma、TxGemma和MedSigLIP模型，针对诊断和政策框架等挑战提出解决方案。"
    },
    {
      "title": "A differentially private framework for gaining insights into AI chatbot use",
      "link": "https://research.google/blog/a-differentially-private-framework-for-gaining-insights-into-ai-chatbot-use/",
      "pubDate": "Tue, 09 Dec 2025 16:00:00 GMT",
      "isoDate": "2025-12-09T16:00:00.000Z",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"k5pub\">Large language model (LLM) chatbots are used by hundreds of millions of people daily for tasks ranging from drafting emails and writing code to planning vacations and creating menus for cafes. Understanding these high-level use cases is incredibly valuable for platform providers looking to improve services or enforce safety policies. It also offers the public insights into how AI is shaping our world.</p><p data-block-key=\"ergf9\">But this raises a critical question: How can we gain valuable insights when the conversations themselves might contain private or sensitive information?</p><p data-block-key=\"d3h83\">Existing approaches, like the <a href=\"https://www.anthropic.com/research/clio\" target=\"_blank\" rel=\"noopener noreferrer\">CLIO</a> framework, attempt to solve this by using an LLM to summarize conversations while prompting it to strip out <a href=\"https://en.wikipedia.org/wiki/Personal_data\" target=\"_blank\" rel=\"noopener noreferrer\">personally identifiable information</a> (PII). While a good first step, this method relies on heuristic privacy protections. The resulting privacy guarantee is difficult to formalize and may not hold up as models evolve, making these systems difficult to maintain and audit. This limitation led us to ask if it is possible to achieve similar utility with formal, end-to-end privacy guarantees.</p><p data-block-key=\"7hpa6\">In our paper, \"<a href=\"https://arxiv.org/abs/2506.04681\" target=\"_blank\" rel=\"noopener noreferrer\">Urania: Differentially Private Insights into AI Use</a>,\" presented at <a href=\"https://colmweb.org/\" target=\"_blank\" rel=\"noopener noreferrer\">COLM 2025</a>, we introduce a new framework that generates insights from LLM chatbot interactions with rigorous <a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">differential privacy</a> (DP) guarantees. This framework uses a <a href=\"https://research.google/blog/practical-differentially-private-clustering/\">DP clustering algorithm</a> and keyword extraction method to ensure that no single conversation overly influences the result (i.e., the output summaries do not reveal information about any single individual's conversation). Here we explain the algorithm and demonstrate that this framework is indeed providing better privacy guarantees than prior solutions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_hero1.width-1250.jpg\" alt=\"Gaining_insights_into_AI_chatbot_use_hero1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_hero1.width-1250.jpg\" alt=\"Gaining_insights_into_AI_chatbot_use_hero1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"qlcu8\"><i>Gemini-generated image showing schematically how the algorithm works for one cluster of conversations.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Privacy-preserving framework for insights mining</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">DP uses a privacy budget parameter, ε, to measure the maximum allowed influence of any single user's contributions to the final output of a model. Our framework is designed to rely on two key properties of DP:</p><ol><li data-block-key=\"410n7\"><i>Post-processing:</i> If <i>B</i> is an <i>ε</i>-DP algorithm and <i>A</i> is any non-DP algorithm, then running <i>A</i> on the output of <i>B</i> keeps things private at a <i>ε</i>-DP level.</li><li data-block-key=\"auflb\"><i>Composition:</i> If <i>A</i> and <i>B</i> are two separate <i>ε</i>-DP algorithms, running <i>A</i> on the <i>dataset</i> and the output of <i>B</i> still keeps the entire process private at a 2<i>ε</i>-DP level.</li></ol><p data-block-key=\"9pfmg\">This differentially private pipeline is designed to ensure end-to-end user data protection through the following stages:</p><ol><li data-block-key=\"feaob\"><b>DP clustering</b>: Conversations are first converted into numerical representations (embeddings). The framework then groups representations that are close to each other using a <a href=\"https://research.google/blog/practical-differentially-private-clustering/\">DP clustering algorithm</a>. This ensures that no single conversation overly influences the cluster centers.</li><li data-block-key=\"9393b\"><b>DP keyword extraction</b>: Keywords are extracted from each conversation. For every cluster, our approach computes a histogram of keywords, i.e., it counts the number of times each keyword appears in the cluster, using a DP histogram mechanism (e.g., [<a href=\"https://arxiv.org/abs/2006.03684\" target=\"_blank\" rel=\"noopener noreferrer\">1</a>, <a href=\"https://arxiv.org/abs/2301.01998\" target=\"_blank\" rel=\"noopener noreferrer\">2</a>]). We add noise to the histogram in order to mask the influence of individual conversations, ensuring that only keywords common to multiple users are selected, preventing unique or sensitive terms from being exposed. We explore three methods for creation of keywords for each conversation:<ol><li data-block-key=\"9ve1l\">LLM guided selection: We provide an LLM the conversation and ask it to create the top five most relevant keywords;</li><li data-block-key=\"fuvdn\">DP version of <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" target=\"_blank\" rel=\"noopener noreferrer\">TF-IDF</a>: We get all the words in the conversation and weight them proportionally to the number of times they appear in the text and inversely proportional to the number of times they appear in the corpus; and</li><li data-block-key=\"9r76v\">LLM guided approach where there is an initial list of keywords obtained from public data: Instead of asking the LLM to generate keywords for each conversation independently, we create a list of potential keywords and ask LLM to choose the top 5 most relevant keywords from the list.</li></ol></li><li data-block-key=\"71r3d\"><b>LLM summarization from keywords</b>: Finally, an LLM generates a high-level summary for each cluster using only the privately selected keywords. The LLM never sees the original conversations in the cluster, only the anonymized keywords. This post-processing property ensures the end-to-end privacy of the entire framework.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_hero.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_hero\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_hero.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_hero\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"hhudz\"><i>The framework’s data flow. Yellow nodes denote non-DP data, green nodes represent operations that are either DP or per conversation, light blue nodes denote private data, and dark blue nodes represent non-private operations.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"k5pub\">By integrating DP at its core, this framework’s privacy guarantees are mathematical, not heuristic. They don't depend on an LLM's ability to perfectly redact private data: in other words, even if keywords contain PII or some other sensitive data, the generated summaries are not going to contain this data. In more practical terms, this guarantee makes it impossible for the LLM to reveal sensitive data (e.g., due to prompt injection attacks).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Putting this framework to the test</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">To evaluate our framework’s utility (summary quality) and privacy (protection strength), we compared its performance against Simple-CLIO, a non-private baseline we created inspired by <a href=\"https://www.anthropic.com/research/clio\" target=\"_blank\" rel=\"noopener noreferrer\">CLIO</a>. The baseline follows a two-step process:</p><ol><li data-block-key=\"30t2i\">Conversations are converted into embeddings and clustered non-privately.</li><li data-block-key=\"25ftm\">For each cluster, a sample of conversations is fed to an LLM to generate a summary of those samples.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">The privacy-utility trade-off</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">As expected, we observed a trade-off: stronger privacy settings (lower values of the privacy parameter <i>ϵ</i>) led to a decrease in the granularity of the summaries. For instance, topic coverage dropped as the privacy budget tightened, because the DP clustering algorithm produced fewer and less precise clusters.</p><p data-block-key=\"cg6p6\">However, the results also held a surprise. In head-to-head comparisons, LLM evaluators often preferred the private summaries generated by our framework. In one evaluation, the DP-generated summaries were favored up to 70% of the time. This suggests that the constraints imposed by this DP pipeline — forcing summaries to be based on general, frequent keywords — can lead to outputs that are more concise and focused than those from an unconstrained, non-private approach.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Empirical privacy evaluation</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">To test the framework’s robustness, we ran a <a href=\"https://arxiv.org/abs/1610.05820\" target=\"_blank\" rel=\"noopener noreferrer\">membership inference-style attack</a> designed to identify whether a specific sensitive conversation was included in the dataset. The results were clear: the attack on the DP pipeline performed about as well as random guessing, achieving an <a href=\"https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\" target=\"_blank\" rel=\"noopener noreferrer\">area under the curve</a> (AUC) score of 0.53 (i.e., the integral of the <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ROC</i></a> curve). In contrast, the attack was more successful against the non-private pipeline, which had a higher AUC of 0.58, indicating greater information leakage. This experiment provides empirical evidence that our privacy framework offers significantly stronger protection against privacy leakage.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_2.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_2.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"hhudz\"><i>The</i> <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ROC</i></a><i> curve for DP pipeline shows performance close to random guessing (AUC = 0.53), demonstrating its robustness.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_3.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_3.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"hhudz\"><i>The</i> <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ROC</i></a><i> curve for the non-private pipeline is more vulnerable (AUC = 0.58).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Looking ahead</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">Our work is a first step toward building systems that can analyze large-scale text corpora with formal privacy guarantees. We've shown that it's possible to balance the need for meaningful insights with stringent user privacy.</p><p data-block-key=\"6t4ua\">Looking forward, we see several exciting avenues for future research. These include adapting the framework for online settings where new conversations are constantly added, exploring alternative DP mechanisms to further improve the utility-privacy trade-off, and adding support for multi-modal conversations (i.e., conversations involving images, videos, and audio).</p><p data-block-key=\"9t7eu\">As AI becomes more integrated into our daily lives, developing privacy-preserving methods for understanding its use is not just a technical challenge — it's a fundamental requirement for building trustworthy and responsible AI.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">Thanks to all project contributors, whose essential efforts were pivotal to its success. Special thanks to our colleagues: Yaniv Carmel, Edith Cohen, Rudrajit Das, Chris Dibak, Vadym Doroshenko, Alessandro Epasto, Prem Eruvbetine, Dem Gerolemou, Badih Ghazi, Miguel Guevara, Steve He, Peter Kairouz, Pritish Kamath, Nir Kerem, Ravi Kumar, Ethan Leeman, Pasin Manurangsi, Shlomi Pasternak, Mikhail Pravilov, Adam Sealfon, Yurii Sushko, Da Yu, Chiyuan Zhang.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                ",
      "contentSnippet": "Large language model (LLM) chatbots are used by hundreds of millions of people daily for tasks ranging from drafting emails and writing code to planning vacations and creating menus for cafes. Understanding these high-level use cases is incredibly valuable for platform providers looking to improve services or enforce safety policies. It also offers the public insights into how AI is shaping our world.\nBut this raises a critical question: How can we gain valuable insights when the conversations themselves might contain private or sensitive information?\nExisting approaches, like the CLIO framework, attempt to solve this by using an LLM to summarize conversations while prompting it to strip out personally identifiable information (PII). While a good first step, this method relies on heuristic privacy protections. The resulting privacy guarantee is difficult to formalize and may not hold up as models evolve, making these systems difficult to maintain and audit. This limitation led us to ask if it is possible to achieve similar utility with formal, end-to-end privacy guarantees.\nIn our paper, \"Urania: Differentially Private Insights into AI Use,\" presented at COLM 2025, we introduce a new framework that generates insights from LLM chatbot interactions with rigorous differential privacy (DP) guarantees. This framework uses a DP clustering algorithm and keyword extraction method to ensure that no single conversation overly influences the result (i.e., the output summaries do not reveal information about any single individual's conversation). Here we explain the algorithm and demonstrate that this framework is indeed providing better privacy guarantees than prior solutions.\nGemini-generated image showing schematically how the algorithm works for one cluster of conversations.\nPrivacy-preserving framework for insights mining\nDP uses a privacy budget parameter, ε, to measure the maximum allowed influence of any single user's contributions to the final output of a model. Our framework is designed to rely on two key properties of DP:\n\nPost-processing: If B is an ε-DP algorithm and A is any non-DP algorithm, then running A on the output of B keeps things private at a ε-DP level.\nComposition: If A and B are two separate ε-DP algorithms, running A on the dataset and the output of B still keeps the entire process private at a 2ε-DP level.\n\nThis differentially private pipeline is designed to ensure end-to-end user data protection through the following stages:\n\nDP clustering: Conversations are first converted into numerical representations (embeddings). The framework then groups representations that are close to each other using a DP clustering algorithm. This ensures that no single conversation overly influences the cluster centers.\nDP keyword extraction: Keywords are extracted from each conversation. For every cluster, our approach computes a histogram of keywords, i.e., it counts the number of times each keyword appears in the cluster, using a DP histogram mechanism (e.g., [1, 2]). We add noise to the histogram in order to mask the influence of individual conversations, ensuring that only keywords common to multiple users are selected, preventing unique or sensitive terms from being exposed. We explore three methods for creation of keywords for each conversation:\nLLM guided selection: We provide an LLM the conversation and ask it to create the top five most relevant keywords;\nDP version of TF-IDF: We get all the words in the conversation and weight them proportionally to the number of times they appear in the text and inversely proportional to the number of times they appear in the corpus; and\nLLM guided approach where there is an initial list of keywords obtained from public data: Instead of asking the LLM to generate keywords for each conversation independently, we create a list of potential keywords and ask LLM to choose the top 5 most relevant keywords from the list.\n\nLLM summarization from keywords: Finally, an LLM generates a high-level summary for each cluster using only the privately selected keywords. The LLM never sees the original conversations in the cluster, only the anonymized keywords. This post-processing property ensures the end-to-end privacy of the entire framework.\n\n\n\n    \nThe framework’s data flow. Yellow nodes denote non-DP data, green nodes represent operations that are either DP or per conversation, light blue nodes denote private data, and dark blue nodes represent non-private operations.\nBy integrating DP at its core, this framework’s privacy guarantees are mathematical, not heuristic. They don't depend on an LLM's ability to perfectly redact private data: in other words, even if keywords contain PII or some other sensitive data, the generated summaries are not going to contain this data. In more practical terms, this guarantee makes it impossible for the LLM to reveal sensitive data (e.g., due to prompt injection attacks).\nPutting this framework to the test\nTo evaluate our framework’s utility (summary quality) and privacy (protection strength), we compared its performance against Simple-CLIO, a non-private baseline we created inspired by CLIO. The baseline follows a two-step process:\n\nConversations are converted into embeddings and clustered non-privately.\nFor each cluster, a sample of conversations is fed to an LLM to generate a summary of those samples.\n\n\n\n    \nThe privacy-utility trade-off\nAs expected, we observed a trade-off: stronger privacy settings (lower values of the privacy parameter ϵ) led to a decrease in the granularity of the summaries. For instance, topic coverage dropped as the privacy budget tightened, because the DP clustering algorithm produced fewer and less precise clusters.\nHowever, the results also held a surprise. In head-to-head comparisons, LLM evaluators often preferred the private summaries generated by our framework. In one evaluation, the DP-generated summaries were favored up to 70% of the time. This suggests that the constraints imposed by this DP pipeline — forcing summaries to be based on general, frequent keywords — can lead to outputs that are more concise and focused than those from an unconstrained, non-private approach.\nEmpirical privacy evaluation\nTo test the framework’s robustness, we ran a membership inference-style attack designed to identify whether a specific sensitive conversation was included in the dataset. The results were clear: the attack on the DP pipeline performed about as well as random guessing, achieving an area under the curve (AUC) score of 0.53 (i.e., the integral of the ROC curve). In contrast, the attack was more successful against the non-private pipeline, which had a higher AUC of 0.58, indicating greater information leakage. This experiment provides empirical evidence that our privacy framework offers significantly stronger protection against privacy leakage.\nThe ROC curve for DP pipeline shows performance close to random guessing (AUC = 0.53), demonstrating its robustness.\nThe ROC curve for the non-private pipeline is more vulnerable (AUC = 0.58).\nLooking ahead\nOur work is a first step toward building systems that can analyze large-scale text corpora with formal privacy guarantees. We've shown that it's possible to balance the need for meaningful insights with stringent user privacy.\nLooking forward, we see several exciting avenues for future research. These include adapting the framework for online settings where new conversations are constantly added, exploring alternative DP mechanisms to further improve the utility-privacy trade-off, and adding support for multi-modal conversations (i.e., conversations involving images, videos, and audio).\nAs AI becomes more integrated into our daily lives, developing privacy-preserving methods for understanding its use is not just a technical challenge — it's a fundamental requirement for building trustworthy and responsible AI.\nAcknowledgments\nThanks to all project contributors, whose essential efforts were pivotal to its success. Special thanks to our colleagues: Yaniv Carmel, Edith Cohen, Rudrajit Das, Chris Dibak, Vadym Doroshenko, Alessandro Epasto, Prem Eruvbetine, Dem Gerolemou, Badih Ghazi, Miguel Guevara, Steve He, Peter Kairouz, Pritish Kamath, Nir Kerem, Ravi Kumar, Ethan Leeman, Pasin Manurangsi, Shlomi Pasternak, Mikhail Pravilov, Adam Sealfon, Yurii Sushko, Da Yu, Chiyuan Zhang.",
      "creator": "Google",
      "summary": "文章介绍了一种名为Urania的差分隐私框架，用于从大型语言模型（LLM）聊天机器人交互中提取洞察，同时保证严格的差分隐私保护。该框架采用差分隐私聚类算法和关键词提取方法，确保单个对话不会过度影响结果，并通过后处理机制保护用户数据隐私。与现有方法相比，Urania提供更强的隐私保障，适用于需要分析大量聊天数据并保护用户隐私的场景。"
    },
    {
      "title": "Titans + MIRAS: Helping AI have long-term memory",
      "link": "https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/",
      "pubDate": "Wed, 03 Dec 2025 16:00:00 GMT",
      "isoDate": "2025-12-03T16:00:00.000Z",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"il1w2\">The <a href=\"https://en.wikipedia.org/wiki/Transformer_(deep_learning)\" target=\"_blank\" rel=\"noopener noreferrer\">Transformer architecture</a> revolutionized <a href=\"https://medium.com/machine-learning-basics/sequence-modelling-b2cdf244c233\" target=\"_blank\" rel=\"noopener noreferrer\">sequence modeling</a> with its introduction of <a href=\"https://en.wikipedia.org/wiki/Attention_%28machine_learning%29\" target=\"_blank\" rel=\"noopener noreferrer\">attention</a>, a mechanism by which models look back at earlier inputs to prioritize relevant input data. However, computational cost increases drastically with sequence length, which limits the ability to scale Transformer-based models to extremely long contexts, such as those required for full-document understanding or genomic analysis.</p><p data-block-key=\"36kb5\">The research community explored various approaches for solutions, such as efficient linear <a href=\"https://www.d2l.ai/chapter_recurrent-modern/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">recurrent neural networks</a> (RNNs) and <a href=\"https://huggingface.co/blog/lbourdois/get-on-the-ssm-train\" target=\"_blank\" rel=\"noopener noreferrer\">state space models</a> (SSMs) like <a href=\"https://arxiv.org/pdf/2405.21060\" target=\"_blank\" rel=\"noopener noreferrer\">Mamba-2</a>. These models offer fast, linear scaling by compressing context into a fixed-size. However, this fixed-size compression cannot adequately capture the rich information in very long sequences.</p><p data-block-key=\"40m00\">In two new papers, <a href=\"https://arxiv.org/abs/2501.00663\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Titans</i></a> and <a href=\"https://arxiv.org/pdf/2504.13173\" target=\"_blank\" rel=\"noopener noreferrer\"><i>MIRAS</i></a>, we introduce an architecture and theoretical blueprint that combine the speed of RNNs with the accuracy of transformers. Titans is the specific architecture (the tool), and MIRAS is the theoretical framework (the blueprint) for generalizing these approaches. Together, they advance the concept of test-time memorization, the ability of an AI model to maintain long-term memory by incorporating more powerful “surprise” metrics (i.e., unexpected pieces of information) while the model is running and without dedicated offline retraining.</p><p data-block-key=\"eic3n\">The MIRAS framework, as demonstrated by Titans, introduces a meaningful shift toward real-time adaptation. Instead of compressing information into a static state, this architecture actively learns and updates its own parameters as data streams in. This crucial mechanism enables the model to incorporate new, specific details into its core knowledge instantly.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Titans: Learning new context on the fly\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Titans: Learning new context on the fly</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">An effective learning system requires distinct yet interconnected memory modules, mirroring the <a href=\"https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/\">human brain's separation of short-term and long-term memory</a>.</p><p data-block-key=\"dpe7v\">While attention mechanisms excel for precise, short-term memory, Titans introduces a novel neural <a href=\"https://arxiv.org/abs/2306.07174#:~:text=LongMem%20can:%20*%20Memorize%20long%20past%20context,Yan%20*%20Jianfeng%20Gao%20*%20Furu%20Wei\" target=\"_blank\" rel=\"noopener noreferrer\">long-term memory module</a>, that, unlike the fixed-size vector or matrix memory in traditional RNNs, acts as a deep neural network (specifically, a <a href=\"https://en.wikipedia.org/wiki/Multilayer_perceptron\" target=\"_blank\" rel=\"noopener noreferrer\">multi-layer perceptron</a>). This memory module provides significantly higher expressive power, allowing the model to summarize large volumes of information without losing important context. The model isn't simply taking notes; it's understanding and synthesizing the entire story.</p><p data-block-key=\"e95op\">Crucially, Titans doesn’t just passively store data. It actively learns <i>how</i> to recognize and retain important relationships and conceptual themes that connect tokens across the entire input. A key aspect of this ability is what we call the “surprise metric”. In human psychology, we know we quickly and easily forget routine, expected events but remember things that break the pattern — unexpected, surprising, or highly emotional events.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-1-Overview.width-1250.png\" alt=\"A diagram illustrating a neural architecture with three layers: Contextual Memory (learning), Core (in-context learning), and Persistent Memory (fixed weights).\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-1-Overview.width-1250.png\" alt=\"A diagram illustrating a neural architecture with three layers: Contextual Memory (learning), Core (in-context learning), and Persistent Memory (fixed weights).\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"759np\">Overview of the Titans (MAC) architecture. It uses a long-term memory to compress the past data and then incorporate the summary into the context and pass it to attention. Attention can then decide if it needs to attend to the summary of the past or not.</p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"il1w2\">In the context of Titans, the \"surprise metric\" is the model detecting a large difference between what it currently remembers and what the new input is telling it.</p><ul><li data-block-key=\"a9lns\"><i>Low surprise</i>: If the new word is \"cat\" and the model's memory state already expects an animal word, the gradient (surprise) is low. It can safely skip memorizing the word \"cat\" in its permanent long-term state.</li><li data-block-key=\"2t2sa\"><i>High surprise</i>: If the model's memory state is summarizing a serious financial report, and the new input is a picture of a banana peel (the unexpected event), the gradient (surprise) will be very high. This signals that the new input is important or anomalous, and it must be prioritized for permanent storage in the long-term memory module.</li></ul><p data-block-key=\"djj22\">The model uses this internal error signal (the gradient) as a mathematical equivalent of saying, \"This is unexpected and important!\" This allows the Titans architecture to selectively update its long-term memory only with the most novel and context-breaking information, keeping the overall process fast and efficient.</p><p data-block-key=\"dm2am\">Titans refines this mechanism by incorporating two critical elements:</p><ol><li data-block-key=\"bb101\"><i>Momentum</i>: The model considers both \"momentary surprise\" (the current input) and \"past surprise\" (the recent context flow). This ensures relevant subsequent information is also captured, even if those tokens are not individually surprising.</li><li data-block-key=\"b269a\"><i>Forgetting (weight decay)</i>: To manage the finite capacity of the memory when dealing with extremely long sequences, Titans employ an adaptive weight decay mechanism. This acts as a forgetting gate, allowing the model to discard information that is no longer needed.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"MIRAS: A unified view of sequence modeling\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">MIRAS: A unified view of sequence modeling</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">Every major breakthrough in sequence modeling — from modern transformers to the new, lightning-fast linear RNNs — is essentially the same thing under the hood: a highly complex <a href=\"https://www.geeksforgeeks.org/computer-organization-architecture/associative-memory/\" target=\"_blank\" rel=\"noopener noreferrer\">associative memory</a> module.</p><p data-block-key=\"d91su\">Accordingly, what makes MIRAS both unique and practical is the way it views AI modeling. Instead of seeing diverse architectures, it sees different methods of solving the same problem: efficiently combining new information with old memories without letting the essential concepts be forgotten<b>.</b></p><p data-block-key=\"7u78e\">MIRAS defines a sequence model through four key design choices:</p><ul><li data-block-key=\"abcem\"><i>Memory architecture</i>: The structure that stores information (e.g., a vector, matrix, or a deep multi-layer perceptron, like in Titans).</li><li data-block-key=\"5s0u1\"><i>Attentional bias</i>: The internal learning objective the model optimizes that determines what it prioritizes.</li><li data-block-key=\"4qd03\"><i>Retention gate</i>: The memory regularizer. MIRAS reinterprets \"forgetting mechanisms\" as specific forms of <a href=\"https://dev.to/nareshnishad/day-27-regularization-techniques-for-large-language-models-llms-4af3\" target=\"_blank\" rel=\"noopener noreferrer\">regularization</a> that balance new learning against retaining past knowledge.</li><li data-block-key=\"9savd\"><i>Memory algorithm</i>: The optimization algorithm used to update the memory.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/MIRAS_Framework_Animation.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"759np\">The MIRAS framework overview. In the MIRAS framework, we aim to learn an associative memory, mapping between keys and values. For each token, the memory module internally optimizes its inner attentional bias while using its retention gate to make sure that it does not deviate from its past state. The optimization process is done through gradient-based optimizer.</p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Transcending the mean squared error paradigm\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Transcending the mean squared error paradigm</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">Virtually all successful existing sequence models rely on <a href=\"https://en.wikipedia.org/wiki/Mean_squared_error\" target=\"_blank\" rel=\"noopener noreferrer\">mean squared error</a> (MSE) or <a href=\"https://medium.com/advanced-deep-learning/understanding-vector-similarity-b9c10f7506de\" target=\"_blank\" rel=\"noopener noreferrer\">dot-product similarity</a> for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.</p><p data-block-key=\"1cust\">MIRAS transcends this limitation by providing a generative framework to explore a more rich design space informed by the literature in optimization and statistics. This allows for the creation of novel architectures with <a href=\"https://en.wikipedia.org/wiki/Non-Euclidean_geometry\" target=\"_blank\" rel=\"noopener noreferrer\">non-Euclidean objectives</a> and regularization.</p><p data-block-key=\"40qp3\">Using MIRAS, we created three specific attention-free models:</p><ul><li data-block-key=\"fpeib\"><i>YAAD</i>: We designed this MIRAS variant to be less sensitive to major errors or \"outliers\" (like a single typo in a large document). It uses a gentler math penalty (<a href=\"https://en.wikipedia.org/wiki/Huber_loss\" target=\"_blank\" rel=\"noopener noreferrer\">Huber loss</a>) for mistakes, so it doesn't overreact to one-off issues. This makes the model more robust when the input data is messy or inconsistent.</li><li data-block-key=\"28vrl\"><i>MONETA</i>: This model explores the use of more complex and strict mathematical penalties (called <a href=\"https://en.wikipedia.org/wiki/Norm_(mathematics)\" target=\"_blank\" rel=\"noopener noreferrer\">generalized norms</a>). It investigates whether using these more disciplined rules for both what the model attends to and what it forgets can lead to a more powerful and stable long-term memory system overall.</li><li data-block-key=\"d4e49\"><i>MEMORA</i>: This model focuses on achieving the best possible memory stability by forcing its memory to act like a strict probability map. By using this constraint, it ensures that every time the memory state is updated, the changes are controlled and balanced. This guarantees a clean, stable process for integrating new information.Virtually all successful existing sequence models rely on <a href=\"https://en.wikipedia.org/wiki/Mean_squared_error\" target=\"_blank\" rel=\"noopener noreferrer\">mean squared error</a> (MSE) or <a href=\"https://medium.com/advanced-deep-learning/understanding-vector-similarity-b9c10f7506de\" target=\"_blank\" rel=\"noopener noreferrer\">dot-product similarity</a> for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Experiments and results\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experiments and results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">We rigorously compared Titans along with MIRAS variants (YAAD, MONETA, MEMORA) against leading architectures, including <a href=\"https://arxiv.org/abs/2003.04974\" target=\"_blank\" rel=\"noopener noreferrer\">Transformer++</a>, <a href=\"https://arxiv.org/pdf/2405.21060\" target=\"_blank\" rel=\"noopener noreferrer\">Mamba-2</a>, and <a href=\"https://arxiv.org/pdf/2412.06464\" target=\"_blank\" rel=\"noopener noreferrer\">Gated DeltaNet</a>. We further validated versatility by testing Titans on genomic modeling (DNA) and time-series forecasting, proving the architecture generalizes effectively beyond text.</p><p data-block-key=\"a9v6c\">Across both standard language modeling datasets (<a href=\"https://c4model.com/\" target=\"_blank\" rel=\"noopener noreferrer\">C4</a>, <a href=\"https://huggingface.co/datasets/Salesforce/wikitext\" target=\"_blank\" rel=\"noopener noreferrer\">WikiTex</a>t) and <a href=\"https://medium.com/@hetzer2807/zero-shot-reasoning-unleashed-the-magic-of-large-language-models-4e877dfe470e\" target=\"_blank\" rel=\"noopener noreferrer\">zero-shot reasoning tasks</a> (<a href=\"https://arxiv.org/abs/1905.07830\" target=\"_blank\" rel=\"noopener noreferrer\">HellaSwag</a>, PIQA), our models consistently demonstrated higher accuracy and <a href=\"https://en.wikipedia.org/wiki/Perplexity\" target=\"_blank\" rel=\"noopener noreferrer\">perplexity</a> (a measure of how surprised an LLM is when looking at a piece of text).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"The power of deep memory\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">The power of deep memory</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">Ablation studies clearly show that the depth of the memory architecture is crucial. When comparing long-term memory modules of the same size but different depths, modules with deeper memories consistently achieve lower perplexity in language modeling. Furthermore, they exhibit better scaling properties, maintaining performance as the sequence length increases significantly.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-3-Performance1.width-1250.png\" alt=\"Two line charts showing that LMM and MM models maintain lower perplexity than Mamba as sequence length increases across 360M and 760M parameter scales.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-3-Performance1.width-1250.png\" alt=\"Two line charts showing that LMM and MM models maintain lower perplexity than Mamba as sequence length increases across 360M and 760M parameter scales.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"759np\"><i>The effect of memory depth on the perplexity across 360M and 760M parameter scales.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Language modeling and efficiency\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Language modeling and efficiency</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">In language modeling and commonsense reasoning tasks, Titans architectures outperform state-of-the-art linear recurrent models (such as Mamba-2 and Gated DeltaNet) and Transformer++ baselines of comparable sizes. The novel MIRAS variants (MONETA, YAAD, MEMORA) also achieve improved performance compared to these baselines, validating the benefit of exploring robust, non-MSE optimization mechanisms. Importantly, these models maintain efficient, parallelizable training and fast linear inference speeds.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Extreme long-context recall\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Extreme long-context recall</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">The most significant advantage of these new architectures is their ability to handle extremely long contexts. This is highlighted in the <a href=\"https://github.com/booydar/babilong\" target=\"_blank\" rel=\"noopener noreferrer\">BABILong benchmark</a>, a task requiring reasoning across facts distributed in extremely long documents. In this challenging setting, Titans outperforms all baselines, including extremely large models like GPT-4, despite having many fewer parameters. Titans further demonstrates the capability to scale effectively to context window sizes larger than 2 million tokens.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-4-Performance2.width-1250.png\" alt=\"Line graph showing Titans (MAC)-FT maintains improved accuracy over increasing sequence lengths compared to GPT-4, Mamba-FT, and other models.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-4-Performance2.width-1250.png\" alt=\"Line graph showing Titans (MAC)-FT maintains improved accuracy over increasing sequence lengths compared to GPT-4, Mamba-FT, and other models.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"759np\"><i>Performance of Titans on extreme long-context reasoning.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Conclusion\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">The introduction of Titans and the MIRAS framework marks a significant advancement in sequence modeling. By employing deep neural networks as memory modules that learn to memorize as data is coming in, these approaches overcome the limitations of fixed-size recurrent states. Furthermore, MIRAS provides a powerful theoretical unification, revealing the connection between online optimization, associative memory, and architectural design. By moving beyond the standard Euclidean paradigm, this research opens the door to a new generation of sequence models that combine the efficiency of RNNs with the expressive power needed for the era of long-context AI.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                ",
      "contentSnippet": "The Transformer architecture revolutionized sequence modeling with its introduction of attention, a mechanism by which models look back at earlier inputs to prioritize relevant input data. However, computational cost increases drastically with sequence length, which limits the ability to scale Transformer-based models to extremely long contexts, such as those required for full-document understanding or genomic analysis.\nThe research community explored various approaches for solutions, such as efficient linear recurrent neural networks (RNNs) and state space models (SSMs) like Mamba-2. These models offer fast, linear scaling by compressing context into a fixed-size. However, this fixed-size compression cannot adequately capture the rich information in very long sequences.\nIn two new papers, Titans and MIRAS, we introduce an architecture and theoretical blueprint that combine the speed of RNNs with the accuracy of transformers. Titans is the specific architecture (the tool), and MIRAS is the theoretical framework (the blueprint) for generalizing these approaches. Together, they advance the concept of test-time memorization, the ability of an AI model to maintain long-term memory by incorporating more powerful “surprise” metrics (i.e., unexpected pieces of information) while the model is running and without dedicated offline retraining.\nThe MIRAS framework, as demonstrated by Titans, introduces a meaningful shift toward real-time adaptation. Instead of compressing information into a static state, this architecture actively learns and updates its own parameters as data streams in. This crucial mechanism enables the model to incorporate new, specific details into its core knowledge instantly.\nTitans: Learning new context on the fly\nAn effective learning system requires distinct yet interconnected memory modules, mirroring the human brain's separation of short-term and long-term memory.\nWhile attention mechanisms excel for precise, short-term memory, Titans introduces a novel neural long-term memory module, that, unlike the fixed-size vector or matrix memory in traditional RNNs, acts as a deep neural network (specifically, a multi-layer perceptron). This memory module provides significantly higher expressive power, allowing the model to summarize large volumes of information without losing important context. The model isn't simply taking notes; it's understanding and synthesizing the entire story.\nCrucially, Titans doesn’t just passively store data. It actively learns how to recognize and retain important relationships and conceptual themes that connect tokens across the entire input. A key aspect of this ability is what we call the “surprise metric”. In human psychology, we know we quickly and easily forget routine, expected events but remember things that break the pattern — unexpected, surprising, or highly emotional events.\nOverview of the Titans (MAC) architecture. It uses a long-term memory to compress the past data and then incorporate the summary into the context and pass it to attention. Attention can then decide if it needs to attend to the summary of the past or not.\nIn the context of Titans, the \"surprise metric\" is the model detecting a large difference between what it currently remembers and what the new input is telling it.\n\nLow surprise: If the new word is \"cat\" and the model's memory state already expects an animal word, the gradient (surprise) is low. It can safely skip memorizing the word \"cat\" in its permanent long-term state.\nHigh surprise: If the model's memory state is summarizing a serious financial report, and the new input is a picture of a banana peel (the unexpected event), the gradient (surprise) will be very high. This signals that the new input is important or anomalous, and it must be prioritized for permanent storage in the long-term memory module.\n\nThe model uses this internal error signal (the gradient) as a mathematical equivalent of saying, \"This is unexpected and important!\" This allows the Titans architecture to selectively update its long-term memory only with the most novel and context-breaking information, keeping the overall process fast and efficient.\nTitans refines this mechanism by incorporating two critical elements:\n\nMomentum: The model considers both \"momentary surprise\" (the current input) and \"past surprise\" (the recent context flow). This ensures relevant subsequent information is also captured, even if those tokens are not individually surprising.\nForgetting (weight decay): To manage the finite capacity of the memory when dealing with extremely long sequences, Titans employ an adaptive weight decay mechanism. This acts as a forgetting gate, allowing the model to discard information that is no longer needed.\n\n\n\n    \nMIRAS: A unified view of sequence modeling\nEvery major breakthrough in sequence modeling — from modern transformers to the new, lightning-fast linear RNNs — is essentially the same thing under the hood: a highly complex associative memory module.\nAccordingly, what makes MIRAS both unique and practical is the way it views AI modeling. Instead of seeing diverse architectures, it sees different methods of solving the same problem: efficiently combining new information with old memories without letting the essential concepts be forgotten.\nMIRAS defines a sequence model through four key design choices:\n\nMemory architecture: The structure that stores information (e.g., a vector, matrix, or a deep multi-layer perceptron, like in Titans).\nAttentional bias: The internal learning objective the model optimizes that determines what it prioritizes.\nRetention gate: The memory regularizer. MIRAS reinterprets \"forgetting mechanisms\" as specific forms of regularization that balance new learning against retaining past knowledge.\nMemory algorithm: The optimization algorithm used to update the memory.\n\n\n\n    \nplay silent looping video\n            pause silent looping video\n          \n\n\n\n            \n\n\n\n  \n\n\n  \n\n\n          \nunmute video\n        mute video\n    \n\n            \n\n        \n        \n            \n\n            \n\n        \n    \nThe MIRAS framework overview. In the MIRAS framework, we aim to learn an associative memory, mapping between keys and values. For each token, the memory module internally optimizes its inner attentional bias while using its retention gate to make sure that it does not deviate from its past state. The optimization process is done through gradient-based optimizer.\nTranscending the mean squared error paradigm\nVirtually all successful existing sequence models rely on mean squared error (MSE) or dot-product similarity for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.\nMIRAS transcends this limitation by providing a generative framework to explore a more rich design space informed by the literature in optimization and statistics. This allows for the creation of novel architectures with non-Euclidean objectives and regularization.\nUsing MIRAS, we created three specific attention-free models:\n\nYAAD: We designed this MIRAS variant to be less sensitive to major errors or \"outliers\" (like a single typo in a large document). It uses a gentler math penalty (Huber loss) for mistakes, so it doesn't overreact to one-off issues. This makes the model more robust when the input data is messy or inconsistent.\nMONETA: This model explores the use of more complex and strict mathematical penalties (called generalized norms). It investigates whether using these more disciplined rules for both what the model attends to and what it forgets can lead to a more powerful and stable long-term memory system overall.\nMEMORA: This model focuses on achieving the best possible memory stability by forcing its memory to act like a strict probability map. By using this constraint, it ensures that every time the memory state is updated, the changes are controlled and balanced. This guarantees a clean, stable process for integrating new information.Virtually all successful existing sequence models rely on mean squared error (MSE) or dot-product similarity for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.\n\n\n\n    \nExperiments and results\nWe rigorously compared Titans along with MIRAS variants (YAAD, MONETA, MEMORA) against leading architectures, including Transformer++, Mamba-2, and Gated DeltaNet. We further validated versatility by testing Titans on genomic modeling (DNA) and time-series forecasting, proving the architecture generalizes effectively beyond text.\nAcross both standard language modeling datasets (C4, WikiText) and zero-shot reasoning tasks (HellaSwag, PIQA), our models consistently demonstrated higher accuracy and perplexity (a measure of how surprised an LLM is when looking at a piece of text).\nThe power of deep memory\nAblation studies clearly show that the depth of the memory architecture is crucial. When comparing long-term memory modules of the same size but different depths, modules with deeper memories consistently achieve lower perplexity in language modeling. Furthermore, they exhibit better scaling properties, maintaining performance as the sequence length increases significantly.\nThe effect of memory depth on the perplexity across 360M and 760M parameter scales.\nLanguage modeling and efficiency\nIn language modeling and commonsense reasoning tasks, Titans architectures outperform state-of-the-art linear recurrent models (such as Mamba-2 and Gated DeltaNet) and Transformer++ baselines of comparable sizes. The novel MIRAS variants (MONETA, YAAD, MEMORA) also achieve improved performance compared to these baselines, validating the benefit of exploring robust, non-MSE optimization mechanisms. Importantly, these models maintain efficient, parallelizable training and fast linear inference speeds.\nExtreme long-context recall\nThe most significant advantage of these new architectures is their ability to handle extremely long contexts. This is highlighted in the BABILong benchmark, a task requiring reasoning across facts distributed in extremely long documents. In this challenging setting, Titans outperforms all baselines, including extremely large models like GPT-4, despite having many fewer parameters. Titans further demonstrates the capability to scale effectively to context window sizes larger than 2 million tokens.\nPerformance of Titans on extreme long-context reasoning.\nConclusion\nThe introduction of Titans and the MIRAS framework marks a significant advancement in sequence modeling. By employing deep neural networks as memory modules that learn to memorize as data is coming in, these approaches overcome the limitations of fixed-size recurrent states. Furthermore, MIRAS provides a powerful theoretical unification, revealing the connection between online optimization, associative memory, and architectural design. By moving beyond the standard Euclidean paradigm, this research opens the door to a new generation of sequence models that combine the efficiency of RNNs with the expressive power needed for the era of long-context AI.",
      "creator": "Google",
      "summary": "文章标题：Titans + MIRAS：赋予AI长期记忆能力\n\n文章内容：\nTransformer架构通过注意力机制革新了序列建模，但计算成本随序列长度增加而急剧上升，限制了其处理超长上下文的能力。Titans和MIRAS是新提出的架构和理论框架，结合了RNN的速度和Transformer的准确性，实现测试时记忆。MIRAS框架通过Titans架构，使模型能实时适应数据流，通过“惊喜”指标（即意外信息）更新参数，快速整合新细节。Titans引入了新型神经长期记忆模块，能主动学习并存储重要关系和主题，利用“惊喜”指标识别和保留关键信息，高效更新长期记忆，实现快速准确的长期记忆。"
    },
    {
      "title": "From Waveforms to Wisdom: The New Benchmark for Auditory Intelligence",
      "link": "https://research.google/blog/from-waveforms-to-wisdom-the-new-benchmark-for-auditory-intelligence/",
      "pubDate": "Tue, 02 Dec 2025 16:00:00 GMT",
      "isoDate": "2025-12-02T16:00:00.000Z",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"04uji\">Sound is a critical part of <a href=\"https://courses.lumenlearning.com/waymaker-psychology/chapter/multi-modal-perception/#:~:text=For%20example%2C%20if%20the%20middle,Right%20there\" target=\"_blank\" rel=\"noopener noreferrer\">multimodal perception</a>. For a system — be it a voice assistant, a next-generation security monitor, or an autonomous agent — to behave naturally, it must demonstrate a full spectrum of auditory capabilities. These capabilities include transcription, classification, retrieval, reasoning, segmentation, clustering, reranking, and reconstruction.</p><p data-block-key=\"erg92\">These diverse functions rely on transforming raw sound into an intermediate representation, or <a href=\"https://huggingface.co/spaces/hesamation/primer-llm-embedding\" target=\"_blank\" rel=\"noopener noreferrer\">embedding</a>. But research into improving the auditory capabilities of multimodal perception models has been fragmented, and there remain important unanswered questions: How do we compare performance across domains like human speech and bioacoustics? What is the <i>true</i> performance potential we are leaving on the table? And could a single, general-purpose sound embedding serve as the foundation for all these capabilities?</p><p data-block-key=\"bn8cg\">To investigate these queries and accelerate progress toward robust machine sound intelligence, we created the <a href=\"https://github.com/google-research/mseb/tree/main\" target=\"_blank\" rel=\"noopener noreferrer\">Massive Sound Embedding Benchmark</a> (MSEB), presented at <a href=\"https://neurips.cc/virtual/2025/loc/san-diego/poster/121597\" target=\"_blank\" rel=\"noopener noreferrer\">NeurIPS 2025</a>.</p><p data-block-key=\"932nn\">MSEB provides the necessary structure to answer these questions by:</p><ul><li data-block-key=\"brt1v\">Standardizing evaluation for a comprehensive suite of eight real-world capabilities that we believe every human-like intelligent system must possess.</li><li data-block-key=\"h3sm\">Providing an open and extensible framework that allows researchers to seamlessly integrate and evaluate any model type — from conventional downstream uni-modal models to cascade models to end-to-end multimodal embedding models.</li><li data-block-key=\"8e1pi\">Establishing clear performance goals to objectively highlight research opportunities beyond current state-of-the-art approaches.</li></ul><p data-block-key=\"fl1pu\">Our initial experiments confirm that current sound representations are far from universal, revealing substantial performance \"headroom” (i.e., maximum improvement possible) across all eight tasks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The three pillars of MSEB: A unified framework</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">MSEB is built on three foundational pillars designed to provide the community with the tools needed to build the next generation of sound understanding models.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">1. Diverse datasets for real-world scenarios</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">A benchmark is only as strong as its data. MSEB includes a curated collection of accessible datasets that better reflect our diverse global user community. The cornerstone of our benchmark is the <a href=\"https://huggingface.co/datasets/google/svq\" target=\"_blank\" rel=\"noopener noreferrer\">Simple Voice Questions</a> (SVQ) dataset, a new resource featuring 177,352 short, spoken queries across 26 locales and 17 languages. These recordings were captured in four distinct acoustic environments (clean, background speech, traffic noise, and media noise), and include rich metadata on speaker attributes and time-aligned salient terms. We collected and open-sourced this resource, available on <a href=\"https://huggingface.co/datasets/google/svq/viewer\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face</a>.</p><p data-block-key=\"4t0bt\">MSEB also integrates high-quality, public datasets covering a variety of sound domains:</p><ul><li data-block-key=\"92lni\"><a href=\"https://huggingface.co/datasets/FBK-MT/Speech-MASSIVE\" target=\"_blank\" rel=\"noopener noreferrer\">Speech-MASSIVE</a>: For multilingual spoken language understanding and intent classification.</li><li data-block-key=\"c5qpu\"><a href=\"https://huggingface.co/datasets/Fhrozen/FSD50k\" target=\"_blank\" rel=\"noopener noreferrer\">FSD50K</a>: A large dataset for multi-label environmental sound event recognition (200 classes from the <a href=\"https://research.google.com/audioset/ontology/index.html\">AudioSet Ontology</a>).</li><li data-block-key=\"40c5u\"><a href=\"https://huggingface.co/datasets/DBD-research-group/BirdSet\" target=\"_blank\" rel=\"noopener noreferrer\">BirdSet</a>: A massive-scale benchmark for avian bioacoustics, including complex soundscape recordings.</li></ul><p data-block-key=\"4tacb\">We’re actively working on creating and adding more relevant and large-scale datasets to MSEB. We invite the community to share their suggestions and express interest in collaboration through our <a href=\"https://github.com/google-research/mseb/tree/main/mseb\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repo</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">2. A comprehensive suite of eight core capabilities</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The design of MSEB is built on the premise that the future of AI-based sound interaction is multimodal. Every task uses sound as the critical input, but also incorporates information from other modalities (like text context or knowledge bases) to simulate realistic scenarios.</p><p data-block-key=\"diln9\">MSEB is structured around eight core “super-tasks”, i.e., tasks that represent a capability vital for an intelligent system:</p><ul><li data-block-key=\"53jlg\"><i>Retrieval (voice search)</i>: Simulates voice search by finding relevant documents or passages in a knowledge base from a spoken query.</li><li data-block-key=\"csvqq\"><i>Reasoning (intelligent assistants)</i>: Tests the ability to find a precise answer within a given document or passage based on a spoken question.</li><li data-block-key=\"9hbci\"><i>Classification (monitoring/security)</i>: Categorizes sounds based on speaker attributes, user intent, recording environment, or specific sound events.</li><li data-block-key=\"4ksci\"><i>Transcription</i>: Converts the audio signal into a verbatim text representation (like automatic speech recognition, or ASR, for spoken languages).</li><li data-block-key=\"3a90f\"><i>Segmentation (indexing)</i>: Identifies the most important terms within a sound clip and localizes them with precise start and end times.</li><li data-block-key=\"3c6p0\"><i>Clustering (organization)</i>: Groups a collection of sound samples based on shared attributes (like speaker identity or environment) without relying on predefined labels.</li><li data-block-key=\"a3rmt\"><i>Reranking (hypothesis refinement)</i>: Reorders a list of ambiguous text hypotheses (e.g., ASR output) to better match the original spoken query.</li><li data-block-key=\"5c70t\"><i>Reconstruction (generative AI)</i>: Tests the quality of the embedding by measuring the fidelity with which the original audio waveform can be regenerated from it.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/image2.width-1250.png\" alt=\"Infographic titled Massive Sound Embedding Benchmark (MSEB) displaying icons for eight audio tasks, such as Retrieval, Classification, and Transcription.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/image2.width-1250.png\" alt=\"Infographic titled Massive Sound Embedding Benchmark (MSEB) displaying icons for eight audio tasks, such as Retrieval, Classification, and Transcription.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"i7px7\"><i>MSEB tasks range from information access (retrieval, reranking, reasoning), to fundamental core perception (classification, transcription, segmentation), to higher-level organization generation (clustering, reconstruction).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"04uji\">Future development is focused on practical, multimodal tasks in new domains, like music or combinations with images.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">3. A robust evaluation framework and headroom baselines</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The primary goal of MSEB is to establish strong baselines and reveal the headroom in current AI models by evaluating them across two main task categories:</p><ul><li data-block-key=\"fe0k3\"><i>Semantic (e.g., voice search, reasoning)</i>: Do the models correctly understand the meaning and intent of the spoken words, even when the audio is noisy?</li><li data-block-key=\"e2afs\"><i>Acoustic (e.g., classification, clustering)</i>: Do the models accurately identify who is speaking or what the environmental sound is, regardless of meaning?</li></ul><p data-block-key=\"4ovqc\">The model-agnostic design of the MSEB library is built to evaluate a range of models — from cascade systems to novel end-to-end audio encoders — all within a standardized, comparative framework.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Comparison methodology</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">We used the MSEB framework to test the performance of current sound embedding models to see how close the models are to being truly intelligent and universal.</p><p data-block-key=\"7kl64\">For semantic tasks, the models were compared against the ground-truth text input. For non-semantic tasks, the models are compared against the best current dedicated solution to set a solid performance baseline that any new, general-purpose model must surpass.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Core limitations of existing sound representations</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The results demonstrate that existing AI models have measurable flaws across all key sound-understanding capabilities, which demonstrates the need for an evaluation framework like MSEB.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/image1.width-1250.png\" alt=\"Bar chart comparing performance metrics for Text and Sound inputs across &quot;SuperTasks&quot; like Retrieval, Reasoning, and Classification.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/image1.width-1250.png\" alt=\"Bar chart comparing performance metrics for Text and Sound inputs across &quot;SuperTasks&quot; like Retrieval, Reasoning, and Classification.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"i7px7\"><i>MSEB’s evaluation of AI models across key tasks shows important deficiencies and room for improvement. The metrics used for comparison include the</i> <a href=\"https://en.wikipedia.org/wiki/Mean_reciprocal_rank\" target=\"_blank\" rel=\"noopener noreferrer\"><i>MRR</i></a><i>,</i> <a href=\"https://en.wikipedia.org/wiki/F-score\" target=\"_blank\" rel=\"noopener noreferrer\"><i>F1</i></a><i>,</i> <a href=\"https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)\" target=\"_blank\" rel=\"noopener noreferrer\"><i>mAP</i></a><i>,</i> <a href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ACC</i></a><i>,</i> <a href=\"https://en.wikipedia.org/wiki/Word_error_rate\" target=\"_blank\" rel=\"noopener noreferrer\"><i>WER</i></a><i>,</i> <a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain\" target=\"_blank\" rel=\"noopener noreferrer\"><i>NDCG</i></a><i>,</i> <a href=\"https://www.cs.columbia.edu/~amaxwell/pubs/clustering_metric_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\"><i>VMeasure</i></a><i>, and</i> <a href=\"https://arxiv.org/abs/1812.08466\" target=\"_blank\" rel=\"noopener noreferrer\"><i>FAD</i></a><i>.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"04uji\">This evaluation reveals five major problems that currently limit the capability of sound-processing AI:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">1. Semantic bottlenecks</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">For tasks relying on language content (retrieval, reasoning, reranking), the ASR stage consistently and universally bottlenecks performance, resulting in loss of <a href=\"https://medium.com/@therealitydrift/semantic-fidelity-when-ai-gets-the-facts-right-but-the-meaning-wrong-8dd270b4017f\" target=\"_blank\" rel=\"noopener noreferrer\">semantic fidelity</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">2. Misaligned objectives</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The standard practice in speech technology involves a cascade model: transcribing speech to text, and then relying on that text for all downstream tasks. This is fundamentally wrong because it forces optimization onto the wrong metric. The ASR component is solely trained to minimize word error rate, a goal that is severely misaligned with the needs of real-world applications — which often requires maximizing the relevance, accuracy, or reasoning capability of the output, independent of perfect transcription.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">3. Non-universality</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">Models exhibit a severe lack of reliability, meaning performance varies drastically by language. The systems only work well for major, common languages. When tested on less common languages, the transcription quality collapses, causing critical task failures in search, ranking, and segmentation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">4. Lack of robustness</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The quality of sound reconstruction degrades sharply under noise. When background noise is introduced, the model’s ability to accurately interpret the original sound and environment struggles significantly. This establishes the most challenging benchmarks for the system, highlighting its difficulty in handling complex, general environmental sounds found in real-world settings (like a busy office or a noisy street).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">5. Over-complexity</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">For simple tasks that don't involve understanding meaning (like identifying who is speaking), complicated, pre-trained AI models are surprisingly no better than just using the raw representation of the sound waves. This often leads developers to waste their efforts on overly complex models when basic data works just as well.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The results demonstrate a substantial performance gap in existing general sound-based approaches across all eight super-tasks. This widespread underperformance, relative to the maximum potential defined by these ceilings, underscores the critical need for more research into unified and robust sound representations that can close the gap in machine auditory intelligence.</p><p data-block-key=\"fcbrv\">We envision MSEB as a dynamic and growing platform for the entire sound processing community. We invite you to contribute to this effort by using MSEB to evaluate your own sound representation techniques, contributing new tasks and datasets to the benchmark to help it grow, and joining the collaborative effort to push the boundaries of what's possible in machine sound intelligence.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\"><i>This project was led by Ehsan Variani, Georg Heigold, Tom Bagby, and Cyril Allauzen. The authors sincerely thank all who contributed to this project, whose critical input made it possible. We are especially grateful to our colleagues Hawi Abraham, Shankar Kumar, Ji Ma, Michael Riley, Sunil Vemuri, and Travis Trekel. We also wish to acknowledge those who helped prepare this post: Mark Simborg for his extensive editing, Kimberly Schwede for the wonderful illustrations, and Mickey Wurts for his valuable assistance.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                ",
      "contentSnippet": "Sound is a critical part of multimodal perception. For a system — be it a voice assistant, a next-generation security monitor, or an autonomous agent — to behave naturally, it must demonstrate a full spectrum of auditory capabilities. These capabilities include transcription, classification, retrieval, reasoning, segmentation, clustering, reranking, and reconstruction.\nThese diverse functions rely on transforming raw sound into an intermediate representation, or embedding. But research into improving the auditory capabilities of multimodal perception models has been fragmented, and there remain important unanswered questions: How do we compare performance across domains like human speech and bioacoustics? What is the true performance potential we are leaving on the table? And could a single, general-purpose sound embedding serve as the foundation for all these capabilities?\nTo investigate these queries and accelerate progress toward robust machine sound intelligence, we created the Massive Sound Embedding Benchmark (MSEB), presented at NeurIPS 2025.\nMSEB provides the necessary structure to answer these questions by:\n\nStandardizing evaluation for a comprehensive suite of eight real-world capabilities that we believe every human-like intelligent system must possess.\nProviding an open and extensible framework that allows researchers to seamlessly integrate and evaluate any model type — from conventional downstream uni-modal models to cascade models to end-to-end multimodal embedding models.\nEstablishing clear performance goals to objectively highlight research opportunities beyond current state-of-the-art approaches.\n\nOur initial experiments confirm that current sound representations are far from universal, revealing substantial performance \"headroom” (i.e., maximum improvement possible) across all eight tasks.\nThe three pillars of MSEB: A unified framework\nMSEB is built on three foundational pillars designed to provide the community with the tools needed to build the next generation of sound understanding models.\n1. Diverse datasets for real-world scenarios\nA benchmark is only as strong as its data. MSEB includes a curated collection of accessible datasets that better reflect our diverse global user community. The cornerstone of our benchmark is the Simple Voice Questions (SVQ) dataset, a new resource featuring 177,352 short, spoken queries across 26 locales and 17 languages. These recordings were captured in four distinct acoustic environments (clean, background speech, traffic noise, and media noise), and include rich metadata on speaker attributes and time-aligned salient terms. We collected and open-sourced this resource, available on Hugging Face.\nMSEB also integrates high-quality, public datasets covering a variety of sound domains:\n\nSpeech-MASSIVE: For multilingual spoken language understanding and intent classification.\nFSD50K: A large dataset for multi-label environmental sound event recognition (200 classes from the AudioSet Ontology).\nBirdSet: A massive-scale benchmark for avian bioacoustics, including complex soundscape recordings.\n\nWe’re actively working on creating and adding more relevant and large-scale datasets to MSEB. We invite the community to share their suggestions and express interest in collaboration through our GitHub repo.\n2. A comprehensive suite of eight core capabilities\nThe design of MSEB is built on the premise that the future of AI-based sound interaction is multimodal. Every task uses sound as the critical input, but also incorporates information from other modalities (like text context or knowledge bases) to simulate realistic scenarios.\nMSEB is structured around eight core “super-tasks”, i.e., tasks that represent a capability vital for an intelligent system:\n\nRetrieval (voice search): Simulates voice search by finding relevant documents or passages in a knowledge base from a spoken query.\nReasoning (intelligent assistants): Tests the ability to find a precise answer within a given document or passage based on a spoken question.\nClassification (monitoring/security): Categorizes sounds based on speaker attributes, user intent, recording environment, or specific sound events.\nTranscription: Converts the audio signal into a verbatim text representation (like automatic speech recognition, or ASR, for spoken languages).\nSegmentation (indexing): Identifies the most important terms within a sound clip and localizes them with precise start and end times.\nClustering (organization): Groups a collection of sound samples based on shared attributes (like speaker identity or environment) without relying on predefined labels.\nReranking (hypothesis refinement): Reorders a list of ambiguous text hypotheses (e.g., ASR output) to better match the original spoken query.\nReconstruction (generative AI): Tests the quality of the embedding by measuring the fidelity with which the original audio waveform can be regenerated from it.\n\n\n\n    \nMSEB tasks range from information access (retrieval, reranking, reasoning), to fundamental core perception (classification, transcription, segmentation), to higher-level organization generation (clustering, reconstruction).\nFuture development is focused on practical, multimodal tasks in new domains, like music or combinations with images.\n3. A robust evaluation framework and headroom baselines\nThe primary goal of MSEB is to establish strong baselines and reveal the headroom in current AI models by evaluating them across two main task categories:\n\nSemantic (e.g., voice search, reasoning): Do the models correctly understand the meaning and intent of the spoken words, even when the audio is noisy?\nAcoustic (e.g., classification, clustering): Do the models accurately identify who is speaking or what the environmental sound is, regardless of meaning?\n\nThe model-agnostic design of the MSEB library is built to evaluate a range of models — from cascade systems to novel end-to-end audio encoders — all within a standardized, comparative framework.\nComparison methodology\nWe used the MSEB framework to test the performance of current sound embedding models to see how close the models are to being truly intelligent and universal.\nFor semantic tasks, the models were compared against the ground-truth text input. For non-semantic tasks, the models are compared against the best current dedicated solution to set a solid performance baseline that any new, general-purpose model must surpass.\nCore limitations of existing sound representations\nThe results demonstrate that existing AI models have measurable flaws across all key sound-understanding capabilities, which demonstrates the need for an evaluation framework like MSEB.\nMSEB’s evaluation of AI models across key tasks shows important deficiencies and room for improvement. The metrics used for comparison include the MRR, F1, mAP, ACC, WER, NDCG, VMeasure, and FAD.\nThis evaluation reveals five major problems that currently limit the capability of sound-processing AI:\n1. Semantic bottlenecks\nFor tasks relying on language content (retrieval, reasoning, reranking), the ASR stage consistently and universally bottlenecks performance, resulting in loss of semantic fidelity.\n2. Misaligned objectives\nThe standard practice in speech technology involves a cascade model: transcribing speech to text, and then relying on that text for all downstream tasks. This is fundamentally wrong because it forces optimization onto the wrong metric. The ASR component is solely trained to minimize word error rate, a goal that is severely misaligned with the needs of real-world applications — which often requires maximizing the relevance, accuracy, or reasoning capability of the output, independent of perfect transcription.\n3. Non-universality\nModels exhibit a severe lack of reliability, meaning performance varies drastically by language. The systems only work well for major, common languages. When tested on less common languages, the transcription quality collapses, causing critical task failures in search, ranking, and segmentation.\n4. Lack of robustness\nThe quality of sound reconstruction degrades sharply under noise. When background noise is introduced, the model’s ability to accurately interpret the original sound and environment struggles significantly. This establishes the most challenging benchmarks for the system, highlighting its difficulty in handling complex, general environmental sounds found in real-world settings (like a busy office or a noisy street).\n5. Over-complexity\nFor simple tasks that don't involve understanding meaning (like identifying who is speaking), complicated, pre-trained AI models are surprisingly no better than just using the raw representation of the sound waves. This often leads developers to waste their efforts on overly complex models when basic data works just as well.\nConclusion\nThe results demonstrate a substantial performance gap in existing general sound-based approaches across all eight super-tasks. This widespread underperformance, relative to the maximum potential defined by these ceilings, underscores the critical need for more research into unified and robust sound representations that can close the gap in machine auditory intelligence.\nWe envision MSEB as a dynamic and growing platform for the entire sound processing community. We invite you to contribute to this effort by using MSEB to evaluate your own sound representation techniques, contributing new tasks and datasets to the benchmark to help it grow, and joining the collaborative effort to push the boundaries of what's possible in machine sound intelligence.\nAcknowledgements\nThis project was led by Ehsan Variani, Georg Heigold, Tom Bagby, and Cyril Allauzen. The authors sincerely thank all who contributed to this project, whose critical input made it possible. We are especially grateful to our colleagues Hawi Abraham, Shankar Kumar, Ji Ma, Michael Riley, Sunil Vemuri, and Travis Trekel. We also wish to acknowledge those who helped prepare this post: Mark Simborg for his extensive editing, Kimberly Schwede for the wonderful illustrations, and Mickey Wurts for his valuable assistance.",
      "creator": "Google",
      "summary": "文章介绍了新创建的“海量声音嵌入基准”（MSEB），旨在推动机器听觉智能的发展。MSEB通过标准化评估八种核心听觉能力（如转录、分类、检索等），提供开放框架和多样化数据集，帮助研究人员客观比较不同模型性能，揭示当前声音表示的改进空间。MSEB包含SVQ、Speech-MASSIVE、FSD50K等数据集，并支持多模态任务，以促进更全面的听觉智能研究。"
    },
    {
      "title": "Reducing EV range anxiety: How a simple AI model predicts port availability",
      "link": "https://research.google/blog/reducing-ev-range-anxiety-how-a-simple-ai-model-predicts-port-availability/",
      "pubDate": "Thu, 20 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-20T16:00:00.000Z",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"mhje4\">The transition to electric vehicles (EVs) is accelerating globally, bringing with it the critical need for a reliable and robust charging infrastructure. While building out more physical charging stations is an important step, an equally important task is maximizing the efficiency of this infrastructure and minimizing \"range anxiety”, a term used to describe an EV driver’s fear of running out of battery before reaching their destination or the nearest available charging station. These concerns led us to design an <a href=\"https://research.google/blog/addressing-range-anxiety-with-smart-electric-vehicle-routing/\">approach for EV routing</a> that reduces range anxiety by integrating charging stations into the navigational route based on the battery level and destination.</p><p data-block-key=\"d0uuh\">This week <a href=\"https://blog.google/products/maps/holiday-gemini-tips-new-explore-tab/\" target=\"_blank\" rel=\"noopener noreferrer\">we announced</a> a new lightweight, highly efficient prediction model that can answer the core question, “<i>What is the probability that an EV charging port will be available at a specific station a certain number of minutes from now?</i>” We found that the most sophisticated model isn't always the best solution. By co-designing the model and the deployment infrastructure, we were able to create a highly effective prediction system based on a simple <a href=\"https://en.wikipedia.org/wiki/Linear_regression\" target=\"_blank\" rel=\"noopener noreferrer\">linear regression</a> approach. This model’s simplicity is its strength, allowing it to rely on easily accessible features while still achieving performance improvements over a strong baseline. Our work demonstrates that combining intuitive real-world logic with machine learning can deliver significant operational and user experience benefits.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Creating the model</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">Our goal was to maximize predictive power while minimizing the feature set (i.e., the specific, measurable data points the model uses to make a prediction) to ensure speed and low-latency deployment. After testing various architectures, including a decision tree and a simple neural network, a straightforward linear regression model proved to be the most performant and robust for this specific task.</p><p data-block-key=\"73a8o\">We trained the model using real-time availability data from charging networks to calculate the true number of available charging ports within a certain number of minutes from the current observation time using criteria for model features and weights. We uniformly sampled ports from two distinct regions (CA and Germany). Larger stations were more likely to be included in the training set because they see more traffic than isolated ports and more closely reflect real-world usage.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Features</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">The model uses the hour of the day as a key piece of information (a \"feature\"). It treats each hour (or hour range) separately. For example, \"9 AM\" is one feature, and \"5 PM\" is another.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Weights</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">The \"weights\" are the specific numerical values that the linear regression algorithm learns during training. These numbers dictate how much each specific hour of the day affects the final prediction.</p><ul><li data-block-key=\"ed5n\">A positive weight means that during that hour (e.g., 7:00 AM), ports tend to get occupied (the occupancy is increasing).</li><li data-block-key=\"cleil\">A negative weight means that during that hour (e.g., 5:00 PM), ports tend to get freed up (the occupancy is decreasing).</li><li data-block-key=\"7cqut\">A zero or near-zero weight means that during that hour (e.g., 3:00 AM), there is little change in port status.</li></ul><p data-block-key=\"91vn8\">These “hour feature weights\" are the model's learned coefficients that quantify the predictable rate of EV port occupancy change for every hour of the day. Essentially, the model learns to express the difference between the current number of available ports and the future number of available ports as a function of the hour feature weights.</p><p data-block-key=\"eebfc\">The feature weights learned for each hour of the day are particularly insightful because they directly represent the rate at which port occupancy changes. As illustrated by the chart below, there are clear, predictable trends tied to driver schedules:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-1-30minWeights.width-1250.png\" alt=\"Plot of feature weights for each hour for the 30 minute horizon.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-1-30minWeights.width-1250.png\" alt=\"Plot of feature weights for each hour for the 30 minute horizon.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"ifwbm\"><i>Feature weights for each hour for the 30 minute horizon. They correspond to the rate at which port occupancy changes at each 30 minute bucket.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-2-60minWeights.width-1250.png\" alt=\"Plot of feature weights for each hour for the 60 minute horizon.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-2-60minWeights.width-1250.png\" alt=\"Plot of feature weights for each hour for the 60 minute horizon.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"ifwbm\"><i>Feature weights for each hour for the 60 minute horizon. They correspond to the rate at which port occupancy changes at each 60 minute bucket.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"mhje4\">Note that the model only differentiates from the current state when the change rate is significant (e.g., rush hour) or the station is large (more ports amplify the predicted change), which are intuitively the correct times to issue an updated prediction.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experiments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">Our evaluation was designed to be rigorous and representative of real-world usage. For both the 30-minute and 60-minute time horizons, we evaluated predictions on 100 randomly selected stations, sampling their occupancy status 48 times daily (every 30 minutes) for a full week.</p><p data-block-key=\"6khd7\">The model was benchmarked against a remarkably strong baseline: the \"Keep Current State\" approach. This baseline simply assumes that the number of available ports a certain number of minutes (<i>H</i>) in the future will be exactly the same as the current number.</p><p data-block-key=\"2lqm6\">While simple, this baseline is very hard to beat, especially over short horizons. For example, our data showed that on the US East Coast, never more than 10% of ports change their availability state within a 30-minute block. Since most of the time the state doesn't change, the simplest prediction — no change — is correct most of the time, making the task of adding predictive value extremely difficult.</p><p data-block-key=\"6taqm\">We focused on two key metrics to measure the model’s accuracy for predicting the exact number of free ports: mean squared error (MSE) and mean absolute error (MAE). A ratio of MSE/MAE ≥ 1 free port measures the accuracy of the most critical binary task for the user: “Will I find at least one free port (Yes/No)?”</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">The evaluation confirmed that the linear regression model provides crucial gains over the strong \"Keep Current State\" baseline, primarily by correctly identifying the infrequent, yet vital, moments of high occupancy turnover.</p><p data-block-key=\"em9tt\">We sampled test instances from among stations with at least 6 ports with horizons of 30 to 60 minutes, a realistic set of cases for charging in urban environments. We evaluated the model for the task of predicting the availability of at least one port in a station. This evaluation focused on the station profile and time of day when the model would differentiate from the baseline, namely large stations at times of significant rates of change.</p><p data-block-key=\"2selt\">The table below presents the fraction of time in which we provide a wrong prediction (which is equivalent to the MAE for this problem) for the times of highest change (8am and 8pm).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-3-ErrorRates.width-1250.png\" alt=\"Table comparing error rates on the availability of at least one free port (30 to 60-Minute Horizon).\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-3-ErrorRates.width-1250.png\" alt=\"Table comparing error rates on the availability of at least one free port (30 to 60-Minute Horizon).\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"ifwbm\"><i>Comparison of error rates on the availability of at least one free port (30 to 60-Minute Horizon).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"mhje4\">In summary, deploying the regression model allows us to reduce the number of bad predictions by approximately 20% in morning peak times and by approximately 40% in evening peak times.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Regional differences</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">Further examinations revealed that while the shape of the change rate curve (when ports fill vs. when they empty) is similar across regions, the magnitude of the change is distinct enough to warrant separate models. For instance, training the model separately for regions like California and Germany yielded better performance than pooling all data together, suggesting that it’s necessary to account for unique regional EV usage patterns.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">We have successfully developed and deployed a lightweight, linear regression model that effectively predicts EV charging port availability. By focusing on simplicity, speed, and co-designing the model with the existing infrastructure, we bypassed the complexity and latency associated with more detailed, but often unscalable, approaches.</p><p data-block-key=\"bg92t\">The resulting model provides a crucial predictive advantage over a strong \"Keep Current State\" baseline, particularly during high-traffic periods. This capability translates directly into an improved user experience: reduced anxiety, smarter routing decisions, and a better overall experience that supports the continued growth of electric mobility. Future work will focus on extending the prediction horizons to provide even greater value for long-distance travel planning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\"><i>We thank our collaborators Achir Ramadhan, Sreenivas Gollapudi, Shubham Gupta, Ilya Eyzerman, and Ivan Kuznetsov.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                ",
      "contentSnippet": "The transition to electric vehicles (EVs) is accelerating globally, bringing with it the critical need for a reliable and robust charging infrastructure. While building out more physical charging stations is an important step, an equally important task is maximizing the efficiency of this infrastructure and minimizing \"range anxiety”, a term used to describe an EV driver’s fear of running out of battery before reaching their destination or the nearest available charging station. These concerns led us to design an approach for EV routing that reduces range anxiety by integrating charging stations into the navigational route based on the battery level and destination.\nThis week we announced a new lightweight, highly efficient prediction model that can answer the core question, “What is the probability that an EV charging port will be available at a specific station a certain number of minutes from now?” We found that the most sophisticated model isn't always the best solution. By co-designing the model and the deployment infrastructure, we were able to create a highly effective prediction system based on a simple linear regression approach. This model’s simplicity is its strength, allowing it to rely on easily accessible features while still achieving performance improvements over a strong baseline. Our work demonstrates that combining intuitive real-world logic with machine learning can deliver significant operational and user experience benefits.\nCreating the model\nOur goal was to maximize predictive power while minimizing the feature set (i.e., the specific, measurable data points the model uses to make a prediction) to ensure speed and low-latency deployment. After testing various architectures, including a decision tree and a simple neural network, a straightforward linear regression model proved to be the most performant and robust for this specific task.\nWe trained the model using real-time availability data from charging networks to calculate the true number of available charging ports within a certain number of minutes from the current observation time using criteria for model features and weights. We uniformly sampled ports from two distinct regions (CA and Germany). Larger stations were more likely to be included in the training set because they see more traffic than isolated ports and more closely reflect real-world usage.\nFeatures\nThe model uses the hour of the day as a key piece of information (a \"feature\"). It treats each hour (or hour range) separately. For example, \"9 AM\" is one feature, and \"5 PM\" is another.\nWeights\nThe \"weights\" are the specific numerical values that the linear regression algorithm learns during training. These numbers dictate how much each specific hour of the day affects the final prediction.\n\nA positive weight means that during that hour (e.g., 7:00 AM), ports tend to get occupied (the occupancy is increasing).\nA negative weight means that during that hour (e.g., 5:00 PM), ports tend to get freed up (the occupancy is decreasing).\nA zero or near-zero weight means that during that hour (e.g., 3:00 AM), there is little change in port status.\n\nThese “hour feature weights\" are the model's learned coefficients that quantify the predictable rate of EV port occupancy change for every hour of the day. Essentially, the model learns to express the difference between the current number of available ports and the future number of available ports as a function of the hour feature weights.\nThe feature weights learned for each hour of the day are particularly insightful because they directly represent the rate at which port occupancy changes. As illustrated by the chart below, there are clear, predictable trends tied to driver schedules:\nFeature weights for each hour for the 30 minute horizon. They correspond to the rate at which port occupancy changes at each 30 minute bucket.\nFeature weights for each hour for the 60 minute horizon. They correspond to the rate at which port occupancy changes at each 60 minute bucket.\nNote that the model only differentiates from the current state when the change rate is significant (e.g., rush hour) or the station is large (more ports amplify the predicted change), which are intuitively the correct times to issue an updated prediction.\nExperiments\nOur evaluation was designed to be rigorous and representative of real-world usage. For both the 30-minute and 60-minute time horizons, we evaluated predictions on 100 randomly selected stations, sampling their occupancy status 48 times daily (every 30 minutes) for a full week.\nThe model was benchmarked against a remarkably strong baseline: the \"Keep Current State\" approach. This baseline simply assumes that the number of available ports a certain number of minutes (H) in the future will be exactly the same as the current number.\nWhile simple, this baseline is very hard to beat, especially over short horizons. For example, our data showed that on the US East Coast, never more than 10% of ports change their availability state within a 30-minute block. Since most of the time the state doesn't change, the simplest prediction — no change — is correct most of the time, making the task of adding predictive value extremely difficult.\nWe focused on two key metrics to measure the model’s accuracy for predicting the exact number of free ports: mean squared error (MSE) and mean absolute error (MAE). A ratio of MSE/MAE ≥ 1 free port measures the accuracy of the most critical binary task for the user: “Will I find at least one free port (Yes/No)?”\nResults\nThe evaluation confirmed that the linear regression model provides crucial gains over the strong \"Keep Current State\" baseline, primarily by correctly identifying the infrequent, yet vital, moments of high occupancy turnover.\nWe sampled test instances from among stations with at least 6 ports with horizons of 30 to 60 minutes, a realistic set of cases for charging in urban environments. We evaluated the model for the task of predicting the availability of at least one port in a station. This evaluation focused on the station profile and time of day when the model would differentiate from the baseline, namely large stations at times of significant rates of change.\nThe table below presents the fraction of time in which we provide a wrong prediction (which is equivalent to the MAE for this problem) for the times of highest change (8am and 8pm).\nComparison of error rates on the availability of at least one free port (30 to 60-Minute Horizon).\nIn summary, deploying the regression model allows us to reduce the number of bad predictions by approximately 20% in morning peak times and by approximately 40% in evening peak times.\nRegional differences\nFurther examinations revealed that while the shape of the change rate curve (when ports fill vs. when they empty) is similar across regions, the magnitude of the change is distinct enough to warrant separate models. For instance, training the model separately for regions like California and Germany yielded better performance than pooling all data together, suggesting that it’s necessary to account for unique regional EV usage patterns.\nConclusion\nWe have successfully developed and deployed a lightweight, linear regression model that effectively predicts EV charging port availability. By focusing on simplicity, speed, and co-designing the model with the existing infrastructure, we bypassed the complexity and latency associated with more detailed, but often unscalable, approaches.\nThe resulting model provides a crucial predictive advantage over a strong \"Keep Current State\" baseline, particularly during high-traffic periods. This capability translates directly into an improved user experience: reduced anxiety, smarter routing decisions, and a better overall experience that supports the continued growth of electric mobility. Future work will focus on extending the prediction horizons to provide even greater value for long-distance travel planning.\nAcknowledgements\nWe thank our collaborators Achir Ramadhan, Sreenivas Gollapudi, Shubham Gupta, Ilya Eyzerman, and Ivan Kuznetsov.",
      "creator": "Google",
      "summary": "文章介绍了一种基于简单线性回归的AI模型，用于预测电动汽车充电桩在一定时间内的可用性，以减少驾驶者的里程焦虑。该模型通过分析实时充电数据，结合小时特征权重，预测充电桩占用率的变化趋势，实现高效、低延迟的预测。研究结果表明，简单模型结合直观逻辑与机器学习，可有效提升运营效率和用户体验。"
    },
    {
      "title": "Real-time speech-to-speech translation",
      "link": "https://research.google/blog/real-time-speech-to-speech-translation/",
      "pubDate": "Tue, 18 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-18T16:00:00.000Z",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"gi1wx\">Real-time communication is an integral part of both our professional and personal lives. When speaking to people remotely across language barriers, it can be difficult to truly connect by just relying on state-of-the-art translated captions, as they lack personality and real-time responsiveness essential for fluid conversation. The arrival of <a href=\"https://research.google/blog/introducing-translatotron-an-end-to-end-speech-to-speech-translation-model\">speech-to-speech translation</a> (S2ST) bridges this gap by directly generating translated audio, leading to more natural communication. Existing speech-to-speech translation systems often incur significant delays (4–5s), tend to accumulate errors, and typically lack personalization.</p><p data-block-key=\"325ls\">Today we describe an innovative end-to-end S2ST model that overcomes these limitations, enabling live translation in the original speaker's voice with only 2 second delay. The novel architecture leverages a streaming framework and, with training on time-synchronized data, significantly reduces the delay between the original input and the translated speech. To support a breadth of languages, we introduce a scalable time-synced data acquisition pipeline that allows us to gradually expand the system to include more languages. This technology has demonstrated its effectiveness through successful deployment in real-time sensitive use cases.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qrznl\">Cascaded S2ST</h2><p data-block-key=\"6p2r8\">Prior real-time speech-to-speech technologies employed a cascaded pipeline of individual processing blocks:</p><ol><li data-block-key=\"6s8ba\">Firstly, the source audio is transcribed to text using <a href=\"https://en.wikipedia.org/wiki/Speech_recognition\" target=\"_blank\" rel=\"noopener noreferrer\">automatic speech recognition</a> (ASR) AI models.</li><li data-block-key=\"fuunf\">Next, the transcribed text is translated word-for-word to the target language using <a href=\"https://en.wikipedia.org/wiki/Machine_translation\" target=\"_blank\" rel=\"noopener noreferrer\">automatic speech translation</a> (AST).</li><li data-block-key=\"9mjqi\">Finally, the translated text is converted back to audio using text-to-speech pipelines (TTS).</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST1_Cascade.width-1250.png\" alt=\"RTS2ST1_Cascade\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST1_Cascade.width-1250.png\" alt=\"RTS2ST1_Cascade\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"b1hy5\"><i>Schematic representation of classic, cascade-style speech-to-speech translation system.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qrznl\">Despite the high quality of the individual cascade components, achieving a seamless, real-time S2ST experience has been challenging due to three primary factors:</p><ol><li data-block-key=\"grol\">Significant delays of 4–5 seconds, forcing turn-based conversations.</li><li data-block-key=\"bbctr\">Accumulated errors at each stage of the translation process.</li><li data-block-key=\"ba4or\">A notable lack of personalization due to the general-purpose TTS technology.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qrznl\">A novel end-to-end, personalized S2ST</h2><p data-block-key=\"3e2vl\">To significantly advance S2ST, we created a scalable data acquisition pipeline and developed an end-to-end model that provides direct, real-time language translation with just a two-second delay:</p><ol><li data-block-key=\"fl20f\"><i>Scalable data acquisition pipeline</i>: We created a data processing pipeline to convert raw audio into a time-synchronized input/target dataset. This was achieved by integrating existing ASR and TTS technologies with precise alignment steps, ensuring translated audio best matches the input. Rigorous filtering and validation were employed to remove difficult-to-align examples.</li><li data-block-key=\"32p9v\"><i>Real-time speech-to-speech translation architecture</i>: We introduced an audio-specific streaming machine learning architecture to support training on this time-synchronized data. Building on the <a href=\"https://research.google/blog/audiolm-a-language-modeling-approach-to-audio-generation/\">AudioLM framework</a> and fundamental transformer blocks, this architecture is designed to handle continuous audio streams, allowing the model to decide when to output translations. It is also structured to manage hierarchical audio representations using the <a href=\"https://arxiv.org/abs/2508.05207\" target=\"_blank\" rel=\"noopener noreferrer\">SpectroStream</a> codec technology.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    <div class=\"rich-text --theme- --mode-\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p style=\"text-align: center;\"><video muted=\"\" controls=\"controls\" width=\"67%\" height=\"67%\"> <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/spanish_english_s2st_viz.mp4\" type=\"video/mp4\"> </video></p>\n<table class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\" cellspacing=\"0\" cellpadding=\"0\" align=\"center\">\n<tbody>\n<tr>\n<td class=\"tr-caption\" style=\"text-align: center;\">An example of our personalized S2ST applied to a Spanish original translated to English.</td>\n</tr>\n</tbody>\n</table>\n</div>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qrznl\">Scalable data acquisition pipeline</h2><p data-block-key=\"eo8vt\">For a given language pair, initial work starts with raw audio acquisition. We utilize a diverse set of audio sources, including data generated by TTS models. This audio undergoes a cleaning and filtering process to ensure it contains a single speaker of the source language and has an appropriate noise level. After initial data collection, an ASR step transcribes the source text. With both source audio and text available, <a href=\"https://www.danielpovey.com/files/htkbook.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">the forced alignment algorithm</a> generates alignment timesteps (audio-to-text mapping). Any audio segments where alignment fails are discarded.</p><p data-block-key=\"6hb40\">The remaining clips are machine translated from the source into the target language. Subsequently, a series of automated filters validate the translated output, ensuring accuracy and correspondence to the input text. Next, the original transcribed and translated texts are also aligned to generate corresponding timestamp annotations (text-to-translated text mapping).</p><p data-block-key=\"baqfg\">Using <a href=\"https://research.google/blog/audiolm-a-language-modeling-approach-to-audio-generation/\">a custom text-to-speech generation engine</a>, the translated text is converted into translated audio, preserving the voice characteristics from the original audio while producing natural-sounding output. The pipeline concludes with one more forced alignment step of the translated text and the generated speech (speech-to-text mapping).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST2_Pipeline.width-1250.png\" alt=\"RTS2ST2_Pipeline\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST2_Pipeline.width-1250.png\" alt=\"RTS2ST2_Pipeline\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"b1hy5\"><i>Streaming audio-to-audio translation dataset generation.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qrznl\">Utilizing the three generated alignments from the previous steps, the overlap between them is calculated, yielding alignment masks between the source and target audio. These alignment masks are then used to guide the loss computation during training.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-vertical-padding --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST3.width-1250.png\" alt=\"RTS2ST3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST3.width-1250.png\" alt=\"RTS2ST3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n\n  \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n    \n\n\n\n\n\n<div class=\"sound-player --theme-light --mode-standalone\" data-gt-id=\"sound_player\" data-gt-component-name=\"component name\">\n    <div class=\"sound-player__wrapper\">\n        \n            \n\n\n<div class=\"sound-player__column\">\n    \n\n\n\n\n    \n\n<audio controls=\"\">\n    \n    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/RealTimeS2ST_es_alignment.wav\" type=\"audio/wav\">\n    Your browser does not support the audio element.\n</audio>\n\n    \n\n    \n</div>\n\n        \n    </div>\n</div>\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-vertical-padding --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST4.width-1250.png\" alt=\"RTS2ST4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST4.width-1250.png\" alt=\"RTS2ST4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n\n  \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n    \n\n\n\n\n\n<div class=\"sound-player --theme-light --mode-standalone\" data-gt-id=\"sound_player\" data-gt-component-name=\"component name\">\n    <div class=\"sound-player__wrapper\">\n        \n            \n\n\n<div class=\"sound-player__column\">\n    \n\n\n\n\n    \n\n<audio controls=\"\">\n    \n    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/RealTimeS2ST_en_alignment.wav\" type=\"audio/wav\">\n    Your browser does not support the audio element.\n</audio>\n\n    \n        <div class=\"caption --center\">\n            <p data-block-key=\"80ok3\"><i>Text alignment of input and translated audio with corresponding overlaps.</i></p>\n        </div>\n    \n\n    \n</div>\n\n        \n    </div>\n</div>\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"gi1wx\">Invalid overlaps or translations that fail to meet delay requirements are filtered from the training dataset. The remaining aligned data is used to train the streaming S2ST model in chunks of up to 60 seconds. Various audio augmentation techniques are also applied during training, including sample rate reduction, reverberation, saturation, and denoising.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qrznl\">Real-time speech-to-speech translation architecture</h2><p data-block-key=\"38c83\">The end-to-end S2ST model leverages fundamental transformer blocks and consists of two main components:</p><ul><li data-block-key=\"5ra6j\"><i>Streaming encoder:</i> Summarizes source audio data based on the preceding 10 seconds of input.</li><li data-block-key=\"moc\"><i>Streaming decoder:</i> Predicts translated audio autoregressively, using the compressed encoder state and predictions from previous iterations.</li></ul><p data-block-key=\"2g26o\">A feature of these models is their representation of audio as a 2D set of tokens, known as <a href=\"https://arxiv.org/abs/2107.03312\" target=\"_blank\" rel=\"noopener noreferrer\">RVQ audio tokens</a>. As shown below, the X-axis represents time, while the Y-axis represents a set of tokens that describe the current audio segment. When summed, all tokens in a specific set can be readily converted into an audio stream using an ML codec. The number of tokens controls the audio quality for every segment, with more tokens yielding higher fidelity. The model predicts tokens sequentially, prioritizing those at the beginning. Typically, 16 tokens are sufficient for high-quality audio representation of a 100 ms chunk.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST5.width-1250.png\" alt=\"RTS2ST5\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST5.width-1250.png\" alt=\"RTS2ST5\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"b1hy5\"><i>Schematic representation of audio-to-audio streaming inference for S2ST.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qrznl\">The model outputs a single text token in addition to the audio tokens. This text token acts as an extra prior for audio generation and enables direct metric calculation (<a href=\"https://cloud.google.com/translate/docs/advanced/automl-evaluate#bleu\" target=\"_blank\" rel=\"noopener noreferrer\">BLEU</a>) without relying on proxy ASR systems.</p><p data-block-key=\"dh8dl\">During training, a per-token loss is applied to the model to ensure accurate translation. The model's prediction delay, or lookahead, can be adjusted by shifting ground truth tokens to the right, allowing for flexibility based on the target language's complexity. For real-time conversations, a standard 2-second delay is typically used, which is suitable for most languages. While a longer lookahead improves translation quality by providing more context, it negatively impacts the real-time communication experience.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RealTimeS2ST6_Ablation.width-1250.png\" alt=\"RealTimeS2ST6_Ablation\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RealTimeS2ST6_Ablation.width-1250.png\" alt=\"RealTimeS2ST6_Ablation\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"b1hy5\"><i>Ablation of lookahead and corresponding quality of translation for Spanish / English language pair (BLEU, higher is better).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qrznl\">In addition to the internal 2 second delay, the model's inference time contributes to the overall system latency. To minimize this and achieve real-time performance, we implemented several optimization techniques, including hybrid low-bit (<a href=\"https://blog.tensorflow.org/2024/04/faster-dynamically-quantized-inference-with-xnnpack.html\" target=\"_blank\" rel=\"noopener noreferrer\">int8 and int4</a>) quantization and optimized <a href=\"https://arxiv.org/abs/2207.12598\" target=\"_blank\" rel=\"noopener noreferrer\">CFG</a> precomputation.</p><p data-block-key=\"bv7i0\">The examples of translation for different language pairs with developed models with corresponding ground truth (taken from publicly available <a href=\"https://arxiv.org/abs/2201.03713\" target=\"_blank\" rel=\"noopener noreferrer\">CVSS dataset</a>) follow:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    <div class=\"rich-text --theme- --mode-\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <table style=\"border-collapse: collapse; width: 100%;\" border=\"0\">\n<tbody>\n<tr>\n<td style=\"width: 15%;\">\n<p><strong>Direction</strong></p>\n</td>\n<td style=\"width: 24.973%; text-align: center;\">\n<p><strong>Input audio</strong></p>\n</td>\n<td style=\"width: 24.973%; text-align: center;\">\n<p><strong>Translated audio</strong></p>\n</td>\n<td style=\"width: 34%; text-align: center;\">\n<p><strong>Ground truth</strong></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">Spanish to English</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/es_en_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/es_en_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">In coastal areas there is a larger accumulation of water molecules in the air.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">English to Spanish</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_es_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_es_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">Su portaaviones insignia, el Portaviones Formidable fue tocado por un kamikaze sin gravísimas consecuencias.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">German to English</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/de_en_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/de_en_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">The electrician used a piece of aluminum foil to splice the fuse.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\"> English to German</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_de_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_de_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">Margaret versucht mit allen Mitteln, die bevorstehende Katastrophe zu verhindern.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">Italian to English</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/it_en_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/it_en_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">By clicking on the right, on the bell icon, you can activate Pash notifications so that they are updated in real time.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">English to Italian</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_it_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_it_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">La voce popolare parla dei proprietari terrieri, dei mafiosi e dei rappresentanti del Partito conservatore e dei loro nomi, che sono noti a tutti.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">Portuguese to English</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/pt_en_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/pt_en_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">This text is made available under the respective license.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">English to Portuguese</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_pt_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_pt_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">Um homem de camisa azul observa algo projetado na frente dele.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">French to English</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/fr_en_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/fr_en_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">The volunteer firefighters are a full part of our civil security arsenal.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">English to French</span></p>\n</td>\n<td style=\"width: 24.973%;\">\n<p><span style=\"font-weight: 400;\">&nbsp;<audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_fr_source.wav\" controls=\"controls\"></audio></span></p>\n</td>\n<td style=\"width: 24.973%;\">\n<p><span style=\"font-weight: 400;\">&nbsp;<audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_fr_translation.wav\" controls=\"controls\"></audio></span></p>\n</td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">Ce gouvernement et cette majorité portent donc seuls la responsabilité de cette situation.</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n<table class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\" cellspacing=\"0\" cellpadding=\"0\" align=\"center\">\n<tbody>\n<tr>\n<td class=\"tr-caption\" style=\"text-align: center;\">Exemplar bidirectional translation generated by the trained models for English, Spanish, German, Italian, Portuguese, and French languages with corresponding ground truth.</td>\n</tr>\n</tbody>\n</table>\n</div>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qrznl\">Real-world applications</h2><p data-block-key=\"e3d53\">The new end-to-end S2ST technology has been launched in two key areas, highlighting the importance of real-time cross-language communication. It is now available in <a href=\"https://blog.google/products/workspace/google-meet-langauge-translation-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Meet</a> on servers, and as a built-in <a href=\"https://store.google.com/intl/en/ideas/articles/pixel-live-translate/\" target=\"_blank\" rel=\"noopener noreferrer\">on-device feature</a> for the new Pixel 10 devices. Although the products utilize different strategies for running the S2ST pipeline, they share training data and model architecture. The Pixel Voice Translate on-device feature also employs a cascade approach to maximize language coverage. To mitigate potential feature misuse, prior to each translation session, we inform the end-user that the translation is synthetically generated.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"hyXqcsWOONo\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=hyXqcsWOONo\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"5y2ft\"><i>The new end-to-end S2ST technology enables Google Meet speech translation feature.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"k3ba8\">The current end-to-end model delivers robust performance for five Latin-based language pairs (English to and from Spanish, German, French, Italian, Portuguese), enabling our initial product launches. We are also observing promising capabilities in other languages, such as Hindi, that we plan to develop further. Future enhancements will focus on improving the dynamism of the model's lookahead. This will enable the S2ST technology to seamlessly adjust to languages with word orders significantly different from English, facilitating more contextual rather than literal word-for-word translation.</p><p data-block-key=\"e0dg1\">We believe that this breakthrough in S2ST technology will revolutionize real-time, cross-language communication, turning a long-envisioned concept into reality.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"k3ba8\">Acknowledgements</h2><p data-block-key=\"f31cm\"><i>We are sincerely grateful to everyone who contributed to this project; their critical contributions were instrumental in making it a reality. We are particularly thankful to our colleagues Kevin Kilgour, Pen Li, Félix de Chaumont Quitry, Michael Dooley, Jeremy Thorpe, Mihajlo Velimirović, Alex Tudor, Christian Frank, Daniel Johansson, Hanna Silén, Christian Schuldt, Henrik Lundin, Esbjörn Dominique, Marcus Wirebrand, Daniel Kallander, Pablo Barrera González, Huib Kleinhout, Niklas Blum, Fredric Lindstrom, Esha Uboweja, Karthik Raveendran, Frédéric Rechtenstein, Xing Li, Queenie Zhang, Cheng Yang, Jason Fan, Matsvei Zhdanovich, Jianing Wei, and Matthias Grundmann.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                ",
      "contentSnippet": "Real-time communication is an integral part of both our professional and personal lives. When speaking to people remotely across language barriers, it can be difficult to truly connect by just relying on state-of-the-art translated captions, as they lack personality and real-time responsiveness essential for fluid conversation. The arrival of speech-to-speech translation (S2ST) bridges this gap by directly generating translated audio, leading to more natural communication. Existing speech-to-speech translation systems often incur significant delays (4–5s), tend to accumulate errors, and typically lack personalization.\nToday we describe an innovative end-to-end S2ST model that overcomes these limitations, enabling live translation in the original speaker's voice with only 2 second delay. The novel architecture leverages a streaming framework and, with training on time-synchronized data, significantly reduces the delay between the original input and the translated speech. To support a breadth of languages, we introduce a scalable time-synced data acquisition pipeline that allows us to gradually expand the system to include more languages. This technology has demonstrated its effectiveness through successful deployment in real-time sensitive use cases.\nCascaded S2ST\nPrior real-time speech-to-speech technologies employed a cascaded pipeline of individual processing blocks:\n\nFirstly, the source audio is transcribed to text using automatic speech recognition (ASR) AI models.\nNext, the transcribed text is translated word-for-word to the target language using automatic speech translation (AST).\nFinally, the translated text is converted back to audio using text-to-speech pipelines (TTS).\n\n\n\n    \nSchematic representation of classic, cascade-style speech-to-speech translation system.\nDespite the high quality of the individual cascade components, achieving a seamless, real-time S2ST experience has been challenging due to three primary factors:\n\nSignificant delays of 4–5 seconds, forcing turn-based conversations.\nAccumulated errors at each stage of the translation process.\nA notable lack of personalization due to the general-purpose TTS technology.\n\n\n\n    \nA novel end-to-end, personalized S2ST\nTo significantly advance S2ST, we created a scalable data acquisition pipeline and developed an end-to-end model that provides direct, real-time language translation with just a two-second delay:\n\nScalable data acquisition pipeline: We created a data processing pipeline to convert raw audio into a time-synchronized input/target dataset. This was achieved by integrating existing ASR and TTS technologies with precise alignment steps, ensuring translated audio best matches the input. Rigorous filtering and validation were employed to remove difficult-to-align examples.\nReal-time speech-to-speech translation architecture: We introduced an audio-specific streaming machine learning architecture to support training on this time-synchronized data. Building on the AudioLM framework and fundamental transformer blocks, this architecture is designed to handle continuous audio streams, allowing the model to decide when to output translations. It is also structured to manage hierarchical audio representations using the SpectroStream codec technology.\n\n\n\n    \n  \nAn example of our personalized S2ST applied to a Spanish original translated to English.\n\n\n\n\n\n                    \n                    \n    \n\n\n\n    \nScalable data acquisition pipeline\nFor a given language pair, initial work starts with raw audio acquisition. We utilize a diverse set of audio sources, including data generated by TTS models. This audio undergoes a cleaning and filtering process to ensure it contains a single speaker of the source language and has an appropriate noise level. After initial data collection, an ASR step transcribes the source text. With both source audio and text available, the forced alignment algorithm generates alignment timesteps (audio-to-text mapping). Any audio segments where alignment fails are discarded.\nThe remaining clips are machine translated from the source into the target language. Subsequently, a series of automated filters validate the translated output, ensuring accuracy and correspondence to the input text. Next, the original transcribed and translated texts are also aligned to generate corresponding timestamp annotations (text-to-translated text mapping).\nUsing a custom text-to-speech generation engine, the translated text is converted into translated audio, preserving the voice characteristics from the original audio while producing natural-sounding output. The pipeline concludes with one more forced alignment step of the translated text and the generated speech (speech-to-text mapping).\nStreaming audio-to-audio translation dataset generation.\nUtilizing the three generated alignments from the previous steps, the overlap between them is calculated, yielding alignment masks between the source and target audio. These alignment masks are then used to guide the loss computation during training.\nText alignment of input and translated audio with corresponding overlaps.\nInvalid overlaps or translations that fail to meet delay requirements are filtered from the training dataset. The remaining aligned data is used to train the streaming S2ST model in chunks of up to 60 seconds. Various audio augmentation techniques are also applied during training, including sample rate reduction, reverberation, saturation, and denoising.\nReal-time speech-to-speech translation architecture\nThe end-to-end S2ST model leverages fundamental transformer blocks and consists of two main components:\n\nStreaming encoder: Summarizes source audio data based on the preceding 10 seconds of input.\nStreaming decoder: Predicts translated audio autoregressively, using the compressed encoder state and predictions from previous iterations.\n\nA feature of these models is their representation of audio as a 2D set of tokens, known as RVQ audio tokens. As shown below, the X-axis represents time, while the Y-axis represents a set of tokens that describe the current audio segment. When summed, all tokens in a specific set can be readily converted into an audio stream using an ML codec. The number of tokens controls the audio quality for every segment, with more tokens yielding higher fidelity. The model predicts tokens sequentially, prioritizing those at the beginning. Typically, 16 tokens are sufficient for high-quality audio representation of a 100 ms chunk.\nSchematic representation of audio-to-audio streaming inference for S2ST.\nThe model outputs a single text token in addition to the audio tokens. This text token acts as an extra prior for audio generation and enables direct metric calculation (BLEU) without relying on proxy ASR systems.\nDuring training, a per-token loss is applied to the model to ensure accurate translation. The model's prediction delay, or lookahead, can be adjusted by shifting ground truth tokens to the right, allowing for flexibility based on the target language's complexity. For real-time conversations, a standard 2-second delay is typically used, which is suitable for most languages. While a longer lookahead improves translation quality by providing more context, it negatively impacts the real-time communication experience.\nAblation of lookahead and corresponding quality of translation for Spanish / English language pair (BLEU, higher is better).\nIn addition to the internal 2 second delay, the model's inference time contributes to the overall system latency. To minimize this and achieve real-time performance, we implemented several optimization techniques, including hybrid low-bit (int8 and int4) quantization and optimized CFG precomputation.\nThe examples of translation for different language pairs with developed models with corresponding ground truth (taken from publicly available CVSS dataset) follow:\nDirection\nInput audio\nTranslated audio\nGround truth\nSpanish to English\n\n\n\nIn coastal areas there is a larger accumulation of water molecules in the air.\nEnglish to Spanish\n\n\n\nSu portaaviones insignia, el Portaviones Formidable fue tocado por un kamikaze sin gravísimas consecuencias.\nGerman to English\n\n\n\nThe electrician used a piece of aluminum foil to splice the fuse.\n English to German\n\n\n\nMargaret versucht mit allen Mitteln, die bevorstehende Katastrophe zu verhindern.\nItalian to English\n\n\n\nBy clicking on the right, on the bell icon, you can activate Pash notifications so that they are updated in real time.\nEnglish to Italian\n\n\n\nLa voce popolare parla dei proprietari terrieri, dei mafiosi e dei rappresentanti del Partito conservatore e dei loro nomi, che sono noti a tutti.\nPortuguese to English\n\n\n\nThis text is made available under the respective license.\nEnglish to Portuguese\n\n\n\nUm homem de camisa azul observa algo projetado na frente dele.\nFrench to English\n\n\n\nThe volunteer firefighters are a full part of our civil security arsenal.\nEnglish to French\n \n \nCe gouvernement et cette majorité portent donc seuls la responsabilité de cette situation.\nExemplar bidirectional translation generated by the trained models for English, Spanish, German, Italian, Portuguese, and French languages with corresponding ground truth.\n\n\n\n\n\n                    \n                    \n    \n\n\n\n    \nReal-world applications\nThe new end-to-end S2ST technology has been launched in two key areas, highlighting the importance of real-time cross-language communication. It is now available in Google Meet on servers, and as a built-in on-device feature for the new Pixel 10 devices. Although the products utilize different strategies for running the S2ST pipeline, they share training data and model architecture. The Pixel Voice Translate on-device feature also employs a cascade approach to maximize language coverage. To mitigate potential feature misuse, prior to each translation session, we inform the end-user that the translation is synthetically generated.\n\n      \n      \nWatch the film\nLink to Youtube Video\nThe new end-to-end S2ST technology enables Google Meet speech translation feature.\nThe current end-to-end model delivers robust performance for five Latin-based language pairs (English to and from Spanish, German, French, Italian, Portuguese), enabling our initial product launches. We are also observing promising capabilities in other languages, such as Hindi, that we plan to develop further. Future enhancements will focus on improving the dynamism of the model's lookahead. This will enable the S2ST technology to seamlessly adjust to languages with word orders significantly different from English, facilitating more contextual rather than literal word-for-word translation.\nWe believe that this breakthrough in S2ST technology will revolutionize real-time, cross-language communication, turning a long-envisioned concept into reality.\nAcknowledgements\nWe are sincerely grateful to everyone who contributed to this project; their critical contributions were instrumental in making it a reality. We are particularly thankful to our colleagues Kevin Kilgour, Pen Li, Félix de Chaumont Quitry, Michael Dooley, Jeremy Thorpe, Mihajlo Velimirović, Alex Tudor, Christian Frank, Daniel Johansson, Hanna Silén, Christian Schuldt, Henrik Lundin, Esbjörn Dominique, Marcus Wirebrand, Daniel Kallander, Pablo Barrera González, Huib Kleinhout, Niklas Blum, Fredric Lindstrom, Esha Uboweja, Karthik Raveendran, Frédéric Rechtenstein, Xing Li, Queenie Zhang, Cheng Yang, Jason Fan, Matsvei Zhdanovich, Jianing Wei, and Matthias Grundmann.",
      "creator": "Google",
      "summary": "实时语音转语音翻译技术通过直接生成翻译音频，解决了传统字幕翻译缺乏实时性和个性化的问题。该技术采用端到端模型，将延迟缩短至2秒，并支持多种语言。通过创新的流式框架和同步数据训练，系统实现了自然、实时的跨语言交流，已在多个实时应用场景中成功部署。"
    },
    {
      "title": "Generative UI: A rich, custom, visual interactive user experience for any prompt",
      "link": "https://research.google/blog/generative-ui-a-rich-custom-visual-interactive-user-experience-for-any-prompt/",
      "pubDate": "Mon, 17 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-17T16:00:00.000Z",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"j0qlf\">Generative UI is a powerful capability in which an AI model generates not only content but an entire user experience. Today we introduce a novel implementation of generative UI, which dynamically creates immersive visual experiences and interactive interfaces — such as web pages, games, tools, and applications — that are automatically designed and fully customized in response to any question, instruction, or prompt. These prompts can be as simple as a single word, or as long as needed for detailed instructions. These new types of interfaces are markedly different from the static, predefined interfaces in which AI models typically render content.</p><p data-block-key=\"f5456\">In our new paper, “<a href=\"https://generativeui.github.io/static/pdfs/paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Generative UI: LLMs are Effective UI Generators</a>”, we describe the core principles that enabled our implementation of generative UI and demonstrate the effective viability of this new paradigm. Our evaluations indicate that, when ignoring generation speed, the interfaces from our generative UI implementations are strongly preferred by human raters compared to standard LLM outputs. This work represents a first step toward fully AI-generated user experiences, where users automatically get dynamic interfaces tailored to their needs, rather than having to select from an existing catalog of applications.</p><p data-block-key=\"9kn4n\">Our research on generative UI, also referred to as generative interfaces, comes to life today in the <a href=\"https://blog.google/products/gemini/gemini-3-gemini-app\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini app</a> through an experiment called dynamic view and in AI Mode in <a href=\"http://blog.google/products/search/gemini-3-search-ai-mode\" target=\"_blank\" rel=\"noopener noreferrer\">Google Search</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-0b-Hero.width-1250.png\" alt=\"A collage showing three different AI-generated user interfaces. They include a page for &quot;Tailored Fashion Advice&quot;, a science education screen featuring 'Fractals', and a children's learning game interface with math practice options.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-0b-Hero.width-1250.png\" alt=\"A collage showing three different AI-generated user interfaces. They include a page for &quot;Tailored Fashion Advice&quot;, a science education screen featuring 'Fractals', and a children's learning game interface with math practice options.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"do5os\"><i>Generative UI is useful for a range of applications. For any user question, need, or prompt, as simple as a single word or as complex as elaborate instructions, the model creates a fully custom interface.</i> <b><i>Left:</i></b> <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=fashion-advisor\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Getting tailored fashion advice</i></a><i>.</i> <b><i>Middle:</i></b> <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=fractal-explorer\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Learning about fractals</i></a><i>.</i> <b><i>Right:</i></b> <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=basketball-math\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Teaching mathematics</i></a><i>.</i></p><p data-block-key=\"4ukk0\"><i>For more examples see the</i> <a href=\"https://generativeui.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>project page</i></a><i>.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Bringing generative UI to Google products</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"j0qlf\">Generative UI capabilities will be rolled out as two experiments in the <a href=\"https://blog.google/products/gemini/gemini-3-gemini-app\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini app</a>: dynamic view and visual layout. When using dynamic view, an experience built upon our generative UI implementation, Gemini designs and codes a fully customized interactive response for each prompt, using Gemini’s agentic coding capabilities. It customizes the experience with an understanding that explaining the microbiome to a 5 year old requires different content and a different set of features than explaining it to an adult, just as creating a gallery of social media posts for a business requires a completely different interface to generating a plan for an upcoming trip.</p><p data-block-key=\"cvpbe\">Dynamic view can be used for a wide range of scenarios, from learning about <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=rolling-an-8\" target=\"_blank\" rel=\"noopener noreferrer\">probability</a> to helping in practical tasks like <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=thanksgiving\" target=\"_blank\" rel=\"noopener noreferrer\">event planning</a> and getting <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=fashion-advisor\" target=\"_blank\" rel=\"noopener noreferrer\">fashion advice</a>. The interfaces allow users to learn, play or explore interactively. Dynamic view, along with visual layout, are rolling out today. To help us learn about these experiments, users may initially see only one of them.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Dynamic_View_Van_Gogh_1920x1080.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"vu0ul\">Example of generative UI in dynamic view based on the prompt, “Create a Van Gogh gallery with life context for each piece”.</p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"j0qlf\">Generative UI experiences are also integrated into <a href=\"http://blog.google/products/search/gemini-3-search-ai-mode\" target=\"_blank\" rel=\"noopener noreferrer\">Google Search</a> starting with AI Mode, unlocking dynamic visual experiences with interactive tools and simulations that are generated specifically for a user’s question. Now, thanks to Gemini 3’s unparalleled multimodal understanding and powerful agentic coding capabilities, Gemini 3 in AI Mode can interpret the intent behind any prompt to instantly build bespoke generative user interfaces. By generating interactive tools and simulations on the fly, it creates a dynamic environment optimized for deep comprehension and task completion. Generative UI capabilities in AI Mode are available for Google AI Pro and Ultra subscribers in the U.S. starting today. Select \"Thinking\" from the model drop-down menu in AI Mode to try it out.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/AIM-CAPYBARA-RNA-1920x1080-Under20MB.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"3jqbs\">Example of AI Mode in Google Search with the prompt, “show me how rna polymerase works. what are the stages of transcription and how is it different in prokaryotic and eukaryotic cells”.</p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How the generative UI implementation works</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"j0qlf\">Our generative UI implementation, described in the <a href=\"https://generativeui.github.io/static/pdfs/paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>, uses Google’s Gemini 3 Pro model with three important additions:</p><ol><li data-block-key=\"appno\"><i>Tool access</i>: A server provides access to several key tools, like image generation and web search. This allows the results to be made accessible to the model to increase quality or sent directly to the user’s browser to improve efficiency.</li><li data-block-key=\"br80m\"><i>Carefully crafted system instructions</i>: The system is guided by detailed instructions that include the goal, planning, examples and technical specifications, including formatting, tool manuals, and tips for avoiding common errors.</li><li data-block-key=\"cqq2h\"><i>Post-processing</i>: The model’s outputs are passed through a set of post-processors to address potential common issues.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-4-Overview.width-1250.png\" alt=\"Flowchart illustrating the process for generating a web page. The LLM takes input from the user prompt taking into account underlying system instructions, and utilizes Tools where needed. It outputs HTML/CSS/JS to the user's browser.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-4-Overview.width-1250.png\" alt=\"Flowchart illustrating the process for generating a web page. The LLM takes input from the user prompt taking into account underlying system instructions, and utilizes Tools where needed. It outputs HTML/CSS/JS to the user's browser.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"do5os\"><i>A high-level system overview of the generative UI implementation.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"j0qlf\">For some products, it might be preferable to consistently see results in specific styles. Our implementation could be configured for these products so that all results, including generated assets, are created in a consistent style for all users. Without specific styling instructions, the generative UI will select a style automatically, or the user can influence styling in their prompt, as in the case of dynamic view in the Gemini app.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-0-Hero.width-1250.png\" alt=\"Three teal-themed web design concepts displayed side-by-side: &quot;Clash of the Titans,&quot; &quot;Your Perfect Pizza Night,&quot; and &quot;Fabulous Flamingo Flair.&quot;\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-0-Hero.width-1250.png\" alt=\"Three teal-themed web design concepts displayed side-by-side: &quot;Clash of the Titans,&quot; &quot;Your Perfect Pizza Night,&quot; and &quot;Fabulous Flamingo Flair.&quot;\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"do5os\"><i>Screenshots of generative UI results with consistent “Wizard Green” styling.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Generative UI outputs are strongly preferred over standard formats</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"j0qlf\">To facilitate consistent evaluations and comparisons of generative UI implementations, we created PAGEN, a dataset of human expert–made websites and will soon be releasing it to the research community.</p><p data-block-key=\"7bgiq\">To evaluate user preferences, we compared our new generative UI experience against various different formats: a website designed for a specific prompt by human-experts, the top Google Search result for the query, and baseline LLM outputs in raw text or the standard markdown formats.</p><p data-block-key=\"64aut\">The sites designed by human experts had the highest preference rates. These were followed closely by the results from our generative UI implementation, with a substantial gap from all other output methods. This evaluation did not take into account generation speed. We also show that the performance of generative UI strongly depends on the performance of the underlying model, and that our newest models perform substantially better. See more details in the <a href=\"https://generativeui.github.io/static/pdfs/paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Opportunities ahead</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"j0qlf\">We are still in the early days of generative UI, and important opportunities for improvement remain. For example, our current implementation can sometimes take a minute or more to generate results, and there are occasional inaccuracies in the outputs; these are areas of ongoing research. Generative UI is an example of the <a href=\"https://blog.google/technology/research/google-research-team-tackles-big-challenges-with-science/\" target=\"_blank\" rel=\"noopener noreferrer\">magic cycle of research</a>, where research breakthroughs lead to product innovation that opens up new opportunities for addressing user needs and in turn fuel further research. We see potential in extending generative UI to access a wider set of services, adapt to additional context and human feedback, and deliver increasingly more helpful visual and interactive interfaces. We are excited about the further opportunities ahead for generative UI.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                ",
      "contentSnippet": "Generative UI is a powerful capability in which an AI model generates not only content but an entire user experience. Today we introduce a novel implementation of generative UI, which dynamically creates immersive visual experiences and interactive interfaces — such as web pages, games, tools, and applications — that are automatically designed and fully customized in response to any question, instruction, or prompt. These prompts can be as simple as a single word, or as long as needed for detailed instructions. These new types of interfaces are markedly different from the static, predefined interfaces in which AI models typically render content.\nIn our new paper, “Generative UI: LLMs are Effective UI Generators”, we describe the core principles that enabled our implementation of generative UI and demonstrate the effective viability of this new paradigm. Our evaluations indicate that, when ignoring generation speed, the interfaces from our generative UI implementations are strongly preferred by human raters compared to standard LLM outputs. This work represents a first step toward fully AI-generated user experiences, where users automatically get dynamic interfaces tailored to their needs, rather than having to select from an existing catalog of applications.\nOur research on generative UI, also referred to as generative interfaces, comes to life today in the Gemini app through an experiment called dynamic view and in AI Mode in Google Search.\nGenerative UI is useful for a range of applications. For any user question, need, or prompt, as simple as a single word or as complex as elaborate instructions, the model creates a fully custom interface. Left: Getting tailored fashion advice. Middle: Learning about fractals. Right: Teaching mathematics.\nFor more examples see the project page.\nBringing generative UI to Google products\nGenerative UI capabilities will be rolled out as two experiments in the Gemini app: dynamic view and visual layout. When using dynamic view, an experience built upon our generative UI implementation, Gemini designs and codes a fully customized interactive response for each prompt, using Gemini’s agentic coding capabilities. It customizes the experience with an understanding that explaining the microbiome to a 5 year old requires different content and a different set of features than explaining it to an adult, just as creating a gallery of social media posts for a business requires a completely different interface to generating a plan for an upcoming trip.\nDynamic view can be used for a wide range of scenarios, from learning about probability to helping in practical tasks like event planning and getting fashion advice. The interfaces allow users to learn, play or explore interactively. Dynamic view, along with visual layout, are rolling out today. To help us learn about these experiments, users may initially see only one of them.\nplay silent looping video\n            pause silent looping video\n          \n\n\n\n            \n\n\n\n  \n\n\n  \n\n\n          \nunmute video\n        mute video\n    \n\n            \n\n        \n        \n            \n\n            \n\n        \n    \nExample of generative UI in dynamic view based on the prompt, “Create a Van Gogh gallery with life context for each piece”.\nGenerative UI experiences are also integrated into Google Search starting with AI Mode, unlocking dynamic visual experiences with interactive tools and simulations that are generated specifically for a user’s question. Now, thanks to Gemini 3’s unparalleled multimodal understanding and powerful agentic coding capabilities, Gemini 3 in AI Mode can interpret the intent behind any prompt to instantly build bespoke generative user interfaces. By generating interactive tools and simulations on the fly, it creates a dynamic environment optimized for deep comprehension and task completion. Generative UI capabilities in AI Mode are available for Google AI Pro and Ultra subscribers in the U.S. starting today. Select \"Thinking\" from the model drop-down menu in AI Mode to try it out.\nplay silent looping video\n            pause silent looping video\n          \n\n\n\n            \n\n\n\n  \n\n\n  \n\n\n          \nunmute video\n        mute video\n    \n\n            \n\n        \n        \n            \n\n            \n\n        \n    \nExample of AI Mode in Google Search with the prompt, “show me how rna polymerase works. what are the stages of transcription and how is it different in prokaryotic and eukaryotic cells”.\nHow the generative UI implementation works\nOur generative UI implementation, described in the paper, uses Google’s Gemini 3 Pro model with three important additions:\n\nTool access: A server provides access to several key tools, like image generation and web search. This allows the results to be made accessible to the model to increase quality or sent directly to the user’s browser to improve efficiency.\nCarefully crafted system instructions: The system is guided by detailed instructions that include the goal, planning, examples and technical specifications, including formatting, tool manuals, and tips for avoiding common errors.\nPost-processing: The model’s outputs are passed through a set of post-processors to address potential common issues.\n\n\n\n    \nA high-level system overview of the generative UI implementation.\nFor some products, it might be preferable to consistently see results in specific styles. Our implementation could be configured for these products so that all results, including generated assets, are created in a consistent style for all users. Without specific styling instructions, the generative UI will select a style automatically, or the user can influence styling in their prompt, as in the case of dynamic view in the Gemini app.\nScreenshots of generative UI results with consistent “Wizard Green” styling.\nGenerative UI outputs are strongly preferred over standard formats\nTo facilitate consistent evaluations and comparisons of generative UI implementations, we created PAGEN, a dataset of human expert–made websites and will soon be releasing it to the research community.\nTo evaluate user preferences, we compared our new generative UI experience against various different formats: a website designed for a specific prompt by human-experts, the top Google Search result for the query, and baseline LLM outputs in raw text or the standard markdown formats.\nThe sites designed by human experts had the highest preference rates. These were followed closely by the results from our generative UI implementation, with a substantial gap from all other output methods. This evaluation did not take into account generation speed. We also show that the performance of generative UI strongly depends on the performance of the underlying model, and that our newest models perform substantially better. See more details in the paper.\nOpportunities ahead\nWe are still in the early days of generative UI, and important opportunities for improvement remain. For example, our current implementation can sometimes take a minute or more to generate results, and there are occasional inaccuracies in the outputs; these are areas of ongoing research. Generative UI is an example of the magic cycle of research, where research breakthroughs lead to product innovation that opens up new opportunities for addressing user needs and in turn fuel further research. We see potential in extending generative UI to access a wider set of services, adapt to additional context and human feedback, and deliver increasingly more helpful visual and interactive interfaces. We are excited about the further opportunities ahead for generative UI.",
      "creator": "Google",
      "summary": "\"Generative UI\"是一种AI技术，能根据用户提问或指令动态生成定制化的交互式界面，如网页、游戏等，提供沉浸式体验。新界面与静态预设界面不同，能根据需求自动设计，用户无需从现有选项中选择。该技术已在Google Gemini应用和搜索中的AI模式中实现，通过动态视图和视觉布局实验，为用户生成符合需求的交互工具和模拟环境，提升理解和任务完成效率。"
    },
    {
      "title": "Separating natural forests from other tree cover with AI for deforestation-free supply chains",
      "link": "https://research.google/blog/separating-natural-forests-from-other-tree-cover-with-ai-for-deforestation-free-supply-chains/",
      "pubDate": "Wed, 12 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-12T16:00:00.000Z",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"1h9b3\">Forests are vital for our planet as they regulate rainfall, mitigate floods, store and sequester carbon, and help sustain the majority of the <a href=\"https://zenodo.org/records/6417333\" target=\"_blank\" rel=\"noopener noreferrer\">planet’s land-based species</a>. Despite their importance, deforestation continues at an alarming rate. A key challenge in conservation efforts is differentiating centuries-old natural ecosystems from newly planted forests or tree crop plantations with satellite data. Most existing maps simply show \"tree cover,\" a basic measure of any woody vegetation, leading to an \"apples-to-oranges\" comparison. This conflates the harvesting of a short-term plantation with the permanent loss of an irreplaceable, biodiversity-rich natural forest.</p><p data-block-key=\"dg5d6\">The need for this distinction is more important than ever due to new global regulations, like the <a href=\"https://environment.ec.europa.eu/topics/forests/deforestation/regulation-deforestation-free-products_en\" target=\"_blank\" rel=\"noopener noreferrer\">European Union Regulation on Deforestation-free Products</a> (EUDR). This regulation mandates that products like coffee, cocoa, rubber, timber, and palm oil sold in the EU cannot come from land that was deforested or degraded after December 31, 2020, with the goal of protecting natural forests, like primary and naturally regenerating forests. This policy creates a need for a reliable, high-resolution, and globally-consistent map of natural forests as they existed in 2020. The protection of these forests is also a central pillar for <a href=\"https://unfccc.int/cop30\" target=\"_blank\" rel=\"noopener noreferrer\">COP30</a>, which recognizes their crucial role in climate stability and human well-being.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-1.width-1250.png\" alt=\"Natural Forests-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-1.width-1250.png\" alt=\"Natural Forests-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"2ra3h\"><i>Gemini generated image showing natural forest (</i><b><i>left</i></b><i>) bordering a planted forest (</i><b><i>right</i></b><i>). Global satellite-based models struggle to distinguish between them, complicating efforts to protect the more biodiversity-rich natural forest.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"1h9b3\">In an effort to help meet this need, together with <a href=\"https://deepmind.google/\" target=\"_blank\" rel=\"noopener noreferrer\">Google DeepMind</a>, we’re releasing <a href=\"https://plus.figshare.com/articles/dataset/Natural_forests_of_the_world_2020_-_probability_maps/28881731\" target=\"_blank\" rel=\"noopener noreferrer\">Natural Forests of the World 2020</a>, a new map and dataset, published in <a href=\"https://www.doi.org/10.1038/s41597-025-06097-z\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature Scientific Data</i></a>. This project stems from a collaboration with the <a href=\"https://www.wri.org/\" target=\"_blank\" rel=\"noopener noreferrer\">World Resources Institute</a> and the <a href=\"https://iiasa.ac.at/\" target=\"_blank\" rel=\"noopener noreferrer\">International Institute for Applied Systems Analysis</a>, and provides a critical baseline for deforestation and degradation monitoring. We provide the first globally consistent, <a href=\"https://developers.google.com/earth-engine/datasets/catalog/projects_nature-trace_assets_forest_typology_natural_forest_2020_v1_0_collection\" target=\"_blank\" rel=\"noopener noreferrer\">10-meter resolution map</a> that differentiates natural forests from other tree cover and achieves a best-in-class accuracy of 92.2% when validated against a <a href=\"https://figshare.com/articles/dataset/Natural_forests_of_the_world_2020_-_validation_dataset/30051517?file=57664081\" target=\"_blank\" rel=\"noopener noreferrer\">global independent dataset</a>. We hope that this publicly available baseline can help companies conduct due diligence, support governments in monitoring deforestation, and empower conservation groups to target their efforts to protect what matters most.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-2.width-1250.png\" alt=\"Natural Forests-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-2.width-1250.png\" alt=\"Natural Forests-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"2ra3h\"><i>The global extent of natural forests in 2020 (originally at 10-meter resolution).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How AI can separate the forest from the trees</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1h9b3\">Distinguishing a natural forest from a complex agroforestry system or a 50-year-old planted forest is difficult using a single satellite image. To overcome this, we developed an AI model that acts like a forester, observing a patch of land over the course of a year, segmenting a 1280 x 1280 meter patch and estimating the likelihood that each 10 x 10 meter pixel within it is a natural forest. This allows the model to make assessments based on the surrounding context, rather than a single snapshot. This novel <i>multi-modal temporal-spatial</i> <a href=\"https://en.wikipedia.org/wiki/Vision_transformer\" target=\"_blank\" rel=\"noopener noreferrer\"><i>vision transformer</i></a> (MTSViT) model analyzes seasonal <a href=\"https://sentinels.copernicus.eu/web/sentinel/copernicus/sentinel-2\" target=\"_blank\" rel=\"noopener noreferrer\">Sentinel-2</a> satellite imagery and topographical data (e.g., elevation and slope), along with the sample’s geographical coordinate. By observing satellite imagery over time, the model identifies distinct spectral, temporal, and texture signatures (i.e., data patterns used to recognize different forest types) that differentiate complex, natural forests from uniform, fast-growing commercial plantations and other land use and land cover.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/natural_forests_video_im_560p_lq.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"z7b76\">To build the Natural Forests of the World 2020 map, we sampled over 1.2 million global 1280 x 1280 meter patch locations at 10-meter resolution to create a massive, multi-source training dataset. We used this data to train the MTSViT model to recognize complex patterns of natural forests and other land types. We then applied the trained MTSViT model across all land on Earth, generating a seamless, globally consistent 10-meter probability map. To rigorously validate the map, we created an <a href=\"https://figshare.com/articles/dataset/Natural_forests_of_the_world_2020_-_validation_dataset/30051517?file=57664081\" target=\"_blank\" rel=\"noopener noreferrer\">evaluation dataset</a> by repurposing an <a href=\"https://www.nature.com/articles/s41597-022-01332-3\" target=\"_blank\" rel=\"noopener noreferrer\">independent dataset</a> focused on global forest management for 2015 and updating its labels to focus on natural forests for 2020. See more details in the <a href=\"https://doi.org/10.1038/s41597-025-06097-z\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-3.width-1250.png\" alt=\"Natural Forests-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-3.width-1250.png\" alt=\"Natural Forests-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"2ra3h\"><i>End-to-end workflow of the Natural Forests map generation (annotating data generation, processing, model training, map generation, and validation steps).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What's next: A new vision for forest understanding</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1h9b3\">We hope that the Natural Forests of the World 2020 baseline proves to be a valuable resource for policymakers, auditors, and companies seeking to comply with new deforestation-free regulations such as the EUDR. But forests are not static. To truly support global conservation and sustainability, we need to distinguish between more classes of forest and, crucially, understand how they change over time. This involves differentiating between and locating key forest types: natural forests (carbon-dense and biodiversity-rich forests), planted forests, plantations, and commercial tree crops (such as ecosystem-friendly coffee and cocoa agroforestry systems).</p><p data-block-key=\"e8q12\">To advance this effort, we’re developing a new multi-year series of global forest type maps, powered by next-generation AI models. These maps will categorize the world's land into six distinct types: Primary Forest, Naturally Regenerating Forest, Planted Forest, Plantation Forest, Tree Crops, and Other Land Cover. We expect to release these comprehensive maps in 2026.</p><p data-block-key=\"2b9sn\">To encourage the broader research community to contribute to this effort, we have also released two large-scale benchmark datasets. These datasets are important for developing and rigorously testing the next generation of AI models designed to analyze the world’s forests. The <a href=\"https://arxiv.org/abs/2406.18554\" target=\"_blank\" rel=\"noopener noreferrer\">Planted dataset</a> is a global, multi-sensor long-temporal collection featuring over 2.3 million time-series classification examples. It is specifically designed to help AI models recognize 64 different (species or genera) types of planted forests and tree crops worldwide. The <a href=\"https://arxiv.org/abs/2505.01805\" target=\"_blank\" rel=\"noopener noreferrer\">Forest Typology</a> (ForTy) benchmark provides a truly global-scale dataset with 200,000 multi-source and multi-temporal image patches with per-pixel labels for semantic segmentation models. This resource is tailored for the core task of mapping the key classes: natural forest, planted forest, and tree crops.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Helping to protect our planet</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1h9b3\">Turning climate and nature ambitions into action requires transparent, trusted, and high-resolution data. We are committed to making these tools as accessible as possible. We hope these new datasets and tools will help governments, companies, and communities work together to meet their deforestation-free goals and protect the critical ecosystems on which we all depend.</p><p data-block-key=\"159ar\">Learn more about our AI and sustainability efforts by checking out <a href=\"https://blog.google/technology/research/new-updates-and-more-access-to-google-earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth AI</a>, <a href=\"https://cloud.google.com/blog/topics/sustainability/look-back-at-a-year-of-earth-engine-advancements\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth Engine</a>, and <a href=\"https://deepmind.google/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/\" target=\"_blank\" rel=\"noopener noreferrer\">AlphaEarth Foundations</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1h9b3\"><i>This research was co-developed by Google Deepmind and Google Research in collaboration with WRI and IIASA.</i></p><p data-block-key=\"1fca\"><i>We thank our collaborators at Google, World Resources Institute (WRI) / Global Forest Watch (GFW), and International Institute for Applied Systems Analysis (IIASA): Anton Raichuk, Charlotte Stanton, Dan Morris, Drew Purves, Elizabeth Goldman, Katelyn Tarrio, Keith Anderson, Maxim Neumann, Mélanie Rey, Michelle J. Sims, Myroslava Lesiv, Nicholas Clinton, Petra Poklukar, Radost Stanimirova, Sarah Carter, Steffen Fritz, Yuchang Jiang.</i></p><p data-block-key=\"ftoen\"><i>Special thanks to early map reviewers: Andrew Lister (United States Forest Service), Astrid Verheggen (Joint Research Centre), Clement Bourgoin (Joint Research Centre), Erin Glen (WRI), Frederic Achard (Joint Research Centre), Jonas Fridman (Swedish University of Agricultural Sciences), Jukka Meiteninen (VTT), Karen Saunders (World Wildlife Fund Canada), Louis Reymondin (Alliance Bioversity International - CIAT), Martin Herold (GFZ Helmholtz Centre for Geosciences), Olga Nepomshina (GFZ Helmholtz Centre for Geosciences), Peter Potapov (University of Maryland/WRI), Rene Colditz (Joint Research Centre), Thibaud Vantalon (Alliance Bioversity International - CIAT), and Viviana Zalles (WRI).</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                ",
      "contentSnippet": "Forests are vital for our planet as they regulate rainfall, mitigate floods, store and sequester carbon, and help sustain the majority of the planet’s land-based species. Despite their importance, deforestation continues at an alarming rate. A key challenge in conservation efforts is differentiating centuries-old natural ecosystems from newly planted forests or tree crop plantations with satellite data. Most existing maps simply show \"tree cover,\" a basic measure of any woody vegetation, leading to an \"apples-to-oranges\" comparison. This conflates the harvesting of a short-term plantation with the permanent loss of an irreplaceable, biodiversity-rich natural forest.\nThe need for this distinction is more important than ever due to new global regulations, like the European Union Regulation on Deforestation-free Products (EUDR). This regulation mandates that products like coffee, cocoa, rubber, timber, and palm oil sold in the EU cannot come from land that was deforested or degraded after December 31, 2020, with the goal of protecting natural forests, like primary and naturally regenerating forests. This policy creates a need for a reliable, high-resolution, and globally-consistent map of natural forests as they existed in 2020. The protection of these forests is also a central pillar for COP30, which recognizes their crucial role in climate stability and human well-being.\nGemini generated image showing natural forest (left) bordering a planted forest (right). Global satellite-based models struggle to distinguish between them, complicating efforts to protect the more biodiversity-rich natural forest.\nIn an effort to help meet this need, together with Google DeepMind, we’re releasing Natural Forests of the World 2020, a new map and dataset, published in Nature Scientific Data. This project stems from a collaboration with the World Resources Institute and the International Institute for Applied Systems Analysis, and provides a critical baseline for deforestation and degradation monitoring. We provide the first globally consistent, 10-meter resolution map that differentiates natural forests from other tree cover and achieves a best-in-class accuracy of 92.2% when validated against a global independent dataset. We hope that this publicly available baseline can help companies conduct due diligence, support governments in monitoring deforestation, and empower conservation groups to target their efforts to protect what matters most.\nThe global extent of natural forests in 2020 (originally at 10-meter resolution).\nHow AI can separate the forest from the trees\nDistinguishing a natural forest from a complex agroforestry system or a 50-year-old planted forest is difficult using a single satellite image. To overcome this, we developed an AI model that acts like a forester, observing a patch of land over the course of a year, segmenting a 1280 x 1280 meter patch and estimating the likelihood that each 10 x 10 meter pixel within it is a natural forest. This allows the model to make assessments based on the surrounding context, rather than a single snapshot. This novel multi-modal temporal-spatial vision transformer (MTSViT) model analyzes seasonal Sentinel-2 satellite imagery and topographical data (e.g., elevation and slope), along with the sample’s geographical coordinate. By observing satellite imagery over time, the model identifies distinct spectral, temporal, and texture signatures (i.e., data patterns used to recognize different forest types) that differentiate complex, natural forests from uniform, fast-growing commercial plantations and other land use and land cover.\nplay silent looping video\n            pause silent looping video\n          \n\n\n\n            \n\n\n\n  \n\n\n  \n\n\n          \nunmute video\n        mute video\n    \n\n            \n\n        \n        \n            \n\n            \n\n        \n    \nTo build the Natural Forests of the World 2020 map, we sampled over 1.2 million global 1280 x 1280 meter patch locations at 10-meter resolution to create a massive, multi-source training dataset. We used this data to train the MTSViT model to recognize complex patterns of natural forests and other land types. We then applied the trained MTSViT model across all land on Earth, generating a seamless, globally consistent 10-meter probability map. To rigorously validate the map, we created an evaluation dataset by repurposing an independent dataset focused on global forest management for 2015 and updating its labels to focus on natural forests for 2020. See more details in the paper.\nEnd-to-end workflow of the Natural Forests map generation (annotating data generation, processing, model training, map generation, and validation steps).\nWhat's next: A new vision for forest understanding\nWe hope that the Natural Forests of the World 2020 baseline proves to be a valuable resource for policymakers, auditors, and companies seeking to comply with new deforestation-free regulations such as the EUDR. But forests are not static. To truly support global conservation and sustainability, we need to distinguish between more classes of forest and, crucially, understand how they change over time. This involves differentiating between and locating key forest types: natural forests (carbon-dense and biodiversity-rich forests), planted forests, plantations, and commercial tree crops (such as ecosystem-friendly coffee and cocoa agroforestry systems).\nTo advance this effort, we’re developing a new multi-year series of global forest type maps, powered by next-generation AI models. These maps will categorize the world's land into six distinct types: Primary Forest, Naturally Regenerating Forest, Planted Forest, Plantation Forest, Tree Crops, and Other Land Cover. We expect to release these comprehensive maps in 2026.\nTo encourage the broader research community to contribute to this effort, we have also released two large-scale benchmark datasets. These datasets are important for developing and rigorously testing the next generation of AI models designed to analyze the world’s forests. The Planted dataset is a global, multi-sensor long-temporal collection featuring over 2.3 million time-series classification examples. It is specifically designed to help AI models recognize 64 different (species or genera) types of planted forests and tree crops worldwide. The Forest Typology (ForTy) benchmark provides a truly global-scale dataset with 200,000 multi-source and multi-temporal image patches with per-pixel labels for semantic segmentation models. This resource is tailored for the core task of mapping the key classes: natural forest, planted forest, and tree crops.\nHelping to protect our planet\nTurning climate and nature ambitions into action requires transparent, trusted, and high-resolution data. We are committed to making these tools as accessible as possible. We hope these new datasets and tools will help governments, companies, and communities work together to meet their deforestation-free goals and protect the critical ecosystems on which we all depend.\nLearn more about our AI and sustainability efforts by checking out Google Earth AI, Google Earth Engine, and AlphaEarth Foundations.\nAcknowledgments\nThis research was co-developed by Google Deepmind and Google Research in collaboration with WRI and IIASA.\nWe thank our collaborators at Google, World Resources Institute (WRI) / Global Forest Watch (GFW), and International Institute for Applied Systems Analysis (IIASA): Anton Raichuk, Charlotte Stanton, Dan Morris, Drew Purves, Elizabeth Goldman, Katelyn Tarrio, Keith Anderson, Maxim Neumann, Mélanie Rey, Michelle J. Sims, Myroslava Lesiv, Nicholas Clinton, Petra Poklukar, Radost Stanimirova, Sarah Carter, Steffen Fritz, Yuchang Jiang.\nSpecial thanks to early map reviewers: Andrew Lister (United States Forest Service), Astrid Verheggen (Joint Research Centre), Clement Bourgoin (Joint Research Centre), Erin Glen (WRI), Frederic Achard (Joint Research Centre), Jonas Fridman (Swedish University of Agricultural Sciences), Jukka Meiteninen (VTT), Karen Saunders (World Wildlife Fund Canada), Louis Reymondin (Alliance Bioversity International - CIAT), Martin Herold (GFZ Helmholtz Centre for Geosciences), Olga Nepomshina (GFZ Helmholtz Centre for Geosciences), Peter Potapov (University of Maryland/WRI), Rene Colditz (Joint Research Centre), Thibaud Vantalon (Alliance Bioversity International - CIAT), and Viviana Zalles (WRI).",
      "creator": "Google",
      "summary": "为应对森林砍伐问题，研究人员利用AI技术区分自然森林与其他树冠，以支持无森林砍伐供应链。现有地图通常仅显示“树冠”，难以区分自然森林与人工林，导致政策执行困难。新发布的《2020年世界自然森林地图》采用AI模型分析卫星影像和地形数据，实现92.2%的准确率，为企业和政府提供可靠数据，支持自然森林保护。"
    },
    {
      "title": "A new quantum toolkit for optimization",
      "link": "https://research.google/blog/a-new-quantum-toolkit-for-optimization/",
      "pubDate": "Wed, 12 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-12T16:00:00.000Z",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"16fxo\">From designing more efficient airline routes to organizing clinical trials, optimization problems are everywhere. Yet for many real-world challenges, even our most powerful supercomputers struggle to find the best solution. This has led to a major, decades-long question in quantum computing: could quantum machines succeed on optimization problems where classical ones fall short? This has proven to be a very difficult mathematical question, which remains largely open. As the capabilities of quantum hardware <a href=\"https://research.google/blog/making-quantum-error-correction-work/\">undergo</a> <a href=\"https://research.google/blog/a-colorful-quantum-future/\">rapid</a> <a href=\"https://research.google/blog/a-new-hybrid-platform-for-quantum-simulation-of-magnetism/\">advancement</a>, such theoretical problems of working out the eventual commercial and scientific use cases of large-scale error-corrected quantum computers become only more urgent.</p><p data-block-key=\"5a78j\">In <a href=\"https://www.nature.com/articles/s41586-025-09527-5\" target=\"_blank\" rel=\"noopener noreferrer\">a recent Nature paper</a>, researchers from Google Quantum AI and collaborators from Stanford, MIT, and Caltech shed new light on this question. We introduce an efficient quantum algorithm — called Decoded Quantum Interferometry (DQI) — that uses the wavelike nature of quantum mechanics to create interference patterns that converge on near-optimal solutions that are incredibly difficult to find using classical computers.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"16fxo\">A quantum link between optimization and decoding</h2><p data-block-key=\"eflao\">There is a catch, however. To build the necessary interference patterns, one must solve another hard computational problem called decoding. In a decoding problem one is given a lattice and a point in space, and one needs to find the nearest lattice element to the point. For example, the corners of the squares on a chessboard form a two dimensional lattice. After dropping a grain of sand at a random location on a chessboard, the decoding problem would be to find the nearest corner. Although this problem is easy for a square lattice in two dimensions, it can become very difficult on some lattices in hundreds or thousands of dimensions.</p><p data-block-key=\"bhrt5\">Fortunately, decoding problems have been extremely well studied over the past several decades, mainly due to applications in correcting errors incurred during data storage or transmission. Many sophisticated and powerful algorithms have been devised to solve decoding problems for various specially structured lattices. We have discovered that for certain kinds of optimization problems, the related decoding problems have the right kind of structure to be solved by some of these powerful decoding algorithms. However, it is only through the power of quantum computing that these decoding algorithms can be leveraged to also solve optimization problems. By pairing the quantum interference of DQI with these sophisticated decoding algorithms, a sufficiently large quantum computer could find approximate solutions to these optimization problems — solutions that appear to be beyond the reach of any known classical method.</p><p data-block-key=\"c6d4f\">This mathematical discovery of a quantum algorithm that offers speedup for optimization improves our understanding of the eventual use cases for quantum computers. When quantum computing hardware is advanced enough, researchers can use the DQI algorithm to solve classically challenging optimization problems.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DQI1_Animation.width-800.gif\" alt=\"DQI1_Animation\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DQI1_Animation.width-800.gif\" alt=\"DQI1_Animation\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"bj89f\"><i>A figurative representation of the conversion of an optimization problem on a rugged cost function landscape into a decoding problem for a periodic lattice.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"16fxo\">A clear quantum win: Optimal polynomial intersection</h2><p data-block-key=\"bh66r\">In this work, our best result is for a problem that we call <a href=\"https://arxiv.org/abs/2408.08292\" target=\"_blank\" rel=\"noopener noreferrer\">optimal polynomial intersection</a> (OPI). In the OPI problem, one is given a list of target points and wishes to intersect as many as possible by tuning the coefficients of a polynomial whose degree is lower than the number of points. This is a common task in data science known as <a href=\"https://en.wikipedia.org/wiki/Polynomial_regression\" target=\"_blank\" rel=\"noopener noreferrer\">polynomial regression</a>. Variants of this problem have arisen in the context of <a href=\"https://www.youtube.com/watch?v=X8jsijhllIA\" target=\"_blank\" rel=\"noopener noreferrer\">digital error correction</a> as well as <a href=\"https://www.wisdom.weizmann.ac.il/~naor/PAPERS/ope.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">cryptography</a>. Consequently, sophisticated algorithms have been developed for solving it in certain special cases, but for other cases, the problem remains hopelessly difficult to solve using known algorithms with conventional classical computers.</p><p data-block-key=\"36hdr\">Using DQI, a quantum computer could convert this into a problem of decoding <a href=\"https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction\" target=\"_blank\" rel=\"noopener noreferrer\">Reed-Solomon codes</a> (a widely-used family of codes found in DVDs and QR codes). Very good decoding algorithms have been developed for decoding Reed-Solomon codes, and as a result, quantum computers using DQI can find better approximate optima to the OPI problem than can be found by known algorithms on classical computers. For example, <a href=\"https://arxiv.org/abs/2510.10967\" target=\"_blank\" rel=\"noopener noreferrer\">our analysis</a> shows that certain examples of the OPI problem could be solved by quantum computers using only on the order of a few million elementary quantum logic operations which would require over 10<sup class=\"superscript\">23</sup> (one hundred sextillion) elementary operations to solve on a conventional classical computer using the most efficient known classical algorithm.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DQI2_OPI.width-1250.png\" alt=\"DQI2_OPI\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DQI2_OPI.width-1250.png\" alt=\"DQI2_OPI\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"bj89f\"><i>An illustration of the OPI problem: One wishes to find a low degree polynomial intersecting as many as possible of the target sets. The polynomials</i> f<i> and</i> g<i> displayed here, despite being very different, each intersect three of the target sets, thus scoring the same value of the objective. This phenomenon of distant solutions achieving equal objective values is one reason why it is so hard to solve using conventional computers.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"16fxo\">Where does the quantum advantage come from?</h2><p data-block-key=\"38qmv\">Taking a step back, we can ask why converting optimization problems into decoding problems should ever be advantageous in the first place? By understanding this more deeply, one could hope to gain intuition to guide the search for additional optimization problems on which quantum computers may provide advantage.</p><p data-block-key=\"64okm\">Both the optimization problems that we start with and the decoding problems that we convert them into are something called <a href=\"https://jeffe.cs.illinois.edu/teaching/algorithms/book/12-nphard.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">NP-hard problems</a>. This suggests that it is impossible to efficiently find exact solutions to all instances of these problems, even with the help of quantum computers. By using quantum effects, DQI has converted one hard problem into another hard problem. How does this accomplish anything? The key is that the NP-hardness speaks to the difficulty of the <i>very hardest instances</i> of a given problem. If the problem instances are restricted to have some additional structure, <i>this can make them easier</i>. The promise of DQI is that certain kinds of structure may make the decoding problem much easier, without also making the optimization problem easier to solve using conventional computers.</p><p data-block-key=\"43cq0\">In the OPI problem, the lattice that arises is algebraically structured; the components of the basis vectors, instead of being arbitrary, are obtained by raising a number to successively higher powers. This algebraic structure is reflected in both the original optimization problem (OPI) and the decoding problem that quantum computers can convert it into (Reed-Solomon decoding). This structure makes the decoding problem much easier, but as far as we can tell does not make the optimization problem easier for conventional computers. In this circumstance, the ability to convert the optimization problem into the decoding problem, using the power of quantum computing, provides advantage.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"16fxo\">Random sparse optimization problems: A tantalizing challenge</h2><p data-block-key=\"4oejb\">In the paper, we also consider more generic lattices that lack algebraic structure but whose basis vectors are sparse, i.e., consisting mostly of zeros. The corresponding optimization problem is called <code>max-k-XORSAT</code> and is illustrated below. The sparsity of the lattice is reflected in the fact that each of the constraints involves only a few of the variables (at most <i>k</i>). In <code>max-k-XORSAT</code> there are more constraints than variables and it is impossible to satisfy all of them. Instead, one wishes to find a solution that satisfies as many constraints as possible. Though it may sound abstract, the <code>max-k-XORSAT</code> problem is commonly used as a testbed for new optimization algorithms and includes a number of other well-known optimization problems as special cases, such as <a href=\"https://en.wikipedia.org/wiki/Maximum_cut\" target=\"_blank\" rel=\"noopener noreferrer\">max-cut</a> and <a href=\"https://en.wikipedia.org/wiki/Qubo\" target=\"_blank\" rel=\"noopener noreferrer\">QUBO</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DQI3_Example.width-1250.png\" alt=\"DQI3_Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DQI3_Example.width-1250.png\" alt=\"DQI3_Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"bj89f\"><i>An example of a</i> <code><i>max-k-XORSAT</i></code><i> problem with</i> k<i> = 2, which has 4 variables and 5 constraints. By assigning each of A, B, C, D to 0 or 1, how many constraints can you satisfy?</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"16fxo\">DQI can convert <code>max-k-XORSAT</code> into a decoding problem for codes defined by sparse matrices. Such codes are called <a href=\"https://en.wikipedia.org/wiki/Low-density_parity-check_code\" target=\"_blank\" rel=\"noopener noreferrer\">low density parity check (LDPC) codes</a>. It was discovered in the 1960s that sparsity makes the decoding problem much easier. However, the sparsity of the original <code>max-k-XORSAT</code> problem also makes it easier to solve on conventional computers using an algorithm called <a href=\"https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=150774\" target=\"_blank\" rel=\"noopener noreferrer\">simulated annealing</a>. Thus, it is hard to find <code>max-k-XORSAT</code> problems that have just the right sparsity so that the decoder is helped more than the simulated annealing algorithm that we are comparing against. In the paper we present one example problem where the sparsity is just right so that DQI appears to have a speed advantage over simulated annealing. However, we managed to solve this efficiently on a conventional computer using a specialized algorithm that we tailored to our example. So, at present, unlike for OPI, we do not have an example of a <code>max-k-XORSAT</code> problem that both can be solved by DQI and cannot be solved efficiently by any known algorithm running on conventional computers.</p><p data-block-key=\"f6rea\">Since sparse optimization problems have widespread practical applications, we continue to search for ways that DQI might achieve quantum advantage on sparse optimization problems. In particular, DQI has motivated new lines of research into both classical and quantum algorithms for decoding LDPC codes.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"16fxo\">Prospects</h2><p data-block-key=\"dtn01\">The DQI algorithm provides a powerful new toolkit for developing quantum optimization algorithms. This approach of translating optimization problems into decoding problems offers a new way to address one of the longest-standing questions in the field. We are excited to see what researchers, both at Google and in the broader community, will build with these tools.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                ",
      "contentSnippet": "From designing more efficient airline routes to organizing clinical trials, optimization problems are everywhere. Yet for many real-world challenges, even our most powerful supercomputers struggle to find the best solution. This has led to a major, decades-long question in quantum computing: could quantum machines succeed on optimization problems where classical ones fall short? This has proven to be a very difficult mathematical question, which remains largely open. As the capabilities of quantum hardware undergo rapid advancement, such theoretical problems of working out the eventual commercial and scientific use cases of large-scale error-corrected quantum computers become only more urgent.\nIn a recent Nature paper, researchers from Google Quantum AI and collaborators from Stanford, MIT, and Caltech shed new light on this question. We introduce an efficient quantum algorithm — called Decoded Quantum Interferometry (DQI) — that uses the wavelike nature of quantum mechanics to create interference patterns that converge on near-optimal solutions that are incredibly difficult to find using classical computers.\nA quantum link between optimization and decoding\nThere is a catch, however. To build the necessary interference patterns, one must solve another hard computational problem called decoding. In a decoding problem one is given a lattice and a point in space, and one needs to find the nearest lattice element to the point. For example, the corners of the squares on a chessboard form a two dimensional lattice. After dropping a grain of sand at a random location on a chessboard, the decoding problem would be to find the nearest corner. Although this problem is easy for a square lattice in two dimensions, it can become very difficult on some lattices in hundreds or thousands of dimensions.\nFortunately, decoding problems have been extremely well studied over the past several decades, mainly due to applications in correcting errors incurred during data storage or transmission. Many sophisticated and powerful algorithms have been devised to solve decoding problems for various specially structured lattices. We have discovered that for certain kinds of optimization problems, the related decoding problems have the right kind of structure to be solved by some of these powerful decoding algorithms. However, it is only through the power of quantum computing that these decoding algorithms can be leveraged to also solve optimization problems. By pairing the quantum interference of DQI with these sophisticated decoding algorithms, a sufficiently large quantum computer could find approximate solutions to these optimization problems — solutions that appear to be beyond the reach of any known classical method.\nThis mathematical discovery of a quantum algorithm that offers speedup for optimization improves our understanding of the eventual use cases for quantum computers. When quantum computing hardware is advanced enough, researchers can use the DQI algorithm to solve classically challenging optimization problems.\nA figurative representation of the conversion of an optimization problem on a rugged cost function landscape into a decoding problem for a periodic lattice.\nA clear quantum win: Optimal polynomial intersection\nIn this work, our best result is for a problem that we call optimal polynomial intersection (OPI). In the OPI problem, one is given a list of target points and wishes to intersect as many as possible by tuning the coefficients of a polynomial whose degree is lower than the number of points. This is a common task in data science known as polynomial regression. Variants of this problem have arisen in the context of digital error correction as well as cryptography. Consequently, sophisticated algorithms have been developed for solving it in certain special cases, but for other cases, the problem remains hopelessly difficult to solve using known algorithms with conventional classical computers.\nUsing DQI, a quantum computer could convert this into a problem of decoding Reed-Solomon codes (a widely-used family of codes found in DVDs and QR codes). Very good decoding algorithms have been developed for decoding Reed-Solomon codes, and as a result, quantum computers using DQI can find better approximate optima to the OPI problem than can be found by known algorithms on classical computers. For example, our analysis shows that certain examples of the OPI problem could be solved by quantum computers using only on the order of a few million elementary quantum logic operations which would require over 1023 (one hundred sextillion) elementary operations to solve on a conventional classical computer using the most efficient known classical algorithm.\nAn illustration of the OPI problem: One wishes to find a low degree polynomial intersecting as many as possible of the target sets. The polynomials f and g displayed here, despite being very different, each intersect three of the target sets, thus scoring the same value of the objective. This phenomenon of distant solutions achieving equal objective values is one reason why it is so hard to solve using conventional computers.\nWhere does the quantum advantage come from?\nTaking a step back, we can ask why converting optimization problems into decoding problems should ever be advantageous in the first place? By understanding this more deeply, one could hope to gain intuition to guide the search for additional optimization problems on which quantum computers may provide advantage.\nBoth the optimization problems that we start with and the decoding problems that we convert them into are something called NP-hard problems. This suggests that it is impossible to efficiently find exact solutions to all instances of these problems, even with the help of quantum computers. By using quantum effects, DQI has converted one hard problem into another hard problem. How does this accomplish anything? The key is that the NP-hardness speaks to the difficulty of the very hardest instances of a given problem. If the problem instances are restricted to have some additional structure, this can make them easier. The promise of DQI is that certain kinds of structure may make the decoding problem much easier, without also making the optimization problem easier to solve using conventional computers.\nIn the OPI problem, the lattice that arises is algebraically structured; the components of the basis vectors, instead of being arbitrary, are obtained by raising a number to successively higher powers. This algebraic structure is reflected in both the original optimization problem (OPI) and the decoding problem that quantum computers can convert it into (Reed-Solomon decoding). This structure makes the decoding problem much easier, but as far as we can tell does not make the optimization problem easier for conventional computers. In this circumstance, the ability to convert the optimization problem into the decoding problem, using the power of quantum computing, provides advantage.\nRandom sparse optimization problems: A tantalizing challenge\nIn the paper, we also consider more generic lattices that lack algebraic structure but whose basis vectors are sparse, i.e., consisting mostly of zeros. The corresponding optimization problem is called max-k-XORSAT and is illustrated below. The sparsity of the lattice is reflected in the fact that each of the constraints involves only a few of the variables (at most k). In max-k-XORSAT there are more constraints than variables and it is impossible to satisfy all of them. Instead, one wishes to find a solution that satisfies as many constraints as possible. Though it may sound abstract, the max-k-XORSAT problem is commonly used as a testbed for new optimization algorithms and includes a number of other well-known optimization problems as special cases, such as max-cut and QUBO.\nAn example of a max-k-XORSAT problem with k = 2, which has 4 variables and 5 constraints. By assigning each of A, B, C, D to 0 or 1, how many constraints can you satisfy?\nDQI can convert max-k-XORSAT into a decoding problem for codes defined by sparse matrices. Such codes are called low density parity check (LDPC) codes. It was discovered in the 1960s that sparsity makes the decoding problem much easier. However, the sparsity of the original max-k-XORSAT problem also makes it easier to solve on conventional computers using an algorithm called simulated annealing. Thus, it is hard to find max-k-XORSAT problems that have just the right sparsity so that the decoder is helped more than the simulated annealing algorithm that we are comparing against. In the paper we present one example problem where the sparsity is just right so that DQI appears to have a speed advantage over simulated annealing. However, we managed to solve this efficiently on a conventional computer using a specialized algorithm that we tailored to our example. So, at present, unlike for OPI, we do not have an example of a max-k-XORSAT problem that both can be solved by DQI and cannot be solved efficiently by any known algorithm running on conventional computers.\nSince sparse optimization problems have widespread practical applications, we continue to search for ways that DQI might achieve quantum advantage on sparse optimization problems. In particular, DQI has motivated new lines of research into both classical and quantum algorithms for decoding LDPC codes.\nProspects\nThe DQI algorithm provides a powerful new toolkit for developing quantum optimization algorithms. This approach of translating optimization problems into decoding problems offers a new way to address one of the longest-standing questions in the field. We are excited to see what researchers, both at Google and in the broader community, will build with these tools.",
      "creator": "Google",
      "summary": "文章介绍了一种名为“解码量子干涉”（DQI）的新型量子优化算法。该算法利用量子力学的波粒二象性，通过干涉模式在经典计算机难以解决的复杂优化问题中寻找近似最优解。DQI与解码算法结合，可应用于多项式回归等数据科学问题，展现出超越经典方法的潜力。这一发现有助于理解量子计算机在优化领域的应用前景。"
    },
    {
      "title": "Differentially private machine learning at scale with JAX-Privacy",
      "link": "https://research.google/blog/differentially-private-machine-learning-at-scale-with-jax-privacy/",
      "pubDate": "Tue, 11 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-11T16:00:00.000Z",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xtj9j\">From personalized recommendations to scientific advances, AI models are helping to improve lives and transform industries. But the impact and accuracy of these AI models is often determined by the quality of data they use. Large, high-quality datasets are crucial for developing accurate and representative AI models, however, they must be used in ways that preserve individual privacy.</p><p data-block-key=\"b23gn\">That’s where <a href=\"https://docs.jax.dev/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\">JAX</a> and <a href=\"https://github.com/google-deepmind/jax_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">JAX-Privacy</a> come in. Introduced in 2020, JAX is a high-performance numerical computing library designed for large-scale machine learning (ML). Its core features — including <a href=\"https://en.wikipedia.org/wiki/Automatic_differentiation\" target=\"_blank\" rel=\"noopener noreferrer\">automatic differentiation</a>, <a href=\"https://en.wikipedia.org/wiki/Just-in-time_compilation\" target=\"_blank\" rel=\"noopener noreferrer\">just-in-time compilation</a>, and seamless scaling across multiple accelerators — make it an ideal platform for building and training complex models efficiently. JAX <a href=\"https://github.com/jax-ml/jax/network/dependents\" target=\"_blank\" rel=\"noopener noreferrer\">has become a cornerstone</a> for researchers and engineers pushing the boundaries of AI. Its surrounding ecosystem includes a robust set of domain-specific libraries, including <a href=\"https://flax.readthedocs.io/en/latest/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">Flax</a>, which simplifies the implementation of neural network architectures, and <a href=\"https://github.com/google-deepmind/optax\" target=\"_blank\" rel=\"noopener noreferrer\">Optax</a>, which implements state-of-the-art optimizers.</p><p data-block-key=\"fkr7i\">Built on JAX, JAX-Privacy is a robust toolkit for building and auditing differentially private models. It enables researchers and developers to quickly and efficiently implement <a href=\"https://en.wikipedia.org/wiki/Differential_privacyhttps://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">differentially private</a> (DP) algorithms for training deep learning models on large datasets, and provides the core tools needed to integrate private training into modern distributed training workflows. The original version of JAX-Privacy was introduced in 2022 to enable external researchers to reproduce and validate some of our <a href=\"https://github.com/google-deepmind/jax_privacy#reproducing-results\" target=\"_blank\" rel=\"noopener noreferrer\">advances on private training</a>. It has since evolved into a hub where research teams across Google integrate their novel research insights into DP training and auditing algorithms.</p><p data-block-key=\"857iv\">Today, we are proud to announce the release of <a href=\"https://github.com/google-deepmind/jax_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">JAX-Privacy 1.0</a>. Integrating our latest research advances and re-designed for modularity, this new version makes it easier than ever for researchers and developers to build DP training pipelines that combine state-of-the-art DP algorithms with the scalability provided by JAX.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xtj9j\">How we got here: The need for JAX-Privacy</h2><p data-block-key=\"d6p4s\">For years, researchers have turned to DP as the gold standard for quantifying and bounding privacy leakage. DP guarantees that the output of an algorithm is nearly the same whether or not a single individual (or example) is included in the dataset.</p><p data-block-key=\"efhk8\">While the theory of DP is well-established, its practical implementation in large-scale ML can be a challenge. The most common approach, <a href=\"https://arxiv.org/abs/1607.00133\" target=\"_blank\" rel=\"noopener noreferrer\">differentially private stochastic gradient descent</a> (DP-SGD), requires customized batching procedures, per-example gradient clipping, and the addition of carefully calibrated noise. This process is computationally intensive and can be difficult to implement correctly and efficiently, especially at the scale of modern foundation models.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/JAXPrivacy2_Overview.width-1250.png\" alt=\"JAXPrivacy2_Overview\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/JAXPrivacy2_Overview.width-1250.png\" alt=\"JAXPrivacy2_Overview\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"q4d8f\"><i>JAX-Privacy enables researchers and developers to train and fine-tune foundation models on private data using state-of-the-art differentially private algorithms in a scalable and efficient way thanks to its primitive building blocks for gradient clipping and correlated noise generation, both of which work effectively in distributed environments.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xtj9j\">Existing frameworks have made strides, but they often fall short in scalability or flexibility. Our work has consistently pushed the boundaries of private ML, from <a href=\"https://arxiv.org/abs/1607.00133\" target=\"_blank\" rel=\"noopener noreferrer\">pioneering new DP algorithms</a> to <a href=\"https://arxiv.org/abs/2302.07956\" target=\"_blank\" rel=\"noopener noreferrer\">developing sophisticated auditing techniques</a>. We needed a tool that could keep pace with our research — a library that was not only correct and efficient but also designed from the ground up to handle the parallelism and complexity of state-of-the-art models.</p><p data-block-key=\"1shto\">JAX's functional paradigm and powerful transformations, like <code>vmap</code> (for automatic vectorization) and <code>shard_map</code> (for single-program multiple-data parallelization), provided a strong foundation. By building on JAX, we could create a library that was parallelism-ready out-of-the-box, supporting the training of large-scale models across multiple accelerators and supercomputers. JAX-Privacy is the culmination of this effort, a time-tested library that has powered internal production integrations and is now being shared with the broader community.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xtj9j\">What JAX-Privacy delivers</h2><p data-block-key=\"ba6f3\">JAX-Privacy simplifies the complexities of DP by providing a suite of carefully engineered components:</p><ul><li data-block-key=\"1qgme\"><i>Core building blocks</i>: The library offers correct and efficient implementations of the fundamental DP primitives, including <a href=\"https://github.com/google-deepmind/jax_privacy/blob/main/jax_privacy/clipping.py\" target=\"_blank\" rel=\"noopener noreferrer\">per-example gradient clipping</a>, <a href=\"https://github.com/google-deepmind/jax_privacy/blob/main/jax_privacy/noise_addition.py\" target=\"_blank\" rel=\"noopener noreferrer\">noise addition</a>, and <a href=\"https://github.com/google-deepmind/jax_privacy/blob/main/jax_privacy/batch_selection.py\" target=\"_blank\" rel=\"noopener noreferrer\">data batch construction</a>. These components enable developers to build well-known algorithms like DP-SGD and <a href=\"https://arxiv.org/abs/2103.00039\" target=\"_blank\" rel=\"noopener noreferrer\">DP-FTRL</a> with confidence.</li><li data-block-key=\"7gq93\"><i>State-of-the-art algorithms</i>: JAX-Privacy goes beyond the basics, supporting advanced methods like <a href=\"https://arxiv.org/abs/2506.08201\" target=\"_blank\" rel=\"noopener noreferrer\">DP matrix factorization</a> that rely on injecting correlated noise across iterations, which have been shown to improve performance. This makes it easy for researchers to experiment with cutting-edge private training techniques.</li><li data-block-key=\"430g0\"><i>Scalability</i>: All components are designed to work seamlessly with JAX's native parallelism features. This means you can train large-scale models that require data and model parallelism without complex, custom code, making private training on large models a reality. JAX-Privacy also provides tools like micro-batching and padding for seamlessly handling massive, variable-sized batches that are typically needed to obtain the best privacy/utility trade-offs.</li><li data-block-key=\"7nhv3\"><i>Correctness and</i> <a href=\"https://github.com/google-deepmind/jax_privacy/blob/main/jax_privacy/auditing.py\" target=\"_blank\" rel=\"noopener noreferrer\"><i>auditing</i></a>: The library is built on Google's state-of-the-art <a href=\"https://github.com/google/differential-privacy/tree/main/python/dp_accounting\" target=\"_blank\" rel=\"noopener noreferrer\">DP accounting library</a>, ensuring the noise calibration is both mathematically correct and as tight as possible. These formal bounds on the privacy loss can be complemented with metrics that quantify the empirical privacy loss, providing a more complete view of the privacy properties of a training pipeline. Users can easily test and develop their own auditing techniques, like our award-winning work on \"<a href=\"https://www.usenix.org/conference/usenixsecurity23/presentation/nasr\" target=\"_blank\" rel=\"noopener noreferrer\">Tight Auditing of Differentially Private Machine Learning</a>\", which works by injecting \"canaries\" — known data points — and computing auditing metrics at each step.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/JAXPrivacy3_Animation.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"q4d8f\"><i>JAX-Privacy implements a variety of foundational tools for clipping, noise addition, batch selection, accounting, and auditing that can be combined in various ways to construct end-to-end DP training plans.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xtj9j\">From research to practice: Fine-tuning LLMs with confidence</h2><p data-block-key=\"9h6ls\">One of the most exciting aspects of JAX-Privacy is its practical application. The library is designed to support modern ML frameworks used for pre-training and fine-tuning LLMs. A notable example is our recent use of JAX-Privacy building blocks in the training of <a href=\"https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/\">VaultGemma</a>, the world's most capable differentially private LLM.</p><p data-block-key=\"6fss3\">With this open-source release, we want to enable developers to easily fine-tune large models with just a few lines of code via the popular <a href=\"https://keras.io/examples/nlp/\" target=\"_blank\" rel=\"noopener noreferrer\">Keras</a> framework. In particular, we include <a href=\"https://github.com/google-deepmind/jax_privacy/tree/main/examples\" target=\"_blank\" rel=\"noopener noreferrer\">fully-functional examples</a> for fine-tuning models in the <a href=\"https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma family</a>, a collection of open models built by Google DeepMind based on Gemini. These examples demonstrate how to apply JAX-Privacy to tasks like dialogue summarization and synthetic data generation, showing that this library can deliver state-of-the-art results even when working with the most advanced models.</p><p data-block-key=\"b2918\">By simplifying the integration of DP, JAX-Privacy empowers developers to build privacy-preserving applications from the ground up, whether they are fine-tuning a chatbot for a healthcare application or a model for personalized financial advice. It lowers the barrier to entry for privacy-preserving ML and makes powerful, responsible AI more accessible.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xtj9j\">Looking ahead</h2><p data-block-key=\"7gt08\">We are excited to share JAX-Privacy with the research community. This release is the result of years of dedicated effort and represents a significant contribution to the field of privacy-preserving ML. We hope that by providing these tools, we can enable a new wave of research and innovation that benefits everyone.</p><p data-block-key=\"7ne51\">We will continue to support and develop the library, incorporating new research advances and responding to the needs of the community. We look forward to seeing what you build using JAX-Privacy. Check out the <a href=\"https://github.com/google-deepmind/jax_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">repository on GitHub</a> or the <a href=\"https://pypi.org/project/jax-privacy/\" target=\"_blank\" rel=\"noopener noreferrer\">PIP package</a> to start training privacy-preserving ML models today.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xtj9j\">Acknowledgements</h2><p data-block-key=\"5vdv8\"><i>JAX-Privacy includes contributions from: Leonard Berrada, Robert Stanforth, Brendan McMahan, Christopher A. Choquette-Choo, Galen Andrew, Mikhail Pravilov, Sahra Ghalebikesabi, Aneesh Pappu, Michael Reneer, Jamie Hayes, Vadym Doroshenko, Keith Rush, Dj Dvijotham, Zachary Charles, Peter Kairouz, Soham De, Samuel L. Smith, Judy Hanwen Shen.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                ",
      "contentSnippet": "From personalized recommendations to scientific advances, AI models are helping to improve lives and transform industries. But the impact and accuracy of these AI models is often determined by the quality of data they use. Large, high-quality datasets are crucial for developing accurate and representative AI models, however, they must be used in ways that preserve individual privacy.\nThat’s where JAX and JAX-Privacy come in. Introduced in 2020, JAX is a high-performance numerical computing library designed for large-scale machine learning (ML). Its core features — including automatic differentiation, just-in-time compilation, and seamless scaling across multiple accelerators — make it an ideal platform for building and training complex models efficiently. JAX has become a cornerstone for researchers and engineers pushing the boundaries of AI. Its surrounding ecosystem includes a robust set of domain-specific libraries, including Flax, which simplifies the implementation of neural network architectures, and Optax, which implements state-of-the-art optimizers.\nBuilt on JAX, JAX-Privacy is a robust toolkit for building and auditing differentially private models. It enables researchers and developers to quickly and efficiently implement differentially private (DP) algorithms for training deep learning models on large datasets, and provides the core tools needed to integrate private training into modern distributed training workflows. The original version of JAX-Privacy was introduced in 2022 to enable external researchers to reproduce and validate some of our advances on private training. It has since evolved into a hub where research teams across Google integrate their novel research insights into DP training and auditing algorithms.\nToday, we are proud to announce the release of JAX-Privacy 1.0. Integrating our latest research advances and re-designed for modularity, this new version makes it easier than ever for researchers and developers to build DP training pipelines that combine state-of-the-art DP algorithms with the scalability provided by JAX.\nHow we got here: The need for JAX-Privacy\nFor years, researchers have turned to DP as the gold standard for quantifying and bounding privacy leakage. DP guarantees that the output of an algorithm is nearly the same whether or not a single individual (or example) is included in the dataset.\nWhile the theory of DP is well-established, its practical implementation in large-scale ML can be a challenge. The most common approach, differentially private stochastic gradient descent (DP-SGD), requires customized batching procedures, per-example gradient clipping, and the addition of carefully calibrated noise. This process is computationally intensive and can be difficult to implement correctly and efficiently, especially at the scale of modern foundation models.\nJAX-Privacy enables researchers and developers to train and fine-tune foundation models on private data using state-of-the-art differentially private algorithms in a scalable and efficient way thanks to its primitive building blocks for gradient clipping and correlated noise generation, both of which work effectively in distributed environments.\nExisting frameworks have made strides, but they often fall short in scalability or flexibility. Our work has consistently pushed the boundaries of private ML, from pioneering new DP algorithms to developing sophisticated auditing techniques. We needed a tool that could keep pace with our research — a library that was not only correct and efficient but also designed from the ground up to handle the parallelism and complexity of state-of-the-art models.\nJAX's functional paradigm and powerful transformations, like vmap (for automatic vectorization) and shard_map (for single-program multiple-data parallelization), provided a strong foundation. By building on JAX, we could create a library that was parallelism-ready out-of-the-box, supporting the training of large-scale models across multiple accelerators and supercomputers. JAX-Privacy is the culmination of this effort, a time-tested library that has powered internal production integrations and is now being shared with the broader community.\nWhat JAX-Privacy delivers\nJAX-Privacy simplifies the complexities of DP by providing a suite of carefully engineered components:\n\nCore building blocks: The library offers correct and efficient implementations of the fundamental DP primitives, including per-example gradient clipping, noise addition, and data batch construction. These components enable developers to build well-known algorithms like DP-SGD and DP-FTRL with confidence.\nState-of-the-art algorithms: JAX-Privacy goes beyond the basics, supporting advanced methods like DP matrix factorization that rely on injecting correlated noise across iterations, which have been shown to improve performance. This makes it easy for researchers to experiment with cutting-edge private training techniques.\nScalability: All components are designed to work seamlessly with JAX's native parallelism features. This means you can train large-scale models that require data and model parallelism without complex, custom code, making private training on large models a reality. JAX-Privacy also provides tools like micro-batching and padding for seamlessly handling massive, variable-sized batches that are typically needed to obtain the best privacy/utility trade-offs.\nCorrectness and auditing: The library is built on Google's state-of-the-art DP accounting library, ensuring the noise calibration is both mathematically correct and as tight as possible. These formal bounds on the privacy loss can be complemented with metrics that quantify the empirical privacy loss, providing a more complete view of the privacy properties of a training pipeline. Users can easily test and develop their own auditing techniques, like our award-winning work on \"Tight Auditing of Differentially Private Machine Learning\", which works by injecting \"canaries\" — known data points — and computing auditing metrics at each step.\n\n\n\n    \nplay silent looping video\n            pause silent looping video\n          \n\n\n\n            \n\n\n\n  \n\n\n  \n\n\n          \nunmute video\n        mute video\n    \n\n            \n\n        \n        \n            \n\n            \n\n        \n    \nJAX-Privacy implements a variety of foundational tools for clipping, noise addition, batch selection, accounting, and auditing that can be combined in various ways to construct end-to-end DP training plans.\nFrom research to practice: Fine-tuning LLMs with confidence\nOne of the most exciting aspects of JAX-Privacy is its practical application. The library is designed to support modern ML frameworks used for pre-training and fine-tuning LLMs. A notable example is our recent use of JAX-Privacy building blocks in the training of VaultGemma, the world's most capable differentially private LLM.\nWith this open-source release, we want to enable developers to easily fine-tune large models with just a few lines of code via the popular Keras framework. In particular, we include fully-functional examples for fine-tuning models in the Gemma family, a collection of open models built by Google DeepMind based on Gemini. These examples demonstrate how to apply JAX-Privacy to tasks like dialogue summarization and synthetic data generation, showing that this library can deliver state-of-the-art results even when working with the most advanced models.\nBy simplifying the integration of DP, JAX-Privacy empowers developers to build privacy-preserving applications from the ground up, whether they are fine-tuning a chatbot for a healthcare application or a model for personalized financial advice. It lowers the barrier to entry for privacy-preserving ML and makes powerful, responsible AI more accessible.\nLooking ahead\nWe are excited to share JAX-Privacy with the research community. This release is the result of years of dedicated effort and represents a significant contribution to the field of privacy-preserving ML. We hope that by providing these tools, we can enable a new wave of research and innovation that benefits everyone.\nWe will continue to support and develop the library, incorporating new research advances and responding to the needs of the community. We look forward to seeing what you build using JAX-Privacy. Check out the repository on GitHub or the PIP package to start training privacy-preserving ML models today.\nAcknowledgements\nJAX-Privacy includes contributions from: Leonard Berrada, Robert Stanforth, Brendan McMahan, Christopher A. Choquette-Choo, Galen Andrew, Mikhail Pravilov, Sahra Ghalebikesabi, Aneesh Pappu, Michael Reneer, Jamie Hayes, Vadym Doroshenko, Keith Rush, Dj Dvijotham, Zachary Charles, Peter Kairouz, Soham De, Samuel L. Smith, Judy Hanwen Shen.",
      "creator": "Google",
      "summary": "JAX-Privacy 是一个基于 JAX 的差分隐私机器学习工具包，用于在大型数据集上高效训练深度学习模型。它提供先进的差分隐私算法和模块化设计，支持分布式训练工作流，简化了隐私保护机器学习的实现。JAX-Privacy 1.0 版本集成了最新研究进展，增强了可扩展性和灵活性，使研究人员和开发者能够更轻松地构建隐私保护模型。"
    }
  ],
  "lastUpdated": "2025-12-27T15:23:56.320Z"
}